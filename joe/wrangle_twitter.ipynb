{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import random\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from datetime import date\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import concurrent.futures\n",
    "\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from pandas.plotting import table \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib_venn_wordcloud import venn3_wordcloud,venn2_wordcloud\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from matplotlib_venn import venn3, venn3_circles,venn2_circles,venn2,venn2_unweighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read in top 1,000 celebs\n",
    "url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "celebs=pd.read_csv(url)\n",
    "celebs=celebs.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Below are two ways of scraping using CLI commands.\n",
    "# Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "# then output two different csv files.\n",
    "\n",
    "# Query by username\n",
    "# # Setting variables to be used in format string command below\n",
    "\n",
    "\n",
    "# tweet_count =100 #this number is the last n tweets\n",
    "# dflist=[]\n",
    "# dfdict={}\n",
    "# count=-1*tweet_count\n",
    "# celeblen=len(celebs.get('twitter').keys())\n",
    "# numindex=list(range(0,tweet_count*celeblen))\n",
    "# len(numindex)\n",
    "\n",
    "\n",
    "# dictcount=0\n",
    "# errornames=[]\n",
    "\n",
    "# for c in range(0,celeblen):\n",
    "#     dictcount+=1\n",
    "#     count=count+tweet_count\n",
    "#     maxnum=count+tweet_count\n",
    "\n",
    "#     twitter_handle=list(celebs.get('twitter')[c])\n",
    "#     name=list(celebs.get('name')[c])\n",
    "    \n",
    "#     cur=numindex[count:maxnum]\n",
    "   \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "  \n",
    "\n",
    "  \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next  \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "    \n",
    "#         if dictcount==1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                 tweets_df1.update({'name':nameseries})\n",
    "#                 tweets_df1.update({'handle':handleseries})\n",
    "#                 a=tweets_df1.get(key)\n",
    "#                 b=dfdict.get(key)\n",
    "#                 c={**a,**b}\n",
    "#                 dfdict.update({key:c})\n",
    "#     except:\n",
    "#         errornames.append(name)\n",
    "#         print(errornames)\n",
    "#         pass\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "# dataframe=pd.DataFrame(dfdict).sort_index()\n",
    "# cols=list(set(dataframe.columns)-{'name','handle'})\n",
    "# cols.insert(0,'name')\n",
    "# cols.insert(0,'handle')\n",
    "# dataframe=dataframe[cols]\n",
    "\n",
    "# pd.to_pickle(dataframe,\"./fivezerominpull.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning you need to uncomment above to scrape and create the pickle. \n",
    "\n",
    "## Your speed my vary but it took 45 mins for the scrape\n",
    "\n",
    "## The code below directly below is to the big scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "885"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read from the pickle and then display the unique file\n",
    "\n",
    "dataframe=pd.read_pickle(\"./fivezerominpull.pkl\")\n",
    "dataframe.name.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>outlinks</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>tcooutlinks</th>\n",
       "      <th>...</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>id</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>inReplyToTweetId</th>\n",
       "      <th>user</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>renderedContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4575</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1210</td>\n",
       "      <td>1586844179363250176</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>29840</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>881</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>179</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603</td>\n",
       "      <td>1586251933479739393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>10502</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>üòâ https://t.co/70hbuuC39d</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.Tweet', 'u...</td>\n",
       "      <td>[https://twitter.com/perryorgasm_/status/15857...</td>\n",
       "      <td>2217</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>[https://t.co/70hbuuC39d]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>575</td>\n",
       "      <td>1585998214368722949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>31006</td>\n",
       "      <td>üòâ twitter.com/perryorgasm_/s‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>297</td>\n",
       "      <td>[chainedtothealgorithm, therealproblemlolhaha]</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1585709853367930881</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.585710e+18</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>3042</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>336</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>http://twitter.com/download/iphone</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>1585709755779072000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.585709e+18</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>3654</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99495</th>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>√â n√≥is, letras. https://t.co/gGypAe2CF1</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.Tweet', 'u...</td>\n",
       "      <td>[https://twitter.com/letras/status/15371337358...</td>\n",
       "      <td>57</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>[https://t.co/gGypAe2CF1]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>1537139164084305920</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>595</td>\n",
       "      <td>√â n√≥is, letras. twitter.com/letras/status/‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99496</th>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Senti l√°grimas ü•≤‚ù§ https://t.co/aw9IprSRr9</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.Tweet', 'u...</td>\n",
       "      <td>[https://twitter.com/prinuzelly/status/1537132...</td>\n",
       "      <td>38</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>[https://t.co/aw9IprSRr9]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>1537138986334007298</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>440</td>\n",
       "      <td>Senti l√°grimas ü•≤‚ù§ twitter.com/prinuzelly/sta‚Ä¶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99497</th>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Manda foto da TV!!!! T√° em qual epis√≥dio? http...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.Tweet', 'u...</td>\n",
       "      <td>[https://twitter.com/profxavierx/status/153713...</td>\n",
       "      <td>30</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>[https://t.co/jAWWoNX9vt]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "      <td>1537138787637252098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>405</td>\n",
       "      <td>Manda foto da TV!!!! T√° em qual epis√≥dio? twit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99498</th>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Esse eu gostaria de ter roubado. Errei. https:...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.Tweet', 'u...</td>\n",
       "      <td>[https://twitter.com/gavaxsi/status/1537133732...</td>\n",
       "      <td>42</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>[https://t.co/wlZkxS5sem]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "      <td>1537138410812583936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>547</td>\n",
       "      <td>Esse eu gostaria de ter roubado. Errei. twitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99499</th>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Vida longa a Milene Sampaio! https://t.co/W9kw...</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.Tweet', 'u...</td>\n",
       "      <td>[https://twitter.com/withmanug/status/15371327...</td>\n",
       "      <td>47</td>\n",
       "      <td>None</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>http://twitter.com/download/android</td>\n",
       "      <td>[https://t.co/W9kw83e6FH]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19</td>\n",
       "      <td>1537138323482886145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>382</td>\n",
       "      <td>Vida longa a Milene Sampaio! twitter.com/withm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89500 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            handle          name  \\\n",
       "0        katyperry    katy perry   \n",
       "1        katyperry    katy perry   \n",
       "2        katyperry    katy perry   \n",
       "3        katyperry    katy perry   \n",
       "4        katyperry    katy perry   \n",
       "...            ...           ...   \n",
       "99495  manugavassi  manu gavassi   \n",
       "99496  manugavassi  manu gavassi   \n",
       "99497  manugavassi  manu gavassi   \n",
       "99498  manugavassi  manu gavassi   \n",
       "99499  manugavassi  manu gavassi   \n",
       "\n",
       "                                                 content  \\\n",
       "0      if you wanna know why any human is they way th...   \n",
       "1      wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "2                              üòâ https://t.co/70hbuuC39d   \n",
       "3      heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4      The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "...                                                  ...   \n",
       "99495            √â n√≥is, letras. https://t.co/gGypAe2CF1   \n",
       "99496          Senti l√°grimas ü•≤‚ù§ https://t.co/aw9IprSRr9   \n",
       "99497  Manda foto da TV!!!! T√° em qual epis√≥dio? http...   \n",
       "99498  Esse eu gostaria de ter roubado. Errei. https:...   \n",
       "99499  Vida longa a Milene Sampaio! https://t.co/W9kw...   \n",
       "\n",
       "                                             quotedTweet  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2      {'_type': 'snscrape.modules.twitter.Tweet', 'u...   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "99495  {'_type': 'snscrape.modules.twitter.Tweet', 'u...   \n",
       "99496  {'_type': 'snscrape.modules.twitter.Tweet', 'u...   \n",
       "99497  {'_type': 'snscrape.modules.twitter.Tweet', 'u...   \n",
       "99498  {'_type': 'snscrape.modules.twitter.Tweet', 'u...   \n",
       "99499  {'_type': 'snscrape.modules.twitter.Tweet', 'u...   \n",
       "\n",
       "                                                outlinks  retweetCount  \\\n",
       "0                                                   None          4575   \n",
       "1                                                   None           881   \n",
       "2      [https://twitter.com/perryorgasm_/status/15857...          2217   \n",
       "3                                                   None           297   \n",
       "4                                                   None           336   \n",
       "...                                                  ...           ...   \n",
       "99495  [https://twitter.com/letras/status/15371337358...            57   \n",
       "99496  [https://twitter.com/prinuzelly/status/1537132...            38   \n",
       "99497  [https://twitter.com/profxavierx/status/153713...            30   \n",
       "99498  [https://twitter.com/gavaxsi/status/1537133732...            42   \n",
       "99499  [https://twitter.com/withmanug/status/15371327...            47   \n",
       "\n",
       "                                             hashtags          sourceLabel  \\\n",
       "0                                                None   Twitter for iPhone   \n",
       "1                                                None   Twitter for iPhone   \n",
       "2                                                None   Twitter for iPhone   \n",
       "3      [chainedtothealgorithm, therealproblemlolhaha]   Twitter for iPhone   \n",
       "4                                                None   Twitter for iPhone   \n",
       "...                                               ...                  ...   \n",
       "99495                                            None  Twitter for Android   \n",
       "99496                                            None  Twitter for Android   \n",
       "99497                                            None  Twitter for Android   \n",
       "99498                                            None  Twitter for Android   \n",
       "99499                                            None  Twitter for Android   \n",
       "\n",
       "                                 sourceUrl                tcooutlinks  ...  \\\n",
       "0       http://twitter.com/download/iphone                       None  ...   \n",
       "1       http://twitter.com/download/iphone                       None  ...   \n",
       "2       http://twitter.com/download/iphone  [https://t.co/70hbuuC39d]  ...   \n",
       "3       http://twitter.com/download/iphone                       None  ...   \n",
       "4       http://twitter.com/download/iphone                       None  ...   \n",
       "...                                    ...                        ...  ...   \n",
       "99495  http://twitter.com/download/android  [https://t.co/gGypAe2CF1]  ...   \n",
       "99496  http://twitter.com/download/android  [https://t.co/aw9IprSRr9]  ...   \n",
       "99497  http://twitter.com/download/android  [https://t.co/jAWWoNX9vt]  ...   \n",
       "99498  http://twitter.com/download/android  [https://t.co/wlZkxS5sem]  ...   \n",
       "99499  http://twitter.com/download/android  [https://t.co/W9kw83e6FH]  ...   \n",
       "\n",
       "      mentionedUsers  quoteCount coordinates replyCount                   id  \\\n",
       "0               None         623         NaN       1210  1586844179363250176   \n",
       "1               None         179         NaN        603  1586251933479739393   \n",
       "2               None         114         NaN        575  1585998214368722949   \n",
       "3               None          34         NaN        100  1585709853367930881   \n",
       "4               None          17         NaN         69  1585709755779072000   \n",
       "...              ...         ...         ...        ...                  ...   \n",
       "99495           None           0         NaN         19  1537139164084305920   \n",
       "99496           None           2         NaN         25  1537138986334007298   \n",
       "99497           None           2         NaN         38  1537138787637252098   \n",
       "99498           None           1         NaN         29  1537138410812583936   \n",
       "99499           None           1         NaN         19  1537138323482886145   \n",
       "\n",
       "      cashtags  inReplyToTweetId  \\\n",
       "0          NaN               NaN   \n",
       "1          NaN               NaN   \n",
       "2          NaN               NaN   \n",
       "3          NaN      1.585710e+18   \n",
       "4          NaN      1.585709e+18   \n",
       "...        ...               ...   \n",
       "99495      NaN               NaN   \n",
       "99496      NaN               NaN   \n",
       "99497      NaN               NaN   \n",
       "99498      NaN               NaN   \n",
       "99499      NaN               NaN   \n",
       "\n",
       "                                                    user likeCount  \\\n",
       "0      {'_type': 'snscrape.modules.twitter.User', 'us...     29840   \n",
       "1      {'_type': 'snscrape.modules.twitter.User', 'us...     10502   \n",
       "2      {'_type': 'snscrape.modules.twitter.User', 'us...     31006   \n",
       "3      {'_type': 'snscrape.modules.twitter.User', 'us...      3042   \n",
       "4      {'_type': 'snscrape.modules.twitter.User', 'us...      3654   \n",
       "...                                                  ...       ...   \n",
       "99495  {'_type': 'snscrape.modules.twitter.User', 'us...       595   \n",
       "99496  {'_type': 'snscrape.modules.twitter.User', 'us...       440   \n",
       "99497  {'_type': 'snscrape.modules.twitter.User', 'us...       405   \n",
       "99498  {'_type': 'snscrape.modules.twitter.User', 'us...       547   \n",
       "99499  {'_type': 'snscrape.modules.twitter.User', 'us...       382   \n",
       "\n",
       "                                         renderedContent  \n",
       "0      if you wanna know why any human is they way th...  \n",
       "1      wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...  \n",
       "2                          üòâ twitter.com/perryorgasm_/s‚Ä¶  \n",
       "3      heck I pour beer out of my tits (that‚Äôs a part...  \n",
       "4      The show‚Äôs set list is a fun üé¢  through memory...  \n",
       "...                                                  ...  \n",
       "99495        √â n√≥is, letras. twitter.com/letras/status/‚Ä¶  \n",
       "99496      Senti l√°grimas ü•≤‚ù§ twitter.com/prinuzelly/sta‚Ä¶  \n",
       "99497  Manda foto da TV!!!! T√° em qual epis√≥dio? twit...  \n",
       "99498  Esse eu gostaria de ter roubado. Errei. twitte...  \n",
       "99499  Vida longa a Milene Sampaio! twitter.com/withm...  \n",
       "\n",
       "[89500 rows x 30 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptypeurl='https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv'\n",
    "\n",
    "ptypes=pd.read_csv(ptypeurl);ptypes\n",
    "newcols=[]\n",
    "for x in ptypes.columns.to_list():\n",
    "    y=x.lower()\n",
    "    newcols.append(y)\n",
    "\n",
    "ptypes.columns=newcols\n",
    "ptypes['handle']=ptypes.twitter\n",
    "ptypes.drop(columns='twitter',inplace=True)\n",
    "ptypes.name=ptypes.name.str.lower();ptypes\n",
    "\n",
    "dataframe.name=dataframe.name.str.lower();dataframe\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Twitter for iPhone      58826\n",
       "Twitter Web App         11512\n",
       "Instagram                5823\n",
       "Twitter for Android      4334\n",
       "Twitter Web Client       1616\n",
       "                        ...  \n",
       "TwitLonger Beta             1\n",
       "Twitter for  Android        1\n",
       "OS X                        1\n",
       "ViralSweep App              1\n",
       "QWVR                        1\n",
       "Name: sourceLabel, Length: 70, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sourceLabel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_69996/3421587410.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.dropna(axis=0,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "      <th>591</th>\n",
       "      <th>592</th>\n",
       "      <th>593</th>\n",
       "      <th>594</th>\n",
       "      <th>595</th>\n",
       "      <th>596</th>\n",
       "      <th>597</th>\n",
       "      <th>598</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>justin bieber</td>\n",
       "      <td>taylor swift</td>\n",
       "      <td>rihanna</td>\n",
       "      <td>the countess</td>\n",
       "      <td>justin timberlake</td>\n",
       "      <td>ellen degeneres</td>\n",
       "      <td>britney spears</td>\n",
       "      <td>cristiano ronaldo</td>\n",
       "      <td>kim kardashian west</td>\n",
       "      <td>...</td>\n",
       "      <td>depeche mode</td>\n",
       "      <td>amy schumer</td>\n",
       "      <td>craig ferguson</td>\n",
       "      <td>ayushmann khurrana</td>\n",
       "      <td>birdman</td>\n",
       "      <td>kylie minogue</td>\n",
       "      <td>zedd</td>\n",
       "      <td>troian</td>\n",
       "      <td>juicy j</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 599 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0              1             2        3             4    \\\n",
       "0  katy perry  justin bieber  taylor swift  rihanna  the countess   \n",
       "\n",
       "                 5                6               7                  8    \\\n",
       "0  justin timberlake  ellen degeneres  britney spears  cristiano ronaldo   \n",
       "\n",
       "                   9    ...           589          590             591  \\\n",
       "0  kim kardashian west  ...  depeche mode  amy schumer  craig ferguson   \n",
       "\n",
       "                  592      593            594   595     596      597  \\\n",
       "0  ayushmann khurrana  birdman  kylie minogue  zedd  troian  juicy j   \n",
       "\n",
       "            598  \n",
       "0  manu gavassi  \n",
       "\n",
       "[1 rows x 599 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>37</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>51</td>\n",
       "      <td>66</td>\n",
       "      <td>103</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type  ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "name    37    60    22    51    66   103    23    50    27    15    11    11   \n",
       "\n",
       "type  ISFJ  ISFP  ISTJ  ISTP  \n",
       "name    32    49    12    30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "a=ptypes.name.values.tolist()\n",
    "b=ptypes.type.values.tolist()\n",
    "ptypemap=dict(zip(a,b))\n",
    "ptypemap\n",
    "dataframe.dropna(axis=1,inplace=True)\n",
    "dataframe['type']=dataframe.name.map(ptypemap)\n",
    "\n",
    "\n",
    "\n",
    "cols=list(set(dataframe.columns)-{'name','handle','type'})\n",
    "cols.insert(0,'handle')\n",
    "cols.insert(0,'name')\n",
    "cols.insert(0,'type')\n",
    "\n",
    "dataframe=dataframe[cols]\n",
    "dataframe.dropna(axis=0,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "display(pd.DataFrame((dataframe['name'].unique()),index=range(0,len(dataframe['name'].unique()))).T)\n",
    "display(dataframe[['type','name']].groupby(['type']).nunique().T)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>üòâ twitter.com/perryorgasm_/s‚Ä¶</td>\n",
       "      <td>üòâ https://t.co/70hbuuC39d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99495</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>√â n√≥is, letras. twitter.com/letras/status/‚Ä¶</td>\n",
       "      <td>√â n√≥is, letras. https://t.co/gGypAe2CF1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99496</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Senti l√°grimas ü•≤‚ù§ twitter.com/prinuzelly/sta‚Ä¶</td>\n",
       "      <td>Senti l√°grimas ü•≤‚ù§ https://t.co/aw9IprSRr9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99497</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Manda foto da TV!!!! T√° em qual epis√≥dio? twit...</td>\n",
       "      <td>Manda foto da TV!!!! T√° em qual epis√≥dio? http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99498</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Esse eu gostaria de ter roubado. Errei. twitte...</td>\n",
       "      <td>Esse eu gostaria de ter roubado. Errei. https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99499</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Vida longa a Milene Sampaio! twitter.com/withm...</td>\n",
       "      <td>Vida longa a Milene Sampaio! https://t.co/W9kw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60900 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type          name                                    renderedContent  \\\n",
       "0      ENFJ    katy perry  if you wanna know why any human is they way th...   \n",
       "1      ENFJ    katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "2      ENFJ    katy perry                      üòâ twitter.com/perryorgasm_/s‚Ä¶   \n",
       "3      ENFJ    katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4      ENFJ    katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "...     ...           ...                                                ...   \n",
       "99495  ENFP  manu gavassi        √â n√≥is, letras. twitter.com/letras/status/‚Ä¶   \n",
       "99496  ENFP  manu gavassi      Senti l√°grimas ü•≤‚ù§ twitter.com/prinuzelly/sta‚Ä¶   \n",
       "99497  ENFP  manu gavassi  Manda foto da TV!!!! T√° em qual epis√≥dio? twit...   \n",
       "99498  ENFP  manu gavassi  Esse eu gostaria de ter roubado. Errei. twitte...   \n",
       "99499  ENFP  manu gavassi  Vida longa a Milene Sampaio! twitter.com/withm...   \n",
       "\n",
       "                                                 content  \n",
       "0      if you wanna know why any human is they way th...  \n",
       "1      wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...  \n",
       "2                              üòâ https://t.co/70hbuuC39d  \n",
       "3      heck I pour beer out of my tits (that‚Äôs a part...  \n",
       "4      The show‚Äôs set list is a fun üé¢  through memory...  \n",
       "...                                                  ...  \n",
       "99495            √â n√≥is, letras. https://t.co/gGypAe2CF1  \n",
       "99496          Senti l√°grimas ü•≤‚ù§ https://t.co/aw9IprSRr9  \n",
       "99497  Manda foto da TV!!!! T√° em qual epis√≥dio? http...  \n",
       "99498  Esse eu gostaria de ter roubado. Errei. https:...  \n",
       "99499  Vida longa a Milene Sampaio! https://t.co/W9kw...  \n",
       "\n",
       "[60900 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns.to_frame().T\n",
    "cols=['type','name','renderedContent','content']\n",
    "keep=dataframe[cols];keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquire is done \n",
    "\n",
    "\n",
    "## Prep is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1=keep[['type','name','content']].groupby(by=['type','name'])\n",
    "lista=set(group1.groups.keys())\n",
    "group2=keep[['type','name']].groupby(by=['type'])\n",
    "listb=list(set(group2.groups.keys()))\n",
    "group3=keep[['name','content']].groupby(by=['name'])   \n",
    "indexbyperson={}\n",
    "for b in listb:\n",
    "    g=list(group2.get_group(b).index)\n",
    "    n=list(group2.get_group(b).name.unique())\n",
    "    \n",
    "    ndict={}\n",
    "    for i in n:\n",
    "        k=list(group3.get_group(i).index)\n",
    "        c=list(group3.get_group(i).content)\n",
    "        ndict.update({i:{'index':k,'content':c}})\n",
    "    indexbyperson.update({b:{'index':g,'name':ndict}})\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "#pdf plumber\n",
    "#csv.preview\n",
    "import pandas as pd\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stopwords = ['like', 'im', 'think', 'dont', 'people', 'know', 'one', 'get', 'really','thing',\n",
    "                  'would', 'time', 'type', 'make', 'friend', 'ive', 'feel', 'much', 'love',\n",
    "                 'say', 'way', 'see', 'thing', 'want', 'thing', 'good', 'something', 'lot',\n",
    "                  'also', 'go', 'always', 'even', 'well', 'someone','https','com','co',',',\"'\"]\n",
    "\n",
    "\n",
    "\n",
    "stops=stopwords.words(['french','german','english','spanish','portuguese'])+ more_stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(stops,'stopwords.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "    stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "   \n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''   \n",
    "    Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #make a copy and begin to transform it\n",
    "    newtext = text.lower()\n",
    "\n",
    "    #encode into ascii then decode\n",
    "    newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "\n",
    "    #use re.sub to remove special characters\n",
    "    newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return newtext\n",
    "\n",
    "    \n",
    "def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    \n",
    "      Takes text, tokenizes it, lemmatizes it\n",
    "      lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "      needs to be commented out after the first run (up to modeling)\n",
    "      # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "      needs to be un commented commented\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "\n",
    "    #make ready the lemmatizer object\n",
    "    newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "    lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return lemmafiltered\n",
    "    \n",
    "    \n",
    "def split_apply_join(funct,listobj):\n",
    "    'helperfuction letters'\n",
    "\n",
    "    mapped=map(funct, listobj)\n",
    "    mapped=list(mapped)\n",
    "    mapped=''.join(mapped)\n",
    "  \n",
    "    return mapped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    ''' \n",
    "    For a large file just save it locally\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "    #make ready tokenizer object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "    return newtext\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "bigdict={'type':{},'name':{},'stoped_lemma':{},'freq':{}}\n",
    "for i in list(indexbyperson.keys()):\n",
    "    a=indexbyperson.get(i)\n",
    "    a=a['name']\n",
    "    for i1 in list(a.keys()):\n",
    "        listtonormaliz=str(a[i1]['content'])\n",
    "        newtext=lemmatizor(listtonormaliz,regexfilter=r'[^a-z0-9\\'\\s]')\n",
    "        lemma=newtext\n",
    "       \n",
    "        stoped=stopfilter(lemma)\n",
    "        stoped=stoped.replace('https','').replace('com','').replace('co','').replace(',','').strip()\n",
    "       \n",
    "        a[i1].update({'stopped_lemma':stoped})         \n",
    "     \n",
    "        cool=dict(pd.Series(stoped.split()).value_counts())\n",
    "        a[i1].update({'word freq':cool})\n",
    "        bigdict['type'].update({num:i})\n",
    "        bigdict['stoped_lemma'].update({num:stoped})\n",
    "        bigdict['freq'].update({num:cool})\n",
    "        bigdict['name'].update({num:i1})\n",
    "        num+=1\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>oprah winfrey</td>\n",
       "      <td>created place gather stories black women honor...</td>\n",
       "      <td>{'us': 20, 'new': 12, 'join': 12, 'thank': 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>robin van persie</td>\n",
       "      <td>almost showtime byvp honoured thanks day veree...</td>\n",
       "      <td>{'day': 11, 'best': 9, 'great': 8, 'new': 8, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>j. cole</td>\n",
       "      <td>ari lennox new album age sex location right ar...</td>\n",
       "      <td>{'n': 28, 'album': 20, 'season': 14, 'right': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>fearne cotton</td>\n",
       "      <td>nhave chance look around happy place app yet m...</td>\n",
       "      <td>{'happy': 28, 'place': 26, 'n': 15, 'amp': 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>robert kardashian</td>\n",
       "      <td>mrhodlr legend mrhodlr thank miss cap gm whats...</td>\n",
       "      <td>{'happy': 11, 'thank': 9, 'birthday': 9, 'new'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>nathan kress</td>\n",
       "      <td>goodwill baby ocd side right heres john varvat...</td>\n",
       "      <td>{'icarly': 66, 'episode': 17, 'spencer': 9, 'n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>charlotte crosby</td>\n",
       "      <td>thankyou jakes family sttish oh god gunna wort...</td>\n",
       "      <td>{'actually': 9, 'thankyou': 6, 'mandy': 6, 'ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>luke brooks</td>\n",
       "      <td>reacted old videos years ago via youtube twitt...</td>\n",
       "      <td>{'via': 37, 'youtube': 35, 'years': 8, 'twitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>ayushmann khurrana</td>\n",
       "      <td>anubhuti thank chevane thank nroshan rehta hai...</td>\n",
       "      <td>{'thank': 31, 'anubhuti': 23, 'doctorg': 22, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>oie passando aqui pra livrar voces deserto ris...</td>\n",
       "      <td>{'granola': 28, 'ece': 23, 'nao': 17, 'voces':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type                name  \\\n",
       "0    INFJ       oprah winfrey   \n",
       "1    INFJ    robin van persie   \n",
       "2    INFJ             j. cole   \n",
       "3    INFJ       fearne cotton   \n",
       "4    INFJ   robert kardashian   \n",
       "..    ...                 ...   \n",
       "594  ENFP        nathan kress   \n",
       "595  ENFP    charlotte crosby   \n",
       "596  ENFP         luke brooks   \n",
       "597  ENFP  ayushmann khurrana   \n",
       "598  ENFP        manu gavassi   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "0    created place gather stories black women honor...   \n",
       "1    almost showtime byvp honoured thanks day veree...   \n",
       "2    ari lennox new album age sex location right ar...   \n",
       "3    nhave chance look around happy place app yet m...   \n",
       "4    mrhodlr legend mrhodlr thank miss cap gm whats...   \n",
       "..                                                 ...   \n",
       "594  goodwill baby ocd side right heres john varvat...   \n",
       "595  thankyou jakes family sttish oh god gunna wort...   \n",
       "596  reacted old videos years ago via youtube twitt...   \n",
       "597  anubhuti thank chevane thank nroshan rehta hai...   \n",
       "598  oie passando aqui pra livrar voces deserto ris...   \n",
       "\n",
       "                                                  freq  \n",
       "0    {'us': 20, 'new': 12, 'join': 12, 'thank': 11,...  \n",
       "1    {'day': 11, 'best': 9, 'great': 8, 'new': 8, '...  \n",
       "2    {'n': 28, 'album': 20, 'season': 14, 'right': ...  \n",
       "3    {'happy': 28, 'place': 26, 'n': 15, 'amp': 12,...  \n",
       "4    {'happy': 11, 'thank': 9, 'birthday': 9, 'new'...  \n",
       "..                                                 ...  \n",
       "594  {'icarly': 66, 'episode': 17, 'spencer': 9, 'n...  \n",
       "595  {'actually': 9, 'thankyou': 6, 'mandy': 6, 'ba...  \n",
       "596  {'via': 37, 'youtube': 35, 'years': 8, 'twitte...  \n",
       "597  {'thank': 31, 'anubhuti': 23, 'doctorg': 22, '...  \n",
       "598  {'granola': 28, 'ece': 23, 'nao': 17, 'voces':...  \n",
       "\n",
       "[599 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterwordslemma=pd.DataFrame(bigdict)\n",
    "twitterwordslemma.columns=['type','name','lemmatized','freq']\n",
    "twitterwordslemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infj</td>\n",
       "      <td>oprah winfrey</td>\n",
       "      <td>created place gather stories black women honor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>infj</td>\n",
       "      <td>robin van persie</td>\n",
       "      <td>almost showtime byvp honoured thanks day veree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>infj</td>\n",
       "      <td>j. cole</td>\n",
       "      <td>ari lennox new album age sex location right ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>infj</td>\n",
       "      <td>fearne cotton</td>\n",
       "      <td>nhave chance look around happy place app yet m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>infj</td>\n",
       "      <td>robert kardashian</td>\n",
       "      <td>mrhodlr legend mrhodlr thank miss cap gm whats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>enfp</td>\n",
       "      <td>nathan kress</td>\n",
       "      <td>goodwill baby ocd side right heres john varvat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>enfp</td>\n",
       "      <td>charlotte crosby</td>\n",
       "      <td>thankyou jakes family sttish oh god gunna wort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>enfp</td>\n",
       "      <td>luke brooks</td>\n",
       "      <td>reacted old videos years ago via youtube twitt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>enfp</td>\n",
       "      <td>ayushmann khurrana</td>\n",
       "      <td>anubhuti thank chevane thank nroshan rehta hai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>oie passando aqui pra livrar voces deserto ris...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>599 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type                name  \\\n",
       "0    infj       oprah winfrey   \n",
       "1    infj    robin van persie   \n",
       "2    infj             j. cole   \n",
       "3    infj       fearne cotton   \n",
       "4    infj   robert kardashian   \n",
       "..    ...                 ...   \n",
       "594  enfp        nathan kress   \n",
       "595  enfp    charlotte crosby   \n",
       "596  enfp         luke brooks   \n",
       "597  enfp  ayushmann khurrana   \n",
       "598  enfp        manu gavassi   \n",
       "\n",
       "                                            lemmatized  \n",
       "0    created place gather stories black women honor...  \n",
       "1    almost showtime byvp honoured thanks day veree...  \n",
       "2    ari lennox new album age sex location right ar...  \n",
       "3    nhave chance look around happy place app yet m...  \n",
       "4    mrhodlr legend mrhodlr thank miss cap gm whats...  \n",
       "..                                                 ...  \n",
       "594  goodwill baby ocd side right heres john varvat...  \n",
       "595  thankyou jakes family sttish oh god gunna wort...  \n",
       "596  reacted old videos years ago via youtube twitt...  \n",
       "597  anubhuti thank chevane thank nroshan rehta hai...  \n",
       "598  oie passando aqui pra livrar voces deserto ris...  \n",
       "\n",
       "[599 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterwordslemma['type']=twitterwordslemma.type.str.lower()\n",
    "\n",
    "pd.to_pickle(twitterwordslemma,'maindalemma.pkl')\n",
    "\n",
    "df=pd.read_pickle('maindalemma.pkl')\n",
    "df=df[[\t'type',\t'name',\t'lemmatized'\t]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaing is explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma=pd.read_pickle('maindalemma.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['stoped_lemma'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bigdict_type\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m:{},\u001b[39m'\u001b[39m\u001b[39mstoped_lemma\u001b[39m\u001b[39m'\u001b[39m:{},\u001b[39m'\u001b[39m\u001b[39mfreq\u001b[39m\u001b[39m'\u001b[39m:{}}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m group1\u001b[39m=\u001b[39mtwitterwordslemma[[\u001b[39m'\u001b[39;49m\u001b[39mstoped_lemma\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m group1\u001b[39m.\u001b[39mgroups\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m group1\u001b[39m.\u001b[39mgroups\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['stoped_lemma'] not in index\""
     ]
    }
   ],
   "source": [
    "num=0\n",
    "bigdict_type={'type':{},'stoped_lemma':{},'freq':{}}\n",
    "\n",
    "group1=twitterwordslemma[['stoped_lemma','type']].groupby('type')\n",
    "group1.groups.keys()\n",
    "for i in group1.groups.keys():\n",
    "    \n",
    "    x=(','.join(list(group1.get_group(i).stoped_lemma.values)).strip())\n",
    "  \n",
    "    x=stopfilter(x)\n",
    "    \n",
    "   \n",
    "    y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "    cool=dict(y)\n",
    "    bigdict_type['type'].update({num:i})\n",
    "    bigdict_type['stoped_lemma'].update({num:x})\n",
    "    bigdict_type['freq'].update({num:cool})\n",
    "       \n",
    "\n",
    "    num+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.DataFrame(bigdict_type)\n",
    "x=str(typeslemma.stoped_lemma.values).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'').replace(',',' ').split()\n",
    "y=dict(pd.Series(x).value_counts())\n",
    "aggregatewordfrreq=y\n",
    "# pd.to_pickle(aggregatewordfrreq,'agglemma.pkl')\n",
    "num=len(typeslemma)\n",
    "\n",
    "\n",
    "z=[i.replace(',',' ') for i in typeslemma.stoped_lemma.values]\n",
    "typeslemma=pd.concat([typeslemma,pd.DataFrame({'type':{num:'COMBINED'},'stoped_lemma':{num:str(z).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'')},'freq':{num:aggregatewordfrreq}})])\n",
    "pd.to_pickle(typeslemma,'typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.read_pickle('typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extroverteddf=typeslemma[['type','stoped_lemma']].iloc[0:7]\n",
    "extroverteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "introverteddf=typeslemma[['type','stoped_lemma']].iloc[8:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdict=dict(typeslemma.freq.values[-1])\n",
    "#set stuff\n",
    "setlist=[]\n",
    "for i in typeslemma.freq.values:\n",
    "    setlist.append(set(i.keys()))\n",
    "\n",
    "typelist=[]\n",
    "for i in typeslemma.type.values:\n",
    "    typelist.append(i)\n",
    "\n",
    "typessetdict=dict(zip(typelist,setlist))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys=list(typessetdict.keys())\n",
    "combined=keys.pop(-1);combined\n",
    "intersectiondict={}\n",
    "c=set(typessetdict.get(combined))\n",
    "keys.reverse()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['red','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigint=deepcopy(c)\n",
    "[bigint.intersection_update(typessetdict.get(key))for key in keys]\n",
    "print(f'We have a total of {len(bigint)} in the aggregate intersection\\nWe remove that intersection to compare')\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    kthset=kthset-bigint\n",
    "    int=(c&kthset)-bigint\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    unionwithoutk=unionwithoutk-bigint\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "    unique=(kthset-unionwithoutk)-bigint\n",
    "\n",
    "    print(f'number unique to {k}\\n{len(unique)}\\n',f'percent unique of aggregate union combined:\\n{(len(unique)/len(c))*100:.2f}%')\n",
    "    restint=(kthset&unionwithoutk)-bigint\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','.35','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from itertools import product \n",
    "  \n",
    "# # Get all permutations of length 2 \n",
    "# # and length 2 \n",
    "# x=[\"\".join(seq) for seq in product(\"01\", repeat=4)]\n",
    "# for i in x:\n",
    "#     print(i[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumforavg=dataframe[['type','name']].groupby(['type']).nunique().sum()\n",
    "tochart=(dataframe[['type','name']].groupby(['type']).nunique()/sumforavg)*100\n",
    "\n",
    "tochart=tochart.reset_index()\n",
    "tochart['percent']=tochart['name']\n",
    "\n",
    "tochart.drop(columns='name',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tochart.index=tochart.type\n",
    "tochart.drop(columns='type',inplace=True)\n",
    "tochart=tochart.sort_values(by='percent',ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "genpoppercent=['13.8% 12.3% 11.6% 8.8% 8.7% 8.5% 8.1% 5.4% 4.4% 4.3% 3.3% 3.2% 2.5% 2.1% 1.8% 1.5%']\n",
    "genpoppercent=str(genpoppercent).replace('%','').split()\n",
    "genpoppercent=[float(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in genpoppercent]\n",
    "\n",
    "\n",
    "types=['ISFJ ESFI ISTJ ISFP ESTI ESFP ENFP ISTP INFP ESTP INTP ENTP ENFJ INTJ ENTI INFT']\n",
    "types=str(types).split()\n",
    "\n",
    "\n",
    "types=[(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in types]\n",
    "\n",
    "pop=pd.DataFrame(index=types,data={'pop percentage':genpoppercent})\n",
    "\n",
    "tochart=pd.concat([tochart,pop],axis=1,join='inner')\n",
    "tochart.rename(columns={'percent':'found percent'},inplace=True)\n",
    "cols=['pop percentage','found percent']\n",
    "tochart=tochart[cols]\n",
    "tochart.sort_values(by='pop percentage',ascending=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "styleddf=tochart.T.style.background_gradient(cmap='Blues',axis=1).format(lambda x : f'{x:.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad1=[0,0,0,0]\n",
    "quad2=[0,0,0,0]\n",
    "quad3=[0,.25,.35,.25]\n",
    "quad4=[.35,.25,.35,.25]\n",
    "\n",
    "explode = []\n",
    "explode.extend(quad1)\n",
    "explode.extend(quad2)\n",
    "explode.extend(quad3)\n",
    "explode.extend(quad4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def format_axes(fig):\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n",
    "        ax.tick_params(labelbottom=False, labelleft=False)\n",
    "m=1.23\n",
    "fig = plt.figure(constrained_layout=False,figsize=(m*20,m*12.361))\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('MBTI: General Population Vs Twitter',fontsize=16,weight='demibold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "kwargs1={'title':'General Population (Pie)   ','ax':ax1,'legend':False,'ylabel':'',   'cmap':'Blues'}\n",
    "\n",
    "tochart.plot.pie(y='found percent',**kwargs1)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "kwargs2={'title':'  Twitter (Pie)   ','ax':ax2,'legend':False,'ylabel':'',   'cmap':'viridis'}\n",
    "tochart.plot.pie(y='pop percentage',**kwargs2)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,1])\n",
    "kwargs3={'ax':ax3,'legend':False,'title':'Twitter (Bar)',   'cmap':'viridis'}\n",
    "\n",
    "tochart.plot.barh(y='found percent',**kwargs3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "kwargs4={'ax':ax4,'legend':False,'title':'General Population (Bar)',   'cmap':'Blues'}\n",
    "tochart.plot.barh(y='pop percentage',**kwargs4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"GridSpec\")\n",
    "format_axes(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display('Summary',styleddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Below was an attempt at Multithreading and Paralellism\n",
    "* ## This can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Below are two ways of scraping using CLI commands.\n",
    "# Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "# then output two different csv files.\n",
    "\n",
    "# Query by username\n",
    "# Setting variables to be used in format string command below\n",
    "# def parralledindexer():\n",
    "#  #tweet count number is the last n tweets\n",
    "#  # #read in top 1,000 celebs\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(list(celebs.get('twitter').keys()))\n",
    "#     numindex=range(0,tweet_count*celeblen)\n",
    "\n",
    "#     c_with_slice={}\n",
    "#     for c in range(0,celeblen,1):  \n",
    "#             count=count+tweet_count\n",
    "#             maxnum=count+tweet_count     \n",
    "#             cur=numindex[count:maxnum]\n",
    "#             c_with_slice.update({c:cur})\n",
    "           \n",
    "\n",
    "    # mod10={}\n",
    "    # mod9={}\n",
    "    # mod8={}\n",
    "    # mod7={}\n",
    "    # mod6={}\n",
    "    # mod5={}\n",
    "    # mod4={}\n",
    "    # mod3={}\n",
    "    # mod2={}\n",
    "    # keylist=list(c_with_slice.keys())\n",
    "    # for i in keylist:\n",
    "    #     if i%10==0:  \n",
    "    #         mod10.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%9==0:      \n",
    "    #         mod9.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%8==0:       \n",
    "    #         mod8.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%7==0:      \n",
    "    #         mod7.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%6==0:      \n",
    "    #         mod6.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%5==0:      \n",
    "    #         mod5.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%4==0:       \n",
    "    #         mod4.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%3==0:      \n",
    "    #         mod3.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%2==0:     \n",
    "    #         mod2.update({i:c_with_slice.get(i)})\n",
    "#     moddicts=c_with_slice\n",
    "        \n",
    "\n",
    "#     # moddicts={**mod10,**mod9,\n",
    "#     # **mod8,\n",
    "#     # **mod7,\n",
    "#     # **mod6,\n",
    "#     # **mod5,\n",
    "#     # **mod4,\n",
    "#     # **mod3,\n",
    "#     # **mod2}\n",
    "#     return moddicts,celebs\n",
    "\n",
    "# moddicts,celebs=parralledindexer()\n",
    "\n",
    "# ###Think of schem to split then pu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def actualparrelel():\n",
    "#     '''\n",
    "    \n",
    "#     slow as shit for input output this is for data processing on the comp\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "#         with Pool(10) as pool:\n",
    "#             arglist=[]   \n",
    "#             for m in moddicts:\n",
    "#                 arglist.append((m,celebs)) \n",
    "\n",
    "\n",
    "           \n",
    "#             results_async=pool.starmap_async(partitionableTwitterscraper,arglist)\n",
    "#             # get the return values\n",
    "#             try:\n",
    "#                 for value in results_async.get():\n",
    "#                     values.append(value)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f'Failed with: {e}')\n",
    "#     return values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def actualthreading():\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "  \n",
    "#             arglist=[]   \n",
    "#             for m,v in moddicts.items():\n",
    "#                 arglist.append({m:v})\n",
    "#             # print(arglist) #this is fine    \n",
    "\n",
    "#     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "\n",
    "#     threads = min(50, len(moddicts))   \n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         data=[]\n",
    "           \n",
    "\n",
    "#         for arg in arglist:\n",
    "#             for res in executor.submit(partitionableTwitterscraper(arg)):\n",
    "#                 executor.shutdown(wait=True)\n",
    "    \n",
    "#                 try:\n",
    "#                     data.append((res))\n",
    "#                     # print(res[0])\n",
    "#                     # print(res[1])\n",
    "\n",
    "\n",
    "\n",
    "#                 except Exception as exc:\n",
    "#                     pass\n",
    "#                 #  print('%r generated an exception: %s' % (result, exc))\n",
    "#         # else:\n",
    "#         #     # print('%r page is %d bytes' % (result, len(data)))\n",
    "    \n",
    "#     #sets the number of threads to the lesser of 30 or length of urls\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "  \n",
    "# # empty list with global scope\n",
    "# result = []\n",
    "# # empty list with global scope\n",
    "\n",
    "# def square_list(mylist):\n",
    "#     \"\"\"\n",
    "#     function to square a given list\n",
    "#     \"\"\"\n",
    "#     global result\n",
    "#     # append squares of mylist to global list result\n",
    "#     for num in mylist:\n",
    "#         result.append(num * num)\n",
    "\n",
    "\n",
    "# def partitionableTwitterscraper(c_with_slice):\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     c_with_slice=c_with_slice\n",
    "#     # print(c_with_slice)\n",
    "#     tweet_count =100 \n",
    "#     dictcount=0\n",
    "#     dfdict={}\n",
    "#     errornames=[]\n",
    "\n",
    "\n",
    "    \n",
    "#     print(c_with_slice.keys()) \n",
    "#     print('\\n') \n",
    "#     c=list(c_with_slice.keys())[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     twitter_handle=celebs.get('twitter')\n",
    "#     name=celebs.get('name')\n",
    "#     cur=c_with_slice.get(c)  \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "    \n",
    "\n",
    "    \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next artist i.e. failsafe \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "\n",
    "#         if dictcount<=1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                     tweets_df1.update({'name':nameseries})\n",
    "#                     tweets_df1.update({'handle':handleseries})\n",
    "#                     a=tweets_df1.get(key)\n",
    "#                     b=dfdict.get(key)\n",
    "#                     c={**a,**b}\n",
    "#                     dfdict=dfdict.update({key:c})\n",
    "#     except:\n",
    "#         # errornames.append(name)\n",
    "#         # print('errornames:\\n',len(errornames))\n",
    "#         pass \n",
    "\n",
    "\n",
    "#     display(dfdict)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "   \n",
    "\n",
    "#     return dfdict\n",
    "\n",
    "\n",
    "  \n",
    "# moddicts,celebs=parralledindexer()\n",
    "# values=[]\n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=[]   \n",
    "#     for m,v in moddicts.items():\n",
    "#         arglist.append({m:v})\n",
    "  \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=partitionableTwitterscraper, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "  \n",
    "#     # print global result list\n",
    "#     print(\"Result(in main program): {}\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maindict=list()\n",
    "\n",
    "# def argcreateator(n=3):\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "\n",
    "#     tweet_count =n #this number is the last n tweets\n",
    "#     dflist=[]\n",
    "#     dfdict={}\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(celebs.get('twitter').keys())\n",
    "#     numindex=list(range(0,tweet_count*celeblen))\n",
    "#     len(numindex)\n",
    "#     arglist=[]\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#     for c in range(0,celeblen):\n",
    "#         count=count+tweet_count\n",
    "#         maxnum=count+tweet_count\n",
    "\n",
    "#         twitter_handle=(celebs.get('twitter')[c])\n",
    "#         name=(celebs.get('name')[c])\n",
    "\n",
    "#         cur=pd.Index(numindex[count:maxnum])\n",
    "\n",
    "#         #create Series to append the current handle to the dataframe\n",
    "#         handleseries={i:twitter_handle for i in cur}       \n",
    "#         #create Series to append the current name to dataframe   \n",
    "#         nameseries={i:name for i in cur}\n",
    "#         arglist.append([cur,nameseries,handleseries])\n",
    "#     return arglist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur):\n",
    "#     global maindict\n",
    "#     os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "#      # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#      #if there is an error then it will just move to the next  \n",
    "#     tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=cur).to_dict()\n",
    "#     tweets_df1.update({'name':nameseries})\n",
    "#     tweets_df1.update({'handle':handleseries})\n",
    "#     maindict.append(pd.DataFrame(tweets_df1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def threader(arglist):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     dictcount=0\n",
    "#     errornames=[]\n",
    "\n",
    "   \n",
    "\n",
    "#     # for i in range(0,len(arglist)):\n",
    "#     for i in range(0,len(arglist)):\n",
    "#         tweet_count=len(arglist[i][0]);#display(tweet_count)\n",
    "#         nameseries=arglist[i][1];#display(nameseries)\n",
    "#         handleseries=arglist[i][2];#display(handleseries)\n",
    "#         twitter_handle=list(handleseries.values())[0];#display(twitter_handle)\n",
    "#         cur=arglist[i][0];#display(cur)\n",
    "#         try:\n",
    "#             tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur)\n",
    "            \n",
    "\n",
    "    \n",
    "#         except:\n",
    "#             errornames.append(twitter_handle)\n",
    "#             print(errornames)\n",
    "#             pass\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=argcreateator(n=3)   \n",
    "    \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=threader, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "#     print(maindict)\n",
    "  \n",
    "#     # print global result list\n",
    " \n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
