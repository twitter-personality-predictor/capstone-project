{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import random\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "# import multiprocessing\n",
    "# from multiprocessing import Pool\n",
    "from datetime import date\n",
    "# import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from  nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import concurrent.futures\n",
    "\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from pandas.plotting import table \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from matplotlib_venn_wordcloud import venn3_wordcloud,venn2_wordcloud\n",
    "# from wordcloud import WordCloud,ImageColorGenerator\n",
    "# from matplotlib_venn import venn3, venn3_circles,venn2_circles,venn2,venn2_unweighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read in top 1,000 celebs\n",
    "url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "celebs=pd.read_csv(url)\n",
    "\n",
    "celebs=celebs.to_dict()\n",
    "# celebs['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Below are two ways of scraping using CLI commands.\n",
    "## Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "## then output two different csv files.\n",
    "#\n",
    "## Query by username\n",
    "## Setting variables to be used in format string command below\n",
    "\n",
    "\n",
    "# tweet_count =500 #this number is the last n tweets\n",
    "# dflist=[]\n",
    "# dfdict={}\n",
    "# count=-1*tweet_count\n",
    "# celeblen=len(celebs.get('twitter').keys())\n",
    "# numindex=list(range(0,tweet_count*celeblen))\n",
    "# len(numindex)\n",
    "\n",
    "\n",
    "# dictcount=0\n",
    "# errornames=[]\n",
    "\n",
    "\n",
    "# for c in range(0,celeblen):\n",
    "#     sleep(random.randrange(0,7)/10)\n",
    "#     dictcount+=1\n",
    "#     count=count+tweet_count\n",
    "#     maxnum=count+tweet_count\n",
    "\n",
    "#     twitter_handle=str(celebs.get('twitter')[c])\n",
    "#     name=str(celebs.get('name')[c])\n",
    "    \n",
    "#     cur=numindex[count:maxnum]\n",
    "   \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "  \n",
    "\n",
    "  \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next  \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "    \n",
    "#         if dictcount==1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                 tweets_df1.update({'name':nameseries})\n",
    "#                 tweets_df1.update({'handle':handleseries})\n",
    "#                 a=tweets_df1.get(key)\n",
    "#                 b=dfdict.get(key)\n",
    "#                 c={**a,**b}\n",
    "#                 dfdict.update({key:c})\n",
    "#     except:\n",
    "#         errornames.append(name)\n",
    "#         print(errornames)\n",
    "#         pass\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "# dataframe=pd.DataFrame(dfdict).sort_index()\n",
    "# cols=list(set(dataframe.columns)-{'name','handle'})\n",
    "# cols.insert(0,'name')\n",
    "# cols.insert(0,'handle')\n",
    "\n",
    "\n",
    "# dataframe=dataframe[dataframe.lang=='en']\n",
    "\n",
    "# dataframe\n",
    "\n",
    "\n",
    "# pd.to_pickle(dataframe,\"500perpull.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning you need to uncomment above to scrape and create the pickle. \n",
    "\n",
    "## Your speed my vary but it took 45 mins for the scrape\n",
    "\n",
    "## The code below directly below is to the big scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the pickle and then display the unique file\n",
    "\n",
    "dataframe_a=pd.read_pickle(\"./fivezerominpull.pkl\")\n",
    "dataframe_b=pd.read_pickle(\"500perpull.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50762"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "246412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "297174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe_a=dataframe_a[dataframe_a.lang=='en']\n",
    "display(len(dataframe_a))\n",
    "display(len(dataframe_b))\n",
    "merged=pd.concat([dataframe_a,dataframe_b])\n",
    "display(len(merged))\n",
    "merged.name=merged.name.str.lower()\n",
    "dataframe=merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptypeurl='https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv'\n",
    "\n",
    "ptypes=pd.read_csv(ptypeurl);ptypes\n",
    "newcols=[]\n",
    "for x in ptypes.columns.to_list():\n",
    "    y=x.lower()\n",
    "    newcols.append(y)\n",
    "\n",
    "ptypes.columns=newcols\n",
    "ptypes['handle']=ptypes.twitter\n",
    "ptypes.drop(columns='twitter',inplace=True)\n",
    "ptypes.name=ptypes.name.str.lower();ptypes\n",
    "\n",
    "dataframe.name=dataframe.name.str.lower();dataframe.columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a=ptypes.name.values.tolist()\n",
    "b=ptypes.type.values.tolist()\n",
    "ptypemap=dict(zip(a,b))\n",
    "# ptypemap\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_15888/1665210032.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.dropna(axis=1,inplace=True)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_15888/1665210032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.dropna(axis=0,inplace=True)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_15888/1665210032.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['type']=dataframe.name.map(ptypemap)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "254650"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe=dataframe[dataframe['quotedTweet'].isna()]\n",
    "dataframe.dropna(axis=1,inplace=True)\n",
    "dataframe.dropna(axis=0,inplace=True)\n",
    "dataframe\n",
    "\n",
    "# dataframe.dropna(axis=1,inplace=True)\n",
    "dataframe['type']=dataframe.name.map(ptypemap)\n",
    "\n",
    "\n",
    "\n",
    "cols=list(set(dataframe.columns)-{'name','handle','type'})\n",
    "cols.insert(0,'handle')\n",
    "cols.insert(0,'name')\n",
    "cols.insert(0,'type')\n",
    "\n",
    "dataframe=dataframe[cols]\n",
    "\n",
    "len(dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>837</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>justin bieber</td>\n",
       "      <td>taylor swift</td>\n",
       "      <td>rihanna</td>\n",
       "      <td>the countess</td>\n",
       "      <td>justin timberlake</td>\n",
       "      <td>ellen degeneres</td>\n",
       "      <td>britney spears</td>\n",
       "      <td>cristiano ronaldo</td>\n",
       "      <td>kim kardashian west</td>\n",
       "      <td>...</td>\n",
       "      <td>nicolas vazquez</td>\n",
       "      <td>kenan doƒüulu</td>\n",
       "      <td>flor de la ve</td>\n",
       "      <td>cyril hanouna</td>\n",
       "      <td>man√°</td>\n",
       "      <td>sebastian rulli</td>\n",
       "      <td>carlos baute</td>\n",
       "      <td>levent √ºz√ºmc√º</td>\n",
       "      <td>adal ramones</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 847 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0              1             2        3             4    \\\n",
       "0  katy perry  justin bieber  taylor swift  rihanna  the countess   \n",
       "\n",
       "                 5                6               7                  8    \\\n",
       "0  justin timberlake  ellen degeneres  britney spears  cristiano ronaldo   \n",
       "\n",
       "                   9    ...              837           838            839  \\\n",
       "0  kim kardashian west  ...  nicolas vazquez  kenan doƒüulu  flor de la ve   \n",
       "\n",
       "             840   841              842           843            844  \\\n",
       "0  cyril hanouna  man√°  sebastian rulli  carlos baute  levent √ºz√ºmc√º   \n",
       "\n",
       "            845           846  \n",
       "0  adal ramones  manu gavassi  \n",
       "\n",
       "[1 rows x 847 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>21</td>\n",
       "      <td>49</td>\n",
       "      <td>63</td>\n",
       "      <td>103</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type  ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "name    37    59    21    49    63   103    23    50    25    15    11    11   \n",
       "\n",
       "type  ISFJ  ISFP  ISTJ  ISTP  \n",
       "name    32    49    12    30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "display(pd.DataFrame((dataframe['name'].unique()),index=range(0,len(dataframe['name'].unique()))).T)\n",
    "display(dataframe[['type','name']].groupby(['type']).nunique().T)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>_type</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>lang</th>\n",
       "      <th>id</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>name</td>\n",
       "      <td>handle</td>\n",
       "      <td>source</td>\n",
       "      <td>url</td>\n",
       "      <td>quoteCount</td>\n",
       "      <td>renderedContent</td>\n",
       "      <td>_type</td>\n",
       "      <td>replyCount</td>\n",
       "      <td>sourceUrl</td>\n",
       "      <td>lang</td>\n",
       "      <td>id</td>\n",
       "      <td>conversationId</td>\n",
       "      <td>date</td>\n",
       "      <td>content</td>\n",
       "      <td>retweetCount</td>\n",
       "      <td>likeCount</td>\n",
       "      <td>sourceLabel</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  name  handle  source  url  quoteCount  renderedContent  _type  \\\n",
       "0  type  name  handle  source  url  quoteCount  renderedContent  _type   \n",
       "\n",
       "   replyCount  sourceUrl  lang  id  conversationId  date  content  \\\n",
       "0  replyCount  sourceUrl  lang  id  conversationId  date  content   \n",
       "\n",
       "   retweetCount  likeCount  sourceLabel  user  \n",
       "0  retweetCount  likeCount  sourceLabel  user  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "254650"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "254650"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>handle</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>4575</td>\n",
       "      <td>29840</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>881</td>\n",
       "      <td>10502</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>297</td>\n",
       "      <td>3042</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>336</td>\n",
       "      <td>3654</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>2318</td>\n",
       "      <td>20819</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>juicy j</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>148</td>\n",
       "      <td>511</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>ESFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>91</td>\n",
       "      <td>2126</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>306</td>\n",
       "      <td>4395</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>142</td>\n",
       "      <td>2937</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>1225</td>\n",
       "      <td>13170</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254650 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                      date  \\\n",
       "0         katy perry 2022-10-30 22:15:09+00:00   \n",
       "1         katy perry 2022-10-29 07:01:46+00:00   \n",
       "3         katy perry 2022-10-27 19:07:44+00:00   \n",
       "4         katy perry 2022-10-27 19:07:21+00:00   \n",
       "5         katy perry 2022-10-27 19:05:27+00:00   \n",
       "...              ...                       ...   \n",
       "496999       juicy j 2022-03-17 15:04:10+00:00   \n",
       "497228  manu gavassi 2021-10-30 02:13:17+00:00   \n",
       "497240  manu gavassi 2021-09-28 22:29:07+00:00   \n",
       "497285  manu gavassi 2021-08-13 23:31:10+00:00   \n",
       "497395  manu gavassi 2020-09-30 04:33:31+00:00   \n",
       "\n",
       "                                                  content  retweetCount  \\\n",
       "0       if you wanna know why any human is they way th...          4575   \n",
       "1       wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...           881   \n",
       "3       heck I pour beer out of my tits (that‚Äôs a part...           297   \n",
       "4       The show‚Äôs set list is a fun üé¢  through memory...           336   \n",
       "5       Welcoming all my #flatearthers #spaceisfakers ...          2318   \n",
       "...                                                   ...           ...   \n",
       "496999  Block that devil out your ear this morning  #s...           148   \n",
       "497228                                            Spaces?            91   \n",
       "497240  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...           306   \n",
       "497285                                           .ebaselE           142   \n",
       "497395                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è          1225   \n",
       "\n",
       "        likeCount                                    renderedContent  \\\n",
       "0           29840  if you wanna know why any human is they way th...   \n",
       "1           10502  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3            3042  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4            3654  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5           20819  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...           ...                                                ...   \n",
       "496999        511  Block that devil out your ear this morning  #s...   \n",
       "497228       2126                                            Spaces?   \n",
       "497240       4395  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285       2937                                           .ebaselE   \n",
       "497395      13170                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "               handle  type  \n",
       "0           katyperry  ENFJ  \n",
       "1           katyperry  ENFJ  \n",
       "3           katyperry  ENFJ  \n",
       "4           katyperry  ENFJ  \n",
       "5           katyperry  ENFJ  \n",
       "...               ...   ...  \n",
       "496999  therealjuicyj  ESFP  \n",
       "497228    manugavassi  ENFP  \n",
       "497240    manugavassi  ENFP  \n",
       "497285    manugavassi  ENFP  \n",
       "497395    manugavassi  ENFP  \n",
       "\n",
       "[254650 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dataframe.columns.to_frame().T)\n",
    "cols=['type','name','renderedContent','content','handle','date','lang','likeCount','retweetCount']\n",
    "keep=dataframe[cols];display(len(keep))\n",
    "keep=keep[keep['lang']=='en'];display(len(keep))\n",
    "cols=list(set(keep.columns)-{'lang'})\n",
    "keep=keep[cols]\n",
    "keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquire is done \n",
    "\n",
    "\n",
    "## Prep is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1=keep[['type','name','content']].groupby(by=['type','name'])\n",
    "lista=set(group1.groups.keys())\n",
    "group2=keep[['type','name']].groupby(by=['type'])\n",
    "listb=list(set(group2.groups.keys()))\n",
    "group3=keep[['name','content']].groupby(by=['name'])   \n",
    "indexbyperson={}\n",
    "for b in listb:\n",
    "    g=list(group2.get_group(b).index)\n",
    "    n=list(group2.get_group(b).name.unique())\n",
    "    \n",
    "    ndict={}\n",
    "    for i in n:\n",
    "        k=list(group3.get_group(i).index)\n",
    "        c=list(group3.get_group(i).content)\n",
    "        ndict.update({i:{'index':k,'content':c}})\n",
    "    indexbyperson.update({b:{'index':g,'name':ndict}})\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# namegroups=allthewaydown.groupby('name')\n",
    "# keys=namegroups.groups.keys()\n",
    "# for k in keys:\n",
    "#     print(len(namegroups.get_group(k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "#pdf plumber\n",
    "#csv.preview\n",
    "import pandas as pd\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stopwords = ['like', 'im', 'think', 'dont', 'people', 'know', 'one', 'get', 'really','thing',\n",
    "                  'would', 'time', 'type', 'make', 'friend', 'ive', 'much','amp','twitter',\n",
    "                 'say', 'way', 'see', 'thing', 'want', 'thing', 'good', 'something', 'lot',\n",
    "                  'also', 'go', 'always', 'even', 'well', 'someone','https','http','com','co',',',\"'\"]\n",
    "\n",
    "\n",
    "\n",
    "stops=stopwords.words(['french','german','english','spanish','portuguese'])+ more_stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(stops,'stopwords.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "    stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "   \n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops and len(x)>=2), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    " \n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "# Converting emojis to words\n",
    "def convert_emojis(text):\n",
    "    for emot in EMOJI_UNICODE:\n",
    "        text = text.replace(emot, \"_\".join(EMOJI_UNICODE[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        text = text.replace(':','').replace('_',' ')\n",
    "    return text\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = re.sub(re.escape(emot),EMOTICONS_EMO[emot],text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''   \n",
    "    Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    '''\n",
    "    newtext = convert_emojis(convert_emoticons(text))\n",
    "    \n",
    "    #make a copy and begin to transform it\n",
    "    newtext = newtext.lower()\n",
    "\n",
    "    #encode into ascii then decode\n",
    "    newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "\n",
    "    #use re.sub to remove special characters\n",
    "    newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    return newtext\n",
    "\n",
    "    \n",
    "def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    \n",
    "      Takes text, tokenizes it, lemmatizes it\n",
    "      lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "      needs to be commented out after the first run (up to modeling)\n",
    "      # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "      needs to be un commented commented\n",
    "     \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "    # do basic clean on text and translate emojis/emoticons\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "\n",
    "    #make ready the lemmatizer object\n",
    "    newtext=tokenizer(newtext,regexfilter=regexfilter)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "    # lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return lemmafiltered\n",
    "    \n",
    "    \n",
    "def split_apply_join(funct,listobj):\n",
    "    'helperfuction letters'\n",
    "\n",
    "    mapped=map(funct, listobj)\n",
    "    mapped=list(mapped)\n",
    "    mapped=''.join(mapped)\n",
    "  \n",
    "    return mapped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    ''' \n",
    "    For a large file just save it locally\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "    #make ready tokenizer object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "    return newtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_list = []\n",
    "def domainmapper(spot):\n",
    "        if (spot == 'intj') | (spot == 'entj') | (spot == 'intp') | (spot == 'entp'):\n",
    "            return 'analyst'\n",
    "        elif (spot == 'infj') | (spot == 'enfj') | (spot == 'infp') | (spot == 'enfp'):\n",
    "            return 'diplomat'\n",
    "        elif (spot == 'istj') | (spot == 'estj') | (spot == 'isfj') | (spot == 'esfj'):\n",
    "            return 'sentinel'\n",
    "        elif (spot == 'istp') | (spot == 'estp') | (spot == 'isfp') | (spot == 'esfp'):\n",
    "            return 'explorer'\n",
    "        # else:\n",
    "        #     new_list.append('other')\n",
    "\n",
    "\n",
    "\n",
    "def pairwiseattributemapper(df):\n",
    "    x=df['type']\n",
    "    i_e={}\n",
    "\n",
    "    n_s={}\n",
    "\n",
    "    t_f={}\n",
    "\n",
    "    j_p={}\n",
    "\n",
    "\n",
    "    for spot in x:\n",
    "        # print(spot)\n",
    "        # print(type(spot))\n",
    "        if (spot[0]=='i')|(spot[0]=='e'):\n",
    "            if spot[0]=='i':\n",
    "                i_e.update({spot:'i'})\n",
    "            else:\n",
    "                i_e.update({spot:'e'})    \n",
    "        if (spot[1]=='n')|(spot[1]=='s'):\n",
    "            if spot[1]=='n':\n",
    "                n_s.update({spot:'n'})\n",
    "            else:\n",
    "                n_s.update({spot:'s'})    \n",
    "        if (spot[2]=='t')|(spot[2]=='f'):\n",
    "            if spot[2]=='t':\n",
    "                t_f.update({spot:'t'})\n",
    "            else:\n",
    "                t_f.update({spot:'f'})    \n",
    "        if (spot[3]=='j')|(spot[3]=='p'):\n",
    "            if spot[3]=='j':\n",
    "                j_p.update({spot:'j'})\n",
    "            else:\n",
    "                j_p.update({spot:'p'})    \n",
    "    df['i|e']=x.map(i_e)\n",
    "    df['n|s']=x.map(n_s)\n",
    "    df['t|f']=x.map(t_f)\n",
    "    df['j|p']=x.map(j_p)\n",
    "    return df\n",
    "    \n",
    "       \n",
    "        # else:\n",
    "        #     new_list.append('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tryagain\u001b[38;5;241m=\u001b[39m\u001b[43mkeep\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrenderedContent\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhandle\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlikeCount\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretweetCount\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      2\u001b[0m tryagain\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m=\u001b[39mtryagain\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()        \n\u001b[1;32m      4\u001b[0m tryagain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mtryagain[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrenderedContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keep' is not defined"
     ]
    }
   ],
   "source": [
    "tryagain=keep[['type','name','renderedContent','handle','date','likeCount','retweetCount']]\n",
    "tryagain.type=tryagain.type.str.lower()        \n",
    "\n",
    "tryagain['content']=tryagain['renderedContent']\n",
    "tryagain=tryagain[['type','name','content','handle','date','likeCount','retweetCount']]\n",
    "tryagain.drop_duplicates(inplace=True)\n",
    "tryagain.dropna(inplace=True)\n",
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tryagain['domain'] = tryagain['type'].apply(domainmapper)\n",
    "tryagain=pairwiseattributemapper(tryagain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>domain</th>\n",
       "      <th>i|e</th>\n",
       "      <th>n|s</th>\n",
       "      <th>t|f</th>\n",
       "      <th>j|p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type        name                                            content  \\\n",
       "0  enfj  katy perry  if you wanna know why any human is they way th...   \n",
       "1  enfj  katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3  enfj  katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4  enfj  katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5  enfj  katy perry  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "\n",
       "      handle                      date  likeCount  retweetCount    domain i|e  \\\n",
       "0  katyperry 2022-10-30 22:15:09+00:00      29840          4575  diplomat   e   \n",
       "1  katyperry 2022-10-29 07:01:46+00:00      10502           881  diplomat   e   \n",
       "3  katyperry 2022-10-27 19:07:44+00:00       3042           297  diplomat   e   \n",
       "4  katyperry 2022-10-27 19:07:21+00:00       3654           336  diplomat   e   \n",
       "5  katyperry 2022-10-27 19:05:27+00:00      20819          2318  diplomat   e   \n",
       "\n",
       "  n|s t|f j|p  \n",
       "0   n   f   j  \n",
       "1   n   f   j  \n",
       "3   n   f   j  \n",
       "4   n   f   j  \n",
       "5   n   f   j  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain = pd.read_pickle('tryagain.pkl')\n",
    "tryagain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = lemmatizor(tryagain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>domain</th>\n",
       "      <th>i|e</th>\n",
       "      <th>n|s</th>\n",
       "      <th>t|f</th>\n",
       "      <th>j|p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>esfp</td>\n",
       "      <td>juicy j</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>511</td>\n",
       "      <td>148</td>\n",
       "      <td>explorer</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>2126</td>\n",
       "      <td>91</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>4395</td>\n",
       "      <td>306</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>2937</td>\n",
       "      <td>142</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>13170</td>\n",
       "      <td>1225</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182352 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type          name                                            content  \\\n",
       "0       enfj    katy perry  if you wanna know why any human is they way th...   \n",
       "1       enfj    katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3       enfj    katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4       enfj    katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5       enfj    katy perry  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...      ...           ...                                                ...   \n",
       "496999  esfp       juicy j  Block that devil out your ear this morning  #s...   \n",
       "497228  enfp  manu gavassi                                            Spaces?   \n",
       "497240  enfp  manu gavassi  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285  enfp  manu gavassi                                           .ebaselE   \n",
       "497395  enfp  manu gavassi                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "               handle                      date  likeCount  retweetCount  \\\n",
       "0           katyperry 2022-10-30 22:15:09+00:00      29840          4575   \n",
       "1           katyperry 2022-10-29 07:01:46+00:00      10502           881   \n",
       "3           katyperry 2022-10-27 19:07:44+00:00       3042           297   \n",
       "4           katyperry 2022-10-27 19:07:21+00:00       3654           336   \n",
       "5           katyperry 2022-10-27 19:05:27+00:00      20819          2318   \n",
       "...               ...                       ...        ...           ...   \n",
       "496999  therealjuicyj 2022-03-17 15:04:10+00:00        511           148   \n",
       "497228    manugavassi 2021-10-30 02:13:17+00:00       2126            91   \n",
       "497240    manugavassi 2021-09-28 22:29:07+00:00       4395           306   \n",
       "497285    manugavassi 2021-08-13 23:31:10+00:00       2937           142   \n",
       "497395    manugavassi 2020-09-30 04:33:31+00:00      13170          1225   \n",
       "\n",
       "          domain i|e n|s t|f j|p  \n",
       "0       diplomat   e   n   f   j  \n",
       "1       diplomat   e   n   f   j  \n",
       "3       diplomat   e   n   f   j  \n",
       "4       diplomat   e   n   f   j  \n",
       "5       diplomat   e   n   f   j  \n",
       "...          ...  ..  ..  ..  ..  \n",
       "496999  explorer   e   s   f   p  \n",
       "497228  diplomat   e   n   f   p  \n",
       "497240  diplomat   e   n   f   p  \n",
       "497285  diplomat   e   n   f   p  \n",
       "497395  diplomat   e   n   f   p  \n",
       "\n",
       "[182352 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(tryagain)\n",
    "\n",
    "toplevelGrouplist=['type','domain','i|e','n|s','t|f','j|p']\n",
    "for topGru in toplevelGrouplist:    \n",
    "    types=tryagain.groupby([f'{topGru}','name'])\n",
    "    typekeys=types.groups.keys()\n",
    "    for key in typekeys:\n",
    "        df=types.get_group(key)\n",
    "        namegroup=df.groupby('name')\n",
    "        namekeys=namegroup.groups.keys()\n",
    "        for nm in namekeys:\n",
    "            # print(nm)\n",
    "            celeb=namegroup.get_group(nm)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryagain['words_in_tweet'] = tryagain.content.str.split(' ').apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryagain['char_in_tweet'] = tryagain.content.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "tryagain['sentiment'] = tryagain.content.apply(lambda doc: s.polarity_scores(doc)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182352, 15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "esfp    30609\n",
       "enfp    18955\n",
       "esfj    17908\n",
       "entp    15779\n",
       "isfp    15555\n",
       "estp    15106\n",
       "enfj    10801\n",
       "istp    10709\n",
       "isfj    10249\n",
       "infj     7796\n",
       "entj     7771\n",
       "estj     7087\n",
       "intj     4072\n",
       "infp     3944\n",
       "intp     3355\n",
       "istj     2656\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emot.emo_unicode import EMOJI_UNICODE, EMOTICONS_EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anggun official'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Anggun_Cipta'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "31232"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4890"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'diplomat'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'f'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'j'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['love hearing voice dubbed german artede artefr annoyed uneasy hesitant',\n",
       " 'biggest muslim country world come muslim family father author wanted children open tolerant mind thats put us catholic school artede artefr annoyed uneasy hesitant',\n",
       " 'great news concerts bologna jan sold however due logistic issues dates foligno genova changed feb date grosseto shortly book foligno urlz fr genova urlz fr annoyed uneasy hesitant',\n",
       " 'wear gown big world premiere feb jkt bkk axnasia annoyed uneasy hesitant great',\n",
       " 'change date foligno concert cant wait meet february san domenico tickets gt urlz fr annoyed uneasy hesitant',\n",
       " 'concert bologna crowd change date genova concert due issues cant wait feb genova tickets urlz fr concert dates italy shortly annoyed uneasy hesitant',\n",
       " 'mazy',\n",
       " 'feeling blue wait purple axnasia annoyed uneasy hesitant',\n",
       " 'smile curve sets straight axnasia annoyed uneasy hesitant',\n",
       " 'cant wait premiere february guys axnasia annoyed uneasy hesitant',\n",
       " 'thevoice annoyed uneasy hesitant',\n",
       " 'sisters around world annoyed uneasy hesitant',\n",
       " 'fight join us vimeo',\n",
       " 'singer songs goal music played major internet use work without fair share new law crucial vimeo',\n",
       " 'duo maestro premiere thursday results show cant wait hear blessed honored humbled annoyed uneasy hesitant',\n",
       " 'caruso virtual duet maestro results show annoyed uneasy hesitant',\n",
       " 'watch excerpt live dame paris charity concert aired french national annoyed uneasy hesitant',\n",
       " 'watch snippet singing iconic french chanson prime tv show french annoyed uneasy hesitant',\n",
       " 'italian fans amazing alpe adria arena night concert july annoyed uneasy hesitant',\n",
       " 'sampai ketemu bulan juli di jakarta boys seeing july tennis indoor jakarta july annoyed uneasy hesitant',\n",
       " 'wildlife warrior works since orang utan species forest borneo home dayak whose land reduced massive palm tree annoyed uneasy hesitant',\n",
       " 'shooting ong asf annoyed uneasy hesitant',\n",
       " 'latihan jakarta annoyed uneasy hesitant',\n",
       " 'new youth youth annoyed uneasy hesitant',\n",
       " 'thank harpers august edition cover annoyed uneasy hesitant',\n",
       " 'harpers cover august edition grab copy annoyed uneasy hesitant',\n",
       " 'pesinden javanese word singer take pride great serge used music minor art theres truth pretend art try write songs touch annoyed uneasy hesitant',\n",
       " 'songs french album toujours ailleurs didnt expect wasnt released single become dance anthem america thanks djs dance us support years annoyed uneasy hesitant',\n",
       " 'years old today still feels surreal wax figure never life ever statue grateful touched honor heres wishing twin sister immortal life hair flawless skin annoyed uneasy hesitant',\n",
       " 'wow middle france tv getting fabulous news america dance charts us keep dancing annoyed uneasy hesitant',\n",
       " 'sold soon jakarta annoyed uneasy hesitant',\n",
       " 'feel',\n",
       " 'true easy find truth takes effort believe believe',\n",
       " 'aaaw bless',\n",
       " 'yrs ago released last french album toujours ailleurs went gold went tour year around france western europe seeing happy faces fans brings biggest joy without music nothing annoyed uneasy hesitant',\n",
       " 'wait turkey',\n",
       " 'rather banana gaffa taped id rather talk insanely vergine vellata veiled virgin giovanni strazza made entirely marble imagine skill savoir faire achieve veil marble annoyed uneasy hesitant',\n",
       " 'roxette songs teenage life thank music annoyed uneasy hesitant',\n",
       " 'share annoyed uneasy hesitant',\n",
       " 'tbt print ad campaign annoyed uneasy hesitant',\n",
       " 'soon amazing nights jan part asian awards wait perform talents asia grab tickets bit ly annoyed uneasy hesitant',\n",
       " 'thank honor annoyed uneasy hesitant',\n",
       " 'annoyed uneasy hesitant',\n",
       " 'kpai uk news latest',\n",
       " 'running stop climate change meeting world leaders year could last chance donate biggest climate ever society save planet donate chuffed org project climate annoyed uneasy hesitant',\n",
       " 'read kind harvard review us part taking music medium lim woojin pa part ii world medium lim woojin pa part iii stories bus stops medium lim woojin ex annoyed uneasy hesitant',\n",
       " 'season miss annoyed uneasy hesitant',\n",
       " 'afraid afraid world afraid help afraid afraid god american god american bowie annoyed uneasy hesitant',\n",
       " 'take look lawman beating wrong guy oh wonder ever best selling show life mars bowies songs accuracy todays world annoyed uneasy hesitant',\n",
       " 'shes polite young lady guess pays',\n",
       " 'dreams van halen grew vanhalen cover many songs stage rock days eddie van halen greatest guitar players guitar hero annoyed uneasy hesitant',\n",
       " 'tbt missing asias got talent fellow judges annoyed uneasy hesitant',\n",
       " 'axn stars tv show watch unique filmed home normandy midst nature forget tune saturday nov jkt bkk axn axnasia annoyed uneasy hesitant',\n",
       " 'alsublet bises fort',\n",
       " 'ready axn stars tv show axn asia stars annoyed uneasy hesitant',\n",
       " 'toute takes annoyed uneasy hesitant',\n",
       " 'miss axn stars nov jkt bkk axnasia facebook youtube page brought republic ministry tourism creative economy kemen parekraf annoyed uneasy hesitant',\n",
       " 'fans join live axnasia facebook page fun catch nov jkt bkk ill live session share show annoyed uneasy hesitant',\n",
       " 'galafr mothers tongue french third language learnt arrived paris',\n",
       " 'miss catch axn stars axnasia facebook youtube page november jkt bkk annoyed uneasy hesitant',\n",
       " 'hiiiiiii forget catch tonight axnasia facebook youtube page jakarta bangkok annoyed uneasy hesitant',\n",
       " 'talk youtu annoyed uneasy hesitant',\n",
       " 'tanah toraja lake toba many places cultures need discover youtu annoyed uneasy hesitant',\n",
       " 'left right glowup annoyed uneasy hesitant',\n",
       " 'honor sing basilica di san assisi brothers sisters peace men earth annoyed uneasy hesitant',\n",
       " 'new cover annoyed uneasy hesitant',\n",
       " 'broken wings mr mister tonights pyjama session favorite human enjoy annoyed uneasy hesitant',\n",
       " 'gong xi fa cai kung fat choi may year ox bring health luck annoyed uneasy hesitant',\n",
       " 'kangen maghrib di jakarta husband took gorgeous photos pre pandemic sunset prayer calls hope soon back annoyed uneasy hesitant',\n",
       " 'proud invited perform bravo music awards bolshoi theatre moscow russia annoyed uneasy hesitant',\n",
       " 'sing bolshoi theater omg wish luck guys annoyed uneasy hesitant',\n",
       " 'got award bravo music awards duet year bolshoi theater last night beyond grateful annoyed uneasy hesitant',\n",
       " 'thank dear bruce hope',\n",
       " 'marhaban ramadhan muslim brothers sisters annoyed uneasy hesitant',\n",
       " 'honored parco della musica rome june support world blood donor day blood donation helps saves millions across world daily give details soon annoyed uneasy hesitant',\n",
       " 'support world blood donor day june rome annoyed uneasy hesitant',\n",
       " 'night remember singing grande maestro grateful annoyed uneasy hesitant',\n",
       " 'fans italy annoyed uneasy hesitant',\n",
       " 'maestro stage evening annoyed uneasy hesitant',\n",
       " 'years old today flies day still touched cant help feeling proud thank love annoyed uneasy hesitant',\n",
       " 'rharrys yuk',\n",
       " 'slowly back stage pandemic concerts sassuolo aquileia super happy thank italy youll heart annoyed uneasy hesitant',\n",
       " 'music annoyed uneasy hesitant',\n",
       " 'wil sing vatican annoyed uneasy hesitant',\n",
       " 'singing malam kudus silent night language opening concerto di natale vatican makes moment special talented annoyed uneasy hesitant',\n",
       " 'beyond honoured met pope francis hes soft spoken looks right eyes smiles feel love humanity kindness moment pontifex annoyed uneasy hesitant',\n",
       " 'kaget annoyed uneasy hesitant',\n",
       " 'seal alsublet gorgeous voice everrrrr',\n",
       " 'minal aidin wal faizin mohon maaf lahir dan bathin eid mubarak muslim brothers sisters may peace upon us annoyed uneasy hesitant',\n",
       " 'night remember red carpet wearing husband wears zegna annoyed uneasy hesitant',\n",
       " 'italian fans wait city piacenza special concert benefit african mission charity august book tickets artist anggun annoyed uneasy hesitant',\n",
       " 'demain tomorrow besok annoyed uneasy hesitant',\n",
       " 'fans italy cant wait night concert parco delle rose august annoyed uneasy hesitant',\n",
       " 'swiss lies secret beauty youth niance swiss glacier complex goes deep deep skin youthful skin niance annoyed uneasy hesitant',\n",
       " 'rocked stage wacken tonight queen annoyed uneasy hesitant',\n",
       " 'happy duet super talented watch video amazing track stream share enjoy links annoyed uneasy hesitant',\n",
       " 'elihallo behind scene photos music video elihallo video share us comments happy face smiley links annoyed uneasy hesitant']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "annoyed     83\n",
       "uneasy      83\n",
       "hesitant    83\n",
       "world       10\n",
       "wait         9\n",
       "            ..\n",
       "banana       1\n",
       "turkey       1\n",
       "nothing      1\n",
       "joy          1\n",
       "smiley       1\n",
       "Length: 592, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['love hearing voice dubbed german artede artefr annoyed uneasy hesitant',\n",
       " 'biggest muslim country world come muslim family father author wanted children open tolerant mind thats put us catholic school artede artefr annoyed uneasy hesitant',\n",
       " 'great news concerts bologna jan sold however due logistic issues dates foligno genova changed feb date grosseto shortly book foligno urlz fr genova urlz fr annoyed uneasy hesitant',\n",
       " 'wear gown big world premiere feb jkt bkk axnasia annoyed uneasy hesitant great',\n",
       " 'change date foligno concert cant wait meet february san domenico tickets gt urlz fr annoyed uneasy hesitant',\n",
       " 'concert bologna crowd change date genova concert due issues cant wait feb genova tickets urlz fr concert dates italy shortly annoyed uneasy hesitant',\n",
       " 'mazy',\n",
       " 'feeling blue wait purple axnasia annoyed uneasy hesitant',\n",
       " 'smile curve sets straight axnasia annoyed uneasy hesitant',\n",
       " 'cant wait premiere february guys axnasia annoyed uneasy hesitant',\n",
       " 'thevoice annoyed uneasy hesitant',\n",
       " 'sisters around world annoyed uneasy hesitant',\n",
       " 'fight join us vimeo',\n",
       " 'singer songs goal music played major internet use work without fair share new law crucial vimeo',\n",
       " 'duo maestro premiere thursday results show cant wait hear blessed honored humbled annoyed uneasy hesitant',\n",
       " 'caruso virtual duet maestro results show annoyed uneasy hesitant',\n",
       " 'watch excerpt live dame paris charity concert aired french national annoyed uneasy hesitant',\n",
       " 'watch snippet singing iconic french chanson prime tv show french annoyed uneasy hesitant',\n",
       " 'italian fans amazing alpe adria arena night concert july annoyed uneasy hesitant',\n",
       " 'sampai ketemu bulan juli di jakarta boys seeing july tennis indoor jakarta july annoyed uneasy hesitant',\n",
       " 'wildlife warrior works since orang utan species forest borneo home dayak whose land reduced massive palm tree annoyed uneasy hesitant',\n",
       " 'shooting ong asf annoyed uneasy hesitant',\n",
       " 'latihan jakarta annoyed uneasy hesitant',\n",
       " 'new youth youth annoyed uneasy hesitant',\n",
       " 'thank harpers august edition cover annoyed uneasy hesitant',\n",
       " 'harpers cover august edition grab copy annoyed uneasy hesitant',\n",
       " 'pesinden javanese word singer take pride great serge used music minor art theres truth pretend art try write songs touch annoyed uneasy hesitant',\n",
       " 'songs french album toujours ailleurs didnt expect wasnt released single become dance anthem america thanks djs dance us support years annoyed uneasy hesitant',\n",
       " 'years old today still feels surreal wax figure never life ever statue grateful touched honor heres wishing twin sister immortal life hair flawless skin annoyed uneasy hesitant',\n",
       " 'wow middle france tv getting fabulous news america dance charts us keep dancing annoyed uneasy hesitant',\n",
       " 'sold soon jakarta annoyed uneasy hesitant',\n",
       " 'feel',\n",
       " 'true easy find truth takes effort believe believe',\n",
       " 'aaaw bless',\n",
       " 'yrs ago released last french album toujours ailleurs went gold went tour year around france western europe seeing happy faces fans brings biggest joy without music nothing annoyed uneasy hesitant',\n",
       " 'wait turkey',\n",
       " 'rather banana gaffa taped id rather talk insanely vergine vellata veiled virgin giovanni strazza made entirely marble imagine skill savoir faire achieve veil marble annoyed uneasy hesitant',\n",
       " 'roxette songs teenage life thank music annoyed uneasy hesitant',\n",
       " 'share annoyed uneasy hesitant',\n",
       " 'tbt print ad campaign annoyed uneasy hesitant',\n",
       " 'soon amazing nights jan part asian awards wait perform talents asia grab tickets bit ly annoyed uneasy hesitant',\n",
       " 'thank honor annoyed uneasy hesitant',\n",
       " 'annoyed uneasy hesitant',\n",
       " 'kpai uk news latest',\n",
       " 'running stop climate change meeting world leaders year could last chance donate biggest climate ever society save planet donate chuffed org project climate annoyed uneasy hesitant',\n",
       " 'read kind harvard review us part taking music medium lim woojin pa part ii world medium lim woojin pa part iii stories bus stops medium lim woojin ex annoyed uneasy hesitant',\n",
       " 'season miss annoyed uneasy hesitant',\n",
       " 'afraid afraid world afraid help afraid afraid god american god american bowie annoyed uneasy hesitant',\n",
       " 'take look lawman beating wrong guy oh wonder ever best selling show life mars bowies songs accuracy todays world annoyed uneasy hesitant',\n",
       " 'shes polite young lady guess pays',\n",
       " 'dreams van halen grew vanhalen cover many songs stage rock days eddie van halen greatest guitar players guitar hero annoyed uneasy hesitant',\n",
       " 'tbt missing asias got talent fellow judges annoyed uneasy hesitant',\n",
       " 'axn stars tv show watch unique filmed home normandy midst nature forget tune saturday nov jkt bkk axn axnasia annoyed uneasy hesitant',\n",
       " 'alsublet bises fort',\n",
       " 'ready axn stars tv show axn asia stars annoyed uneasy hesitant',\n",
       " 'toute takes annoyed uneasy hesitant',\n",
       " 'miss axn stars nov jkt bkk axnasia facebook youtube page brought republic ministry tourism creative economy kemen parekraf annoyed uneasy hesitant',\n",
       " 'fans join live axnasia facebook page fun catch nov jkt bkk ill live session share show annoyed uneasy hesitant',\n",
       " 'galafr mothers tongue french third language learnt arrived paris',\n",
       " 'miss catch axn stars axnasia facebook youtube page november jkt bkk annoyed uneasy hesitant',\n",
       " 'hiiiiiii forget catch tonight axnasia facebook youtube page jakarta bangkok annoyed uneasy hesitant',\n",
       " 'talk youtu annoyed uneasy hesitant',\n",
       " 'tanah toraja lake toba many places cultures need discover youtu annoyed uneasy hesitant',\n",
       " 'left right glowup annoyed uneasy hesitant',\n",
       " 'honor sing basilica di san assisi brothers sisters peace men earth annoyed uneasy hesitant',\n",
       " 'new cover annoyed uneasy hesitant',\n",
       " 'broken wings mr mister tonights pyjama session favorite human enjoy annoyed uneasy hesitant',\n",
       " 'gong xi fa cai kung fat choi may year ox bring health luck annoyed uneasy hesitant',\n",
       " 'kangen maghrib di jakarta husband took gorgeous photos pre pandemic sunset prayer calls hope soon back annoyed uneasy hesitant',\n",
       " 'proud invited perform bravo music awards bolshoi theatre moscow russia annoyed uneasy hesitant',\n",
       " 'sing bolshoi theater omg wish luck guys annoyed uneasy hesitant',\n",
       " 'got award bravo music awards duet year bolshoi theater last night beyond grateful annoyed uneasy hesitant',\n",
       " 'thank dear bruce hope',\n",
       " 'marhaban ramadhan muslim brothers sisters annoyed uneasy hesitant',\n",
       " 'honored parco della musica rome june support world blood donor day blood donation helps saves millions across world daily give details soon annoyed uneasy hesitant',\n",
       " 'support world blood donor day june rome annoyed uneasy hesitant',\n",
       " 'night remember singing grande maestro grateful annoyed uneasy hesitant',\n",
       " 'fans italy annoyed uneasy hesitant',\n",
       " 'maestro stage evening annoyed uneasy hesitant',\n",
       " 'years old today flies day still touched cant help feeling proud thank love annoyed uneasy hesitant',\n",
       " 'rharrys yuk',\n",
       " 'slowly back stage pandemic concerts sassuolo aquileia super happy thank italy youll heart annoyed uneasy hesitant',\n",
       " 'music annoyed uneasy hesitant',\n",
       " 'wil sing vatican annoyed uneasy hesitant',\n",
       " 'singing malam kudus silent night language opening concerto di natale vatican makes moment special talented annoyed uneasy hesitant',\n",
       " 'beyond honoured met pope francis hes soft spoken looks right eyes smiles feel love humanity kindness moment pontifex annoyed uneasy hesitant',\n",
       " 'kaget annoyed uneasy hesitant',\n",
       " 'seal alsublet gorgeous voice everrrrr',\n",
       " 'minal aidin wal faizin mohon maaf lahir dan bathin eid mubarak muslim brothers sisters may peace upon us annoyed uneasy hesitant',\n",
       " 'night remember red carpet wearing husband wears zegna annoyed uneasy hesitant',\n",
       " 'italian fans wait city piacenza special concert benefit african mission charity august book tickets artist anggun annoyed uneasy hesitant',\n",
       " 'demain tomorrow besok annoyed uneasy hesitant',\n",
       " 'fans italy cant wait night concert parco delle rose august annoyed uneasy hesitant',\n",
       " 'swiss lies secret beauty youth niance swiss glacier complex goes deep deep skin youthful skin niance annoyed uneasy hesitant',\n",
       " 'rocked stage wacken tonight queen annoyed uneasy hesitant',\n",
       " 'happy duet super talented watch video amazing track stream share enjoy links annoyed uneasy hesitant',\n",
       " 'elihallo behind scene photos music video elihallo video share us comments happy face smiley links annoyed uneasy hesitant']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain=pd.read_pickle('tryagain.pkl')  \n",
    "types=tryagain.groupby(['type','name'])    \n",
    "typekeys=list(types.groups.keys())\n",
    "key=typekeys[0]\n",
    "df=types.get_group(key).sort_values(by='date')\n",
    "# display(key,len(df),df.columns.tolist(),print('\\n\\n'))\n",
    "# display(df)\n",
    "name=list(df['name'].unique())[0];display(name)\n",
    "handle=list(df['handle'].unique())[0];display(handle)\n",
    "date=list(df['date'].unique());#display(date)\n",
    "likeCount=(df['likeCount'].unique()).sum();display(likeCount)\n",
    "retweetCount=(df['retweetCount'].unique()).sum();display(retweetCount)\n",
    "domain=list(df['domain'].unique())[0];display(domain)\n",
    "i_e=df['i|e'].unique().tolist()[0];display(i_e)\n",
    "n_s=df['n|s'].unique().tolist()[0];display(n_s)\n",
    "t_f=df['t|f'].unique().tolist()[0];display(t_f)\n",
    "j_p=df['j|p'].unique().tolist()[0];display(j_p)\n",
    "docs=df['content'].unique()\n",
    "\n",
    "\n",
    "\n",
    "# splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "display(lemma)\n",
    "full_list=\" \".join(lemma).split()\n",
    "full_list=[stopfilter(i) for i in full_list]\n",
    "\n",
    "\n",
    "\n",
    "valcounts=pd.Series(full_list).value_counts();display(valcounts)\n",
    "freq=list(valcounts.values)\n",
    "\n",
    "\n",
    "##TF-IDF calc \n",
    "#this logic needs to be verified\n",
    "# x=np.log10((len(lemma)/freq))\n",
    "# x*=freq\n",
    "# x=np.ndarray.round(x,3)\n",
    "# tf_idf=(dict(zip((pd.Series(full_list).value_counts().index),x)))\n",
    "# freqtable=dict(valcounts)\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryagain.to_csv('clean_tweet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    # create train and test (80/20 split) from the orginal dataframe\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=123, stratify=df.type)\n",
    "    # create train and validate (75/25 split) from the train dataframe\n",
    "    train, val = train_test_split(train, test_size=.25, random_state=123, stratify=train.type)\n",
    "    \n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def allthewaydownfunc(df):\n",
    "\n",
    "    groups=['type', 'domain', 'i|e', 'n|s', 't|f', 'j|p']\n",
    "\n",
    "\n",
    "    allthewaydict={'group':{},'subgroup':{},'name':{},'docs':{},'lemmatized docs':{},'tf-idf w.r.t name':{},'freq table w.r.t name':{},'handle':{},'date':{},'lang'\t:{},'likeCount':{},'retweetCount':{}}\n",
    "    # groups=(groups[2:4])\n",
    "    count=0\n",
    "    for g in groups:\n",
    "        print(f'{g}\\n')\n",
    "        group=tryagain.groupby(g)\n",
    "        keys=group.groups.keys()\n",
    "        for key in keys:\n",
    "            # print(key)\n",
    "            kthgroup=group.get_group(key)\n",
    "            namegru=kthgroup.groupby('name')\n",
    "            subkeys=namegru.groups.keys()\n",
    "\n",
    "\n",
    "            for sk in subkeys:\n",
    "                curname=namegru.get_group(sk)        \n",
    "                nm=curname.name.unique()[0]\n",
    "                docs=curname.content.values\n",
    "                # splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "                lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "                lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "                fulllist=\" \".join(lemma).split()\n",
    "\n",
    "\n",
    "                valcounts=pd.Series(fulllist).value_counts()\n",
    "                freq=valcounts.values\n",
    "\n",
    "\n",
    "                ##TF-IDF calc\n",
    "                x=np.log10((len(lemma)/freq))\n",
    "                x*=freq\n",
    "                x=np.ndarray.round(x,3)\n",
    "                tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n",
    "                freqtable=dict(valcounts)\n",
    "\n",
    "                # df=idfDf(lemma);display(df)\n",
    "                allthewaydict['group'].update({count:g})\n",
    "                allthewaydict['subgroup'].update({count:key})\n",
    "                allthewaydict['name'].update({count:nm})\n",
    "                allthewaydict['docs'].update({count:list(docs)})\n",
    "                allthewaydict['lemmatized docs'].update({count:list(lemma)})\n",
    "                allthewaydict['tf-idf w.r.t name'].update({count:tf_idf})\n",
    "                allthewaydict['freq table w.r.t name'].update({count:freqtable})\n",
    "                allthewaydict['handle'].update({count:curname.handle.values[0]})\n",
    "                allthewaydict['date'].update({count:curname.date.values})\n",
    "                allthewaydict['lang'].update({count:curname.lang.values[0]})\n",
    "                allthewaydict['likeCount'].update({count:curname.likeCount.values.sum()})\n",
    "                allthewaydict['retweetCount'].update({count:curname.retweetCount.values.sum()})\n",
    "\n",
    "\n",
    "                count+=1\n",
    "\n",
    "\n",
    "    allthewaydown=pd.DataFrame(allthewaydict)\n",
    "\n",
    "\n",
    "    return allthewaydown\n",
    "\n",
    "\n",
    "\n",
    "def splitPrep(tryagain):\n",
    "    train, val, test=split_data(tryagain)\n",
    "    train=allthewaydownfunc(train)\n",
    "    val=allthewaydownfunc(val)\n",
    "    test=allthewaydownfunc(test)\n",
    "    pd.to_pickle(train,'mvp_plus_1_train.pkl')\n",
    "    pd.to_pickle(val,'mvp_plus_1_val.pkl')\n",
    "    pd.to_pickle(test,'mvp_plus_1_test.pkl')\n",
    " \n",
    "\n",
    " \n",
    "splitPrep(tryagain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_idf(word,**kwargs):\n",
    "#         documents =kwargs['docs']\n",
    "#         n_occurences = sum([1 for doc in documents if word in doc])\n",
    "#         return len(documents) / n_occurences\n",
    "\t\t\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get a list of the unique words\n",
    "# def idfDf(documents):\n",
    "#     unique_words = pd.Series(','.join(documents).split()).unique()\n",
    "#     kwargs={'docs':documents}\n",
    "#     # put the unique words into a data frame\n",
    "#     df=(pd.DataFrame(dict(word=unique_words))\n",
    "#      # calculate the idf for each word\n",
    "#      .assign(idf=lambda df: df.word.apply(calc_idf))\n",
    "#      # sort the data for presentation purposes\n",
    "#      .set_index('word')\n",
    "#      .sort_values(by='idf', ascending=False))\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# groups=['type', 'domain', 'i|e', 'n|s', 't|f', 'j|p']\n",
    "# allthewaydict={'group':{},'subgroup':{},'name':{},'docs':{},'lemmatized docs':{},'tf-idf w.r.t name':{},'freq table w.r.t name':{},'handle':{},'date':{},'lang'\t:{},'likeCount':{},'retweetCount':{}}\n",
    "# # groups=(groups[2:4])\n",
    "# count=0\n",
    "# g=groups[0]\n",
    "# print(f'{g}\\n')\n",
    "# group=tryagain.groupby(g)\n",
    "# keys=group.groups.keys()\n",
    "# # for key in keys:\n",
    "# key=list(keys)[0]\n",
    "# # print(key)\n",
    "# kthgroup=group.get_group(key)\n",
    "# namegru=kthgroup.groupby('name')\n",
    "# subkeys=namegru.groups.keys()\n",
    "# # for sk in subkeys:\n",
    "# #     count+=\n",
    "# curname=namegru.get_group(list(subkeys)[4])\n",
    "# display(curname)\n",
    "# nm=curname.name.unique()[0]\n",
    "# docs=curname.content.values\n",
    "# # splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "# lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "# lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "# fulllist=\" \".join(lemma).split()\n",
    "\n",
    "\n",
    "# valcounts=pd.Series(fulllist).value_counts()\n",
    "# freq=valcounts.values\n",
    "\n",
    "\n",
    "# ##TF-IDF calc\n",
    "# x=np.log10((len(lemma)/freq))\n",
    "# x*=freq\n",
    "# x=np.ndarray.round(x,3)\n",
    "# tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n",
    "# freqtable=dict(valcounts)\n",
    "\n",
    "# # df=idfDf(lemma);display(df)\n",
    "# allthewaydict['group'].update({count:g})\n",
    "# allthewaydict['subgroup'].update({count:key})\n",
    "# allthewaydict['name'].update({count:nm})\n",
    "# allthewaydict['docs'].update({count:docs})\n",
    "# allthewaydict['lemmatized docs'].update({count:lemma})\n",
    "# allthewaydict['tf-idf w.r.t name'].update({count:tf_idf})\n",
    "# allthewaydict['freq table w.r.t name'].update({count:freqtable})\n",
    "\n",
    "# allthewaydict['handle'].update({count:curname.handle.values[0]})\n",
    "# allthewaydict['date'].update({count:curname.date.values})\n",
    "# allthewaydict['lang'].update({count:curname.lang.values[0]})\n",
    "# allthewaydict['likeCount'].update({count:curname.likeCount.values.sum()})\n",
    "# allthewaydict['retweetCount'].update({count:curname.retweetCount.values.sum()})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# count+=1\n",
    "\n",
    "# pd.DataFrame(allthewaydict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep is finished \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explore is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tryagain=pd.read_pickle('mvp_plus_1_train.pkl')\n",
    "\n",
    "tryagain.columns\n",
    "\n",
    "\n",
    "\n",
    "# s = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "# df['sentiment'] = df.lemmatized.apply(lambda doc: s.polarity_scores(doc)['compound'])\n",
    "# df['message_length'] = df['lemmatized'].str.len()\n",
    "# df['word_count'] = (df['lemmatized'].str.split(' ').apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroupdict={\n",
    "'subgroup':{}, \n",
    "'names':{}, \n",
    "'docs':{}, \n",
    "'lemmatized docs':{},\n",
    "'handle':{}, \n",
    "'date':{}, \n",
    "'lang':{},\n",
    "'likeCount':{},\n",
    "'retweetCount':{},\n",
    "'docs w.r.t subgroup':{}, \n",
    "'lemmatized docs w.r.t subgroup':{},\n",
    "'freq table w.r.t subgroup':{},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "groups=tryagain.groupby('group')\n",
    "keys=groups.groups.keys()\n",
    "for k in keys:\n",
    "    cur=groups.get_group(k)\n",
    "    curgroup=cur.groupby(['group','subgroup'])\n",
    "    curkeys=curgroup.groups.keys()\n",
    "    \n",
    "    print('\\n')\n",
    "    print(k)\n",
    "    print('\\n')\n",
    "    for c in curkeys:\n",
    "        print('\\n')\n",
    "        print(c)\n",
    "        print('\\n')\n",
    "        subcur=cur=curgroup.get_group(c)\n",
    "        # display(subcur)\n",
    "        subgroupdict['subgroup'].update({count:subcur['subgroup'].values[0]})\n",
    "        subgroupdict['names'].update({count:subcur['name'].values})\n",
    "        subgroupdict['docs'].update({count:subcur['docs'].values})\n",
    "        subgroupdict['lemmatized docs'].update({count:subcur['lemmatized docs'].values})\n",
    "        subgroupdict['handle'].update({count:subcur['handle'].values})\n",
    "        subgroupdict['date'].update({count:subcur['date'].values})\n",
    "        subgroupdict['lang'].update({count:subcur['lang'].unique()[0]})\n",
    "        subgroupdict['likeCount'].update({count:sum(subcur['likeCount'].values)})\n",
    "        subgroupdict['retweetCount'].update({count:sum(subcur['retweetCount'].values)})\n",
    "\n",
    "\n",
    "        \n",
    "        docslist= subcur['docs'].values\n",
    "        lemmatized=[]\n",
    "        [lemmatized.extend(i) for i in (subcur['lemmatized docs'].values)]\n",
    "\n",
    "        ## lemmatized is now in sentenaces\n",
    "        docs=[]\n",
    "        [docs.extend(i) for i in docslist]\n",
    "        # lemmatized=' '.join(lemmatized).split(',')\n",
    "        valcounts=pd.Series(lemmatized).value_counts()\n",
    "        \n",
    "        lemmatized=' '.join(lemmatized)\n",
    "        valcounts=pd.Series(lemmatized.split()).value_counts();print(valcounts)\n",
    "        freq=valcounts.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        subgroupdict['docs w.r.t subgroup'].update({count:docs})\n",
    "        subgroupdict['lemmatized docs w.r.t subgroup'].update({count:lemmatized})\n",
    "        subgroupdict['freq table w.r.t subgroup'].update({count:dict(valcounts)})\n",
    "\n",
    "        count+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subgroupDf=pd.DataFrame(subgroupdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrouplist=subgroupDf.subgroup.value_counts().index.tolist()\n",
    "[print(i) for i in subgrouplist if len(i)==4]\n",
    "subgroupDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for the big prep loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "bigdict={'type':{},'name':{},'stoped_lemma':{},'freq':{}}\n",
    "for i in list(indexbyperson.keys()):\n",
    "    a=indexbyperson.get(i)\n",
    "    a=a['name']\n",
    "    for i1 in list(a.keys()):\n",
    "        listtonormaliz=str(a[i1]['content'])\n",
    "        newtext=lemmatizor(listtonormaliz,regexfilter=r'[^a-z0-9\\'\\s]')\n",
    "        lemma=newtext\n",
    "       \n",
    "        stoped=stopfilter(lemma)\n",
    "        stoped=stoped.replace('https','').replace('com','').replace('co','').replace(',','').strip()\n",
    "       \n",
    "        a[i1].update({'stopped_lemma':stoped})         \n",
    "     \n",
    "        cool=dict(pd.Series(stoped.split()).value_counts())\n",
    "        a[i1].update({'word freq':cool})\n",
    "        bigdict['type'].update({num:i})\n",
    "        bigdict['stoped_lemma'].update({num:stoped})\n",
    "        bigdict['freq'].update({num:cool})\n",
    "        bigdict['name'].update({num:i1})\n",
    "        num+=1\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma=pd.DataFrame(bigdict)\n",
    "twitterwordslemma.columns=['type','name','lemmatized','freq']\n",
    "twitterwordslemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma['type']=twitterwordslemma.type.str.lower()\n",
    "\n",
    "pd.to_pickle(twitterwordslemma,'maindalemma.pkl')\n",
    "\n",
    "df=pd.read_pickle('maindalemma.pkl')\n",
    "df=df[[\t'type',\t'name',\t'lemmatized'\t]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaing is explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma=pd.read_pickle('maindalemma.pkl')\n",
    "twitterwordslemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "bigdict_type={'type':{},'stoped_lemma':{},'freq':{}}\n",
    "\n",
    "group1=twitterwordslemma[['stoped_lemma','type']].groupby('type')\n",
    "group1.groups.keys()\n",
    "for i in group1.groups.keys():\n",
    "    \n",
    "    x=(','.join(list(group1.get_group(i).stoped_lemma.values)).strip())\n",
    "  \n",
    "    x=stopfilter(x)\n",
    "    \n",
    "   \n",
    "    y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "    cool=dict(y)\n",
    "    bigdict_type['type'].update({num:i})\n",
    "    bigdict_type['stoped_lemma'].update({num:x})\n",
    "    bigdict_type['freq'].update({num:cool})\n",
    "       \n",
    "\n",
    "    num+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.DataFrame(bigdict_type)\n",
    "x=str(typeslemma.stoped_lemma.values).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'').replace(',',' ').split()\n",
    "y=dict(pd.Series(x).value_counts())\n",
    "aggregatewordfrreq=y\n",
    "# pd.to_pickle(aggregatewordfrreq,'agglemma.pkl')\n",
    "num=len(typeslemma)\n",
    "\n",
    "\n",
    "z=[i.replace(',',' ') for i in typeslemma.stoped_lemma.values]\n",
    "typeslemma=pd.concat([typeslemma,pd.DataFrame({'type':{num:'COMBINED'},'stoped_lemma':{num:str(z).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'')},'freq':{num:aggregatewordfrreq}})])\n",
    "pd.to_pickle(typeslemma,'typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.read_pickle('typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extroverteddf=typeslemma[['type','stoped_lemma']].iloc[0:7]\n",
    "extroverteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "introverteddf=typeslemma[['type','stoped_lemma']].iloc[8:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdict=dict(typeslemma.freq.values[-1])\n",
    "#set stuff\n",
    "setlist=[]\n",
    "for i in typeslemma.freq.values:\n",
    "    setlist.append(set(i.keys()))\n",
    "\n",
    "typelist=[]\n",
    "for i in typeslemma.type.values:\n",
    "    typelist.append(i)\n",
    "\n",
    "typessetdict=dict(zip(typelist,setlist))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys=list(typessetdict.keys())\n",
    "combined=keys.pop(-1);combined\n",
    "intersectiondict={}\n",
    "c=set(typessetdict.get(combined))\n",
    "keys.reverse()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['red','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigint=deepcopy(c)\n",
    "[bigint.intersection_update(typessetdict.get(key))for key in keys]\n",
    "print(f'We have a total of {len(bigint)} in the aggregate intersection\\nWe remove that intersection to compare')\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    kthset=kthset-bigint\n",
    "    int=(c&kthset)-bigint\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    unionwithoutk=unionwithoutk-bigint\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "    unique=(kthset-unionwithoutk)-bigint\n",
    "\n",
    "    print(f'number unique to {k}\\n{len(unique)}\\n',f'percent unique of aggregate union combined:\\n{(len(unique)/len(c))*100:.2f}%')\n",
    "    restint=(kthset&unionwithoutk)-bigint\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','.35','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from itertools import product \n",
    "  \n",
    "# # Get all permutations of length 2 \n",
    "# # and length 2 \n",
    "# x=[\"\".join(seq) for seq in product(\"01\", repeat=4)]\n",
    "# for i in x:\n",
    "#     print(i[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumforavg=dataframe[['type','name']].groupby(['type']).nunique().sum()\n",
    "tochart=(dataframe[['type','name']].groupby(['type']).nunique()/sumforavg)*100\n",
    "\n",
    "tochart=tochart.reset_index()\n",
    "tochart['percent']=tochart['name']\n",
    "\n",
    "tochart.drop(columns='name',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tochart.index=tochart.type\n",
    "tochart.drop(columns='type',inplace=True)\n",
    "tochart=tochart.sort_values(by='percent',ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "genpoppercent=['13.8% 12.3% 11.6% 8.8% 8.7% 8.5% 8.1% 5.4% 4.4% 4.3% 3.3% 3.2% 2.5% 2.1% 1.8% 1.5%']\n",
    "genpoppercent=str(genpoppercent).replace('%','').split()\n",
    "genpoppercent=[float(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in genpoppercent]\n",
    "\n",
    "\n",
    "types=['ISFJ ESFI ISTJ ISFP ESTI ESFP ENFP ISTP INFP ESTP INTP ENTP ENFJ INTJ ENTI INFT']\n",
    "types=str(types).split()\n",
    "\n",
    "\n",
    "types=[(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in types]\n",
    "\n",
    "pop=pd.DataFrame(index=types,data={'pop percentage':genpoppercent})\n",
    "\n",
    "tochart=pd.concat([tochart,pop],axis=1,join='inner')\n",
    "tochart.rename(columns={'percent':'found percent'},inplace=True)\n",
    "cols=['pop percentage','found percent']\n",
    "tochart=tochart[cols]\n",
    "tochart.sort_values(by='pop percentage',ascending=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "styleddf=tochart.T.style.background_gradient(cmap='Blues',axis=1).format(lambda x : f'{x:.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad1=[0,0,0,0]\n",
    "quad2=[0,0,0,0]\n",
    "quad3=[0,.25,.35,.25]\n",
    "quad4=[.35,.25,.35,.25]\n",
    "\n",
    "explode = []\n",
    "explode.extend(quad1)\n",
    "explode.extend(quad2)\n",
    "explode.extend(quad3)\n",
    "explode.extend(quad4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def format_axes(fig):\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n",
    "        ax.tick_params(labelbottom=False, labelleft=False)\n",
    "m=1.23\n",
    "fig = plt.figure(constrained_layout=False,figsize=(m*20,m*12.361))\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('MBTI: General Population Vs Twitter',fontsize=16,weight='demibold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "kwargs1={'title':'General Population (Pie)   ','ax':ax1,'legend':False,'ylabel':'',   'cmap':'Blues'}\n",
    "\n",
    "tochart.plot.pie(y='found percent',**kwargs1)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "kwargs2={'title':'  Twitter (Pie)   ','ax':ax2,'legend':False,'ylabel':'',   'cmap':'viridis'}\n",
    "tochart.plot.pie(y='pop percentage',**kwargs2)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,1])\n",
    "kwargs3={'ax':ax3,'legend':False,'title':'Twitter (Bar)',   'cmap':'viridis'}\n",
    "\n",
    "tochart.plot.barh(y='found percent',**kwargs3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "kwargs4={'ax':ax4,'legend':False,'title':'General Population (Bar)',   'cmap':'Blues'}\n",
    "tochart.plot.barh(y='pop percentage',**kwargs4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"GridSpec\")\n",
    "format_axes(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display('Summary',styleddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Below was an attempt at Multithreading and Paralellism\n",
    "* ## This can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Below are two ways of scraping using CLI commands.\n",
    "# Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "# then output two different csv files.\n",
    "\n",
    "# Query by username\n",
    "# Setting variables to be used in format string command below\n",
    "# def parralledindexer():\n",
    "#  #tweet count number is the last n tweets\n",
    "#  # #read in top 1,000 celebs\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(list(celebs.get('twitter').keys()))\n",
    "#     numindex=range(0,tweet_count*celeblen)\n",
    "\n",
    "#     c_with_slice={}\n",
    "#     for c in range(0,celeblen,1):  \n",
    "#             count=count+tweet_count\n",
    "#             maxnum=count+tweet_count     \n",
    "#             cur=numindex[count:maxnum]\n",
    "#             c_with_slice.update({c:cur})\n",
    "           \n",
    "\n",
    "    # mod10={}\n",
    "    # mod9={}\n",
    "    # mod8={}\n",
    "    # mod7={}\n",
    "    # mod6={}\n",
    "    # mod5={}\n",
    "    # mod4={}\n",
    "    # mod3={}\n",
    "    # mod2={}\n",
    "    # keylist=list(c_with_slice.keys())\n",
    "    # for i in keylist:\n",
    "    #     if i%10==0:  \n",
    "    #         mod10.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%9==0:      \n",
    "    #         mod9.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%8==0:       \n",
    "    #         mod8.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%7==0:      \n",
    "    #         mod7.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%6==0:      \n",
    "    #         mod6.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%5==0:      \n",
    "    #         mod5.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%4==0:       \n",
    "    #         mod4.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%3==0:      \n",
    "    #         mod3.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%2==0:     \n",
    "    #         mod2.update({i:c_with_slice.get(i)})\n",
    "#     moddicts=c_with_slice\n",
    "        \n",
    "\n",
    "#     # moddicts={**mod10,**mod9,\n",
    "#     # **mod8,\n",
    "#     # **mod7,\n",
    "#     # **mod6,\n",
    "#     # **mod5,\n",
    "#     # **mod4,\n",
    "#     # **mod3,\n",
    "#     # **mod2}\n",
    "#     return moddicts,celebs\n",
    "\n",
    "# moddicts,celebs=parralledindexer()\n",
    "\n",
    "# ###Think of schem to split then pu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def actualparrelel():\n",
    "#     '''\n",
    "    \n",
    "#     slow as shit for input output this is for data processing on the comp\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "#         with Pool(10) as pool:\n",
    "#             arglist=[]   \n",
    "#             for m in moddicts:\n",
    "#                 arglist.append((m,celebs)) \n",
    "\n",
    "\n",
    "           \n",
    "#             results_async=pool.starmap_async(partitionableTwitterscraper,arglist)\n",
    "#             # get the return values\n",
    "#             try:\n",
    "#                 for value in results_async.get():\n",
    "#                     values.append(value)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f'Failed with: {e}')\n",
    "#     return values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def actualthreading():\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "  \n",
    "#             arglist=[]   \n",
    "#             for m,v in moddicts.items():\n",
    "#                 arglist.append({m:v})\n",
    "#             # print(arglist) #this is fine    \n",
    "\n",
    "#     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "\n",
    "#     threads = min(50, len(moddicts))   \n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         data=[]\n",
    "           \n",
    "\n",
    "#         for arg in arglist:\n",
    "#             for res in executor.submit(partitionableTwitterscraper(arg)):\n",
    "#                 executor.shutdown(wait=True)\n",
    "    \n",
    "#                 try:\n",
    "#                     data.append((res))\n",
    "#                     # print(res[0])\n",
    "#                     # print(res[1])\n",
    "\n",
    "\n",
    "\n",
    "#                 except Exception as exc:\n",
    "#                     pass\n",
    "#                 #  print('%r generated an exception: %s' % (result, exc))\n",
    "#         # else:\n",
    "#         #     # print('%r page is %d bytes' % (result, len(data)))\n",
    "    \n",
    "#     #sets the number of threads to the lesser of 30 or length of urls\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "  \n",
    "# # empty list with global scope\n",
    "# result = []\n",
    "# # empty list with global scope\n",
    "\n",
    "# def square_list(mylist):\n",
    "#     \"\"\"\n",
    "#     function to square a given list\n",
    "#     \"\"\"\n",
    "#     global result\n",
    "#     # append squares of mylist to global list result\n",
    "#     for num in mylist:\n",
    "#         result.append(num * num)\n",
    "\n",
    "\n",
    "# def partitionableTwitterscraper(c_with_slice):\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     c_with_slice=c_with_slice\n",
    "#     # print(c_with_slice)\n",
    "#     tweet_count =100 \n",
    "#     dictcount=0\n",
    "#     dfdict={}\n",
    "#     errornames=[]\n",
    "\n",
    "\n",
    "    \n",
    "#     print(c_with_slice.keys()) \n",
    "#     print('\\n') \n",
    "#     c=list(c_with_slice.keys())[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     twitter_handle=celebs.get('twitter')\n",
    "#     name=celebs.get('name')\n",
    "#     cur=c_with_slice.get(c)  \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "    \n",
    "\n",
    "    \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next artist i.e. failsafe \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "\n",
    "#         if dictcount<=1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                     tweets_df1.update({'name':nameseries})\n",
    "#                     tweets_df1.update({'handle':handleseries})\n",
    "#                     a=tweets_df1.get(key)\n",
    "#                     b=dfdict.get(key)\n",
    "#                     c={**a,**b}\n",
    "#                     dfdict=dfdict.update({key:c})\n",
    "#     except:\n",
    "#         # errornames.append(name)\n",
    "#         # print('errornames:\\n',len(errornames))\n",
    "#         pass \n",
    "\n",
    "\n",
    "#     display(dfdict)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "   \n",
    "\n",
    "#     return dfdict\n",
    "\n",
    "\n",
    "  \n",
    "# moddicts,celebs=parralledindexer()\n",
    "# values=[]\n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=[]   \n",
    "#     for m,v in moddicts.items():\n",
    "#         arglist.append({m:v})\n",
    "  \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=partitionableTwitterscraper, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "  \n",
    "#     # print global result list\n",
    "#     print(\"Result(in main program): {}\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maindict=list()\n",
    "\n",
    "# def argcreateator(n=3):\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "\n",
    "#     tweet_count =n #this number is the last n tweets\n",
    "#     dflist=[]\n",
    "#     dfdict={}\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(celebs.get('twitter').keys())\n",
    "#     numindex=list(range(0,tweet_count*celeblen))\n",
    "#     len(numindex)\n",
    "#     arglist=[]\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#     for c in range(0,celeblen):\n",
    "#         count=count+tweet_count\n",
    "#         maxnum=count+tweet_count\n",
    "\n",
    "#         twitter_handle=(celebs.get('twitter')[c])\n",
    "#         name=(celebs.get('name')[c])\n",
    "\n",
    "#         cur=pd.Index(numindex[count:maxnum])\n",
    "\n",
    "#         #create Series to append the current handle to the dataframe\n",
    "#         handleseries={i:twitter_handle for i in cur}       \n",
    "#         #create Series to append the current name to dataframe   \n",
    "#         nameseries={i:name for i in cur}\n",
    "#         arglist.append([cur,nameseries,handleseries])\n",
    "#     return arglist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur):\n",
    "#     global maindict\n",
    "#     os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "#      # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#      #if there is an error then it will just move to the next  \n",
    "#     tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=cur).to_dict()\n",
    "#     tweets_df1.update({'name':nameseries})\n",
    "#     tweets_df1.update({'handle':handleseries})\n",
    "#     maindict.append(pd.DataFrame(tweets_df1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def threader(arglist):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     dictcount=0\n",
    "#     errornames=[]\n",
    "\n",
    "   \n",
    "\n",
    "#     # for i in range(0,len(arglist)):\n",
    "#     for i in range(0,len(arglist)):\n",
    "#         tweet_count=len(arglist[i][0]);#display(tweet_count)\n",
    "#         nameseries=arglist[i][1];#display(nameseries)\n",
    "#         handleseries=arglist[i][2];#display(handleseries)\n",
    "#         twitter_handle=list(handleseries.values())[0];#display(twitter_handle)\n",
    "#         cur=arglist[i][0];#display(cur)\n",
    "#         try:\n",
    "#             tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur)\n",
    "            \n",
    "\n",
    "    \n",
    "#         except:\n",
    "#             errornames.append(twitter_handle)\n",
    "#             print(errornames)\n",
    "#             pass\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=argcreateator(n=3)   \n",
    "    \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=threader, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "#     print(maindict)\n",
    "  \n",
    "#     # print global result list\n",
    " \n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
