{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import random\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "# import multiprocessing\n",
    "# from multiprocessing import Pool\n",
    "from datetime import date\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "from  nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import concurrent.futures\n",
    "\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from pandas.plotting import table \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib_venn_wordcloud import venn3_wordcloud,venn2_wordcloud\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from matplotlib_venn import venn3, venn3_circles,venn2_circles,venn2,venn2_unweighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read in top 1,000 celebs\n",
    "url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "celebs=pd.read_csv(url)\n",
    "\n",
    "celebs=celebs.to_dict()\n",
    "# celebs['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Below are two ways of scraping using CLI commands.\n",
    "## Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "## then output two different csv files.\n",
    "#\n",
    "## Query by username\n",
    "## Setting variables to be used in format string command below\n",
    "\n",
    "\n",
    "# tweet_count =500 #this number is the last n tweets\n",
    "# dflist=[]\n",
    "# dfdict={}\n",
    "# count=-1*tweet_count\n",
    "# celeblen=len(celebs.get('twitter').keys())\n",
    "# numindex=list(range(0,tweet_count*celeblen))\n",
    "# len(numindex)\n",
    "\n",
    "\n",
    "# dictcount=0\n",
    "# errornames=[]\n",
    "\n",
    "\n",
    "# for c in range(0,celeblen):\n",
    "#     sleep(random.randrange(0,7)/10)\n",
    "#     dictcount+=1\n",
    "#     count=count+tweet_count\n",
    "#     maxnum=count+tweet_count\n",
    "\n",
    "#     twitter_handle=str(celebs.get('twitter')[c])\n",
    "#     name=str(celebs.get('name')[c])\n",
    "    \n",
    "#     cur=numindex[count:maxnum]\n",
    "   \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "  \n",
    "\n",
    "  \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next  \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "    \n",
    "#         if dictcount==1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                 tweets_df1.update({'name':nameseries})\n",
    "#                 tweets_df1.update({'handle':handleseries})\n",
    "#                 a=tweets_df1.get(key)\n",
    "#                 b=dfdict.get(key)\n",
    "#                 c={**a,**b}\n",
    "#                 dfdict.update({key:c})\n",
    "#     except:\n",
    "#         errornames.append(name)\n",
    "#         print(errornames)\n",
    "#         pass\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "# dataframe=pd.DataFrame(dfdict).sort_index()\n",
    "# cols=list(set(dataframe.columns)-{'name','handle'})\n",
    "# cols.insert(0,'name')\n",
    "# cols.insert(0,'handle')\n",
    "\n",
    "\n",
    "# dataframe=dataframe[dataframe.lang=='en']\n",
    "\n",
    "# dataframe\n",
    "\n",
    "\n",
    "# pd.to_pickle(dataframe,\"500perpull.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      \n",
    "# Wrangling Takeaways\n",
    "\n",
    "### Aquire\n",
    "* For acquire we grabbed the data from https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv then we used the handles to create a list of the twitter handles. \n",
    "\n",
    "* We use the SNSCRAPE twitter module and iterate through  the list gathering the last 500 tweets per celeb. From there the results are saved as a JSON, the JSON is read into a data frame which is updated for each celeb.\n",
    "\n",
    "* After that we merge the data frame with the data from https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv\n",
    "Which wraps up the acquire phase.\n",
    "\n",
    "\n",
    "### Prep\n",
    "\n",
    "* We used several functions for to clean. We cleaned by filters and then lemmatization. We also translated our emojis into text using from emot.emo_unicode import EMOJI_UNICODE, EMOTICONS_EMO.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning you need to uncomment above to scrape and create the pickle. \n",
    "\n",
    "## Your speed my vary but it took 45 mins for the scrape\n",
    "\n",
    "## The code below directly below is to the big scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the pickle and then display the unique file\n",
    "\n",
    "dataframe_a=pd.read_pickle(\"./fivezerominpull.pkl\")\n",
    "dataframe_b=pd.read_pickle(\"500perpull.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50762"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "246412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "297174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe_a=dataframe_a[dataframe_a.lang=='en']\n",
    "display(len(dataframe_a))\n",
    "display(len(dataframe_b))\n",
    "merged=pd.concat([dataframe_a,dataframe_b])\n",
    "display(len(merged))\n",
    "merged.name=merged.name.str.lower()\n",
    "dataframe=merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptypeurl='https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv'\n",
    "\n",
    "ptypes=pd.read_csv(ptypeurl);ptypes\n",
    "newcols=[]\n",
    "for x in ptypes.columns.to_list():\n",
    "    y=x.lower()\n",
    "    newcols.append(y)\n",
    "\n",
    "ptypes.columns=newcols\n",
    "ptypes['handle']=ptypes.twitter\n",
    "ptypes.drop(columns='twitter',inplace=True)\n",
    "ptypes.name=ptypes.name.str.lower();ptypes\n",
    "\n",
    "dataframe.name=dataframe.name.str.lower();dataframe.columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a=ptypes.name.values.tolist()\n",
    "b=ptypes.type.values.tolist()\n",
    "ptypemap=dict(zip(a,b))\n",
    "# ptypemap\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1665210032.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.dropna(axis=1,inplace=True)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1665210032.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe.dropna(axis=0,inplace=True)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1665210032.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['type']=dataframe.name.map(ptypemap)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "254650"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe=dataframe[dataframe['quotedTweet'].isna()]\n",
    "dataframe.dropna(axis=1,inplace=True)\n",
    "dataframe.dropna(axis=0,inplace=True)\n",
    "dataframe\n",
    "\n",
    "# dataframe.dropna(axis=1,inplace=True)\n",
    "dataframe['type']=dataframe.name.map(ptypemap)\n",
    "\n",
    "\n",
    "\n",
    "cols=list(set(dataframe.columns)-{'name','handle','type'})\n",
    "cols.insert(0,'handle')\n",
    "cols.insert(0,'name')\n",
    "cols.insert(0,'type')\n",
    "\n",
    "dataframe=dataframe[cols]\n",
    "\n",
    "len(dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>837</th>\n",
       "      <th>838</th>\n",
       "      <th>839</th>\n",
       "      <th>840</th>\n",
       "      <th>841</th>\n",
       "      <th>842</th>\n",
       "      <th>843</th>\n",
       "      <th>844</th>\n",
       "      <th>845</th>\n",
       "      <th>846</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>katy perry</td>\n",
       "      <td>justin bieber</td>\n",
       "      <td>taylor swift</td>\n",
       "      <td>rihanna</td>\n",
       "      <td>the countess</td>\n",
       "      <td>justin timberlake</td>\n",
       "      <td>ellen degeneres</td>\n",
       "      <td>britney spears</td>\n",
       "      <td>cristiano ronaldo</td>\n",
       "      <td>kim kardashian west</td>\n",
       "      <td>...</td>\n",
       "      <td>nicolas vazquez</td>\n",
       "      <td>kenan doƒüulu</td>\n",
       "      <td>flor de la ve</td>\n",
       "      <td>cyril hanouna</td>\n",
       "      <td>man√°</td>\n",
       "      <td>sebastian rulli</td>\n",
       "      <td>carlos baute</td>\n",
       "      <td>levent √ºz√ºmc√º</td>\n",
       "      <td>adal ramones</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 847 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0              1             2        3             4    \\\n",
       "0  katy perry  justin bieber  taylor swift  rihanna  the countess   \n",
       "\n",
       "                 5                6               7                  8    \\\n",
       "0  justin timberlake  ellen degeneres  britney spears  cristiano ronaldo   \n",
       "\n",
       "                   9    ...              837           838            839  \\\n",
       "0  kim kardashian west  ...  nicolas vazquez  kenan doƒüulu  flor de la ve   \n",
       "\n",
       "             840   841              842           843            844  \\\n",
       "0  cyril hanouna  man√°  sebastian rulli  carlos baute  levent √ºz√ºmc√º   \n",
       "\n",
       "            845           846  \n",
       "0  adal ramones  manu gavassi  \n",
       "\n",
       "[1 rows x 847 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>ENFJ</th>\n",
       "      <th>ENFP</th>\n",
       "      <th>ENTJ</th>\n",
       "      <th>ENTP</th>\n",
       "      <th>ESFJ</th>\n",
       "      <th>ESFP</th>\n",
       "      <th>ESTJ</th>\n",
       "      <th>ESTP</th>\n",
       "      <th>INFJ</th>\n",
       "      <th>INFP</th>\n",
       "      <th>INTJ</th>\n",
       "      <th>INTP</th>\n",
       "      <th>ISFJ</th>\n",
       "      <th>ISFP</th>\n",
       "      <th>ISTJ</th>\n",
       "      <th>ISTP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>21</td>\n",
       "      <td>49</td>\n",
       "      <td>63</td>\n",
       "      <td>103</td>\n",
       "      <td>23</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type  ENFJ  ENFP  ENTJ  ENTP  ESFJ  ESFP  ESTJ  ESTP  INFJ  INFP  INTJ  INTP  \\\n",
       "name    37    59    21    49    63   103    23    50    25    15    11    11   \n",
       "\n",
       "type  ISFJ  ISFP  ISTJ  ISTP  \n",
       "name    32    49    12    30  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "display(pd.DataFrame((dataframe['name'].unique()),index=range(0,len(dataframe['name'].unique()))).T)\n",
    "display(dataframe[['type','name']].groupby(['type']).nunique().T)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>content</th>\n",
       "      <th>conversationId</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>_type</th>\n",
       "      <th>quoteCount</th>\n",
       "      <th>source</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>date</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>url</th>\n",
       "      <th>sourceUrl</th>\n",
       "      <th>sourceLabel</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>name</td>\n",
       "      <td>handle</td>\n",
       "      <td>renderedContent</td>\n",
       "      <td>content</td>\n",
       "      <td>conversationId</td>\n",
       "      <td>likeCount</td>\n",
       "      <td>id</td>\n",
       "      <td>lang</td>\n",
       "      <td>_type</td>\n",
       "      <td>quoteCount</td>\n",
       "      <td>source</td>\n",
       "      <td>retweetCount</td>\n",
       "      <td>date</td>\n",
       "      <td>replyCount</td>\n",
       "      <td>url</td>\n",
       "      <td>sourceUrl</td>\n",
       "      <td>sourceLabel</td>\n",
       "      <td>user</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  name  handle  renderedContent  content  conversationId  likeCount  \\\n",
       "0  type  name  handle  renderedContent  content  conversationId  likeCount   \n",
       "\n",
       "   id  lang  _type  quoteCount  source  retweetCount  date  replyCount  url  \\\n",
       "0  id  lang  _type  quoteCount  source  retweetCount  date  replyCount  url   \n",
       "\n",
       "   sourceUrl  sourceLabel  user  \n",
       "0  sourceUrl  sourceLabel  user  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "254650"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "254650"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>content</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>date</th>\n",
       "      <th>handle</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>katy perry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>ESFP</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>511</td>\n",
       "      <td>148</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>juicy j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>2126</td>\n",
       "      <td>91</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>4395</td>\n",
       "      <td>306</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>2937</td>\n",
       "      <td>142</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>13170</td>\n",
       "      <td>1225</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>manu gavassi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254650 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                    renderedContent  \\\n",
       "0       ENFJ  if you wanna know why any human is they way th...   \n",
       "1       ENFJ  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3       ENFJ  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4       ENFJ  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5       ENFJ  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...      ...                                                ...   \n",
       "496999  ESFP  Block that devil out your ear this morning  #s...   \n",
       "497228  ENFP                                            Spaces?   \n",
       "497240  ENFP  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285  ENFP                                           .ebaselE   \n",
       "497395  ENFP                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "                                                  content  likeCount  \\\n",
       "0       if you wanna know why any human is they way th...      29840   \n",
       "1       wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...      10502   \n",
       "3       heck I pour beer out of my tits (that‚Äôs a part...       3042   \n",
       "4       The show‚Äôs set list is a fun üé¢  through memory...       3654   \n",
       "5       Welcoming all my #flatearthers #spaceisfakers ...      20819   \n",
       "...                                                   ...        ...   \n",
       "496999  Block that devil out your ear this morning  #s...        511   \n",
       "497228                                            Spaces?       2126   \n",
       "497240  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...       4395   \n",
       "497285                                           .ebaselE       2937   \n",
       "497395                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è      13170   \n",
       "\n",
       "        retweetCount                      date         handle          name  \n",
       "0               4575 2022-10-30 22:15:09+00:00      katyperry    katy perry  \n",
       "1                881 2022-10-29 07:01:46+00:00      katyperry    katy perry  \n",
       "3                297 2022-10-27 19:07:44+00:00      katyperry    katy perry  \n",
       "4                336 2022-10-27 19:07:21+00:00      katyperry    katy perry  \n",
       "5               2318 2022-10-27 19:05:27+00:00      katyperry    katy perry  \n",
       "...              ...                       ...            ...           ...  \n",
       "496999           148 2022-03-17 15:04:10+00:00  therealjuicyj       juicy j  \n",
       "497228            91 2021-10-30 02:13:17+00:00    manugavassi  manu gavassi  \n",
       "497240           306 2021-09-28 22:29:07+00:00    manugavassi  manu gavassi  \n",
       "497285           142 2021-08-13 23:31:10+00:00    manugavassi  manu gavassi  \n",
       "497395          1225 2020-09-30 04:33:31+00:00    manugavassi  manu gavassi  \n",
       "\n",
       "[254650 rows x 8 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dataframe.columns.to_frame().T)\n",
    "cols=['type','name','renderedContent','content','handle','date','lang','likeCount','retweetCount']\n",
    "keep=dataframe[cols];display(len(keep))\n",
    "keep=keep[keep['lang']=='en'];display(len(keep))\n",
    "cols=list(set(keep.columns)-{'lang'})\n",
    "keep=keep[cols]\n",
    "keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquire is done \n",
    "\n",
    "\n",
    "## Prep is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1=keep[['type','name','content']].groupby(by=['type','name'])\n",
    "lista=set(group1.groups.keys())\n",
    "group2=keep[['type','name']].groupby(by=['type'])\n",
    "listb=list(set(group2.groups.keys()))\n",
    "group3=keep[['name','content']].groupby(by=['name'])   \n",
    "indexbyperson={}\n",
    "for b in listb:\n",
    "    g=list(group2.get_group(b).index)\n",
    "    n=list(group2.get_group(b).name.unique())\n",
    "    \n",
    "    ndict={}\n",
    "    for i in n:\n",
    "        k=list(group3.get_group(i).index)\n",
    "        c=list(group3.get_group(i).content)\n",
    "        ndict.update({i:{'index':k,'content':c}})\n",
    "    indexbyperson.update({b:{'index':g,'name':ndict}})\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# namegroups=allthewaydown.groupby('name')\n",
    "# keys=namegroups.groups.keys()\n",
    "# for k in keys:\n",
    "#     print(len(namegroups.get_group(k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "#pdf plumber\n",
    "#csv.preview\n",
    "import pandas as pd\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stopwords = ['like', 'im', 'think', 'dont', 'people', 'know', 'one', 'get', 'really','thing',\n",
    "                  'would', 'time', 'type', 'make', 'friend', 'ive', 'much','amp','twitter',\n",
    "                 'say', 'way', 'see', 'thing', 'want', 'thing', 'good', 'something', 'lot',\n",
    "                  'also', 'go', 'always', 'even', 'well', 'someone','https','http','com','co',',',\"'\"]\n",
    "\n",
    "\n",
    "\n",
    "stops=stopwords.words(['french','german','english','spanish','portuguese'])+ more_stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(stops,'stopwords.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "    stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "   \n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops and len(x)>=2), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    " \n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''   \n",
    "    Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #make a copy and begin to transform it\n",
    "    newtext = text.lower()\n",
    "\n",
    "    #encode into ascii then decode\n",
    "    newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "\n",
    "    #use re.sub to remove special characters\n",
    "    newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return newtext\n",
    "\n",
    "    \n",
    "def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    \n",
    "      Takes text, tokenizes it, lemmatizes it\n",
    "      lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "      needs to be commented out after the first run (up to modeling)\n",
    "      # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "      needs to be un commented commented\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "\n",
    "    #make ready the lemmatizer object\n",
    "    newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "    lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return lemmafiltered\n",
    "    \n",
    "    \n",
    "def split_apply_join(funct,listobj):\n",
    "    'helperfuction letters'\n",
    "\n",
    "    mapped=map(funct, listobj)\n",
    "    mapped=list(mapped)\n",
    "    mapped=''.join(mapped)\n",
    "  \n",
    "    return mapped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    ''' \n",
    "    For a large file just save it locally\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "    #make ready tokenizer object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "    return newtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:62: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tryagain.type=tryagain.type.str.lower()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tryagain.drop_duplicates(inplace=True)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tryagain.dropna(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_list = []\n",
    "def domainmapper(spot):\n",
    "        if (spot == 'intj') | (spot == 'entj') | (spot == 'intp') | (spot == 'entp'):\n",
    "            return 'analyst'\n",
    "        elif (spot == 'infj') | (spot == 'enfj') | (spot == 'infp') | (spot == 'enfp'):\n",
    "            return 'diplomat'\n",
    "        elif (spot == 'istj') | (spot == 'estj') | (spot == 'isfj') | (spot == 'esfj'):\n",
    "            return 'sentinel'\n",
    "        elif (spot == 'istp') | (spot == 'estp') | (spot == 'isfp') | (spot == 'esfp'):\n",
    "            return 'explorer'\n",
    "        # else:\n",
    "        #     new_list.append('other')\n",
    "\n",
    "\n",
    "\n",
    "def pairwiseattributemapper(df):\n",
    "    x=df['type']\n",
    "    i_e={}\n",
    "\n",
    "    n_s={}\n",
    "\n",
    "    t_f={}\n",
    "\n",
    "    j_p={}\n",
    "\n",
    "\n",
    "    for spot in x:\n",
    "        # print(spot)\n",
    "        # print(type(spot))\n",
    "        if (spot[0]=='i')|(spot[0]=='e'):\n",
    "            if spot[0]=='i':\n",
    "                i_e.update({spot:'i'})\n",
    "            else:\n",
    "                i_e.update({spot:'e'})    \n",
    "        if (spot[1]=='n')|(spot[1]=='s'):\n",
    "            if spot[1]=='n':\n",
    "                n_s.update({spot:'n'})\n",
    "            else:\n",
    "                n_s.update({spot:'s'})    \n",
    "        if (spot[2]=='t')|(spot[2]=='f'):\n",
    "            if spot[2]=='t':\n",
    "                t_f.update({spot:'t'})\n",
    "            else:\n",
    "                t_f.update({spot:'f'})    \n",
    "        if (spot[3]=='j')|(spot[3]=='p'):\n",
    "            if spot[3]=='j':\n",
    "                j_p.update({spot:'j'})\n",
    "            else:\n",
    "                j_p.update({spot:'p'})    \n",
    "    df['i|e']=x.map(i_e)\n",
    "    df['n|s']=x.map(n_s)\n",
    "    df['t|f']=x.map(t_f)\n",
    "    df['j|p']=x.map(j_p)\n",
    "    return df\n",
    "    \n",
    "       \n",
    "        # else:\n",
    "        #     new_list.append('other')\n",
    "\n",
    "\n",
    "tryagain=keep[['type','name','renderedContent','handle','date','likeCount','retweetCount']]\n",
    "tryagain.type=tryagain.type.str.lower()        \n",
    "\n",
    "tryagain['content']=tryagain['renderedContent']\n",
    "tryagain=tryagain[['type','name','content','handle','date','likeCount','retweetCount']]\n",
    "tryagain.drop_duplicates(inplace=True)\n",
    "tryagain.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3057441650.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tryagain['domain'] = tryagain['type'].apply(domainmapper)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['i|e']=x.map(i_e)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['n|s']=x.map(n_s)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['t|f']=x.map(t_f)\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/1320820379.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['j|p']=x.map(j_p)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tryagain['domain'] = tryagain['type'].apply(domainmapper)\n",
    "tryagain=pairwiseattributemapper(tryagain)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>domain</th>\n",
       "      <th>i|e</th>\n",
       "      <th>n|s</th>\n",
       "      <th>t|f</th>\n",
       "      <th>j|p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>esfp</td>\n",
       "      <td>juicy j</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>511</td>\n",
       "      <td>148</td>\n",
       "      <td>explorer</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>2126</td>\n",
       "      <td>91</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>4395</td>\n",
       "      <td>306</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>2937</td>\n",
       "      <td>142</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>13170</td>\n",
       "      <td>1225</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182352 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type          name                                            content  \\\n",
       "0       enfj    katy perry  if you wanna know why any human is they way th...   \n",
       "1       enfj    katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3       enfj    katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4       enfj    katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5       enfj    katy perry  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...      ...           ...                                                ...   \n",
       "496999  esfp       juicy j  Block that devil out your ear this morning  #s...   \n",
       "497228  enfp  manu gavassi                                            Spaces?   \n",
       "497240  enfp  manu gavassi  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285  enfp  manu gavassi                                           .ebaselE   \n",
       "497395  enfp  manu gavassi                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "               handle                      date  likeCount  retweetCount  \\\n",
       "0           katyperry 2022-10-30 22:15:09+00:00      29840          4575   \n",
       "1           katyperry 2022-10-29 07:01:46+00:00      10502           881   \n",
       "3           katyperry 2022-10-27 19:07:44+00:00       3042           297   \n",
       "4           katyperry 2022-10-27 19:07:21+00:00       3654           336   \n",
       "5           katyperry 2022-10-27 19:05:27+00:00      20819          2318   \n",
       "...               ...                       ...        ...           ...   \n",
       "496999  therealjuicyj 2022-03-17 15:04:10+00:00        511           148   \n",
       "497228    manugavassi 2021-10-30 02:13:17+00:00       2126            91   \n",
       "497240    manugavassi 2021-09-28 22:29:07+00:00       4395           306   \n",
       "497285    manugavassi 2021-08-13 23:31:10+00:00       2937           142   \n",
       "497395    manugavassi 2020-09-30 04:33:31+00:00      13170          1225   \n",
       "\n",
       "          domain i|e n|s t|f j|p  \n",
       "0       diplomat   e   n   f   j  \n",
       "1       diplomat   e   n   f   j  \n",
       "3       diplomat   e   n   f   j  \n",
       "4       diplomat   e   n   f   j  \n",
       "5       diplomat   e   n   f   j  \n",
       "...          ...  ..  ..  ..  ..  \n",
       "496999  explorer   e   s   f   p  \n",
       "497228  diplomat   e   n   f   p  \n",
       "497240  diplomat   e   n   f   p  \n",
       "497285  diplomat   e   n   f   p  \n",
       "497395  diplomat   e   n   f   p  \n",
       "\n",
       "[182352 rows x 12 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>domain</th>\n",
       "      <th>i|e</th>\n",
       "      <th>n|s</th>\n",
       "      <th>t|f</th>\n",
       "      <th>j|p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>esfp</td>\n",
       "      <td>juicy j</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>511</td>\n",
       "      <td>148</td>\n",
       "      <td>explorer</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>2126</td>\n",
       "      <td>91</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>4395</td>\n",
       "      <td>306</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>2937</td>\n",
       "      <td>142</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>13170</td>\n",
       "      <td>1225</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182352 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type          name                                            content  \\\n",
       "0       enfj    katy perry  if you wanna know why any human is they way th...   \n",
       "1       enfj    katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3       enfj    katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4       enfj    katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5       enfj    katy perry  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...      ...           ...                                                ...   \n",
       "496999  esfp       juicy j  Block that devil out your ear this morning  #s...   \n",
       "497228  enfp  manu gavassi                                            Spaces?   \n",
       "497240  enfp  manu gavassi  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285  enfp  manu gavassi                                           .ebaselE   \n",
       "497395  enfp  manu gavassi                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "               handle                      date  likeCount  retweetCount  \\\n",
       "0           katyperry 2022-10-30 22:15:09+00:00      29840          4575   \n",
       "1           katyperry 2022-10-29 07:01:46+00:00      10502           881   \n",
       "3           katyperry 2022-10-27 19:07:44+00:00       3042           297   \n",
       "4           katyperry 2022-10-27 19:07:21+00:00       3654           336   \n",
       "5           katyperry 2022-10-27 19:05:27+00:00      20819          2318   \n",
       "...               ...                       ...        ...           ...   \n",
       "496999  therealjuicyj 2022-03-17 15:04:10+00:00        511           148   \n",
       "497228    manugavassi 2021-10-30 02:13:17+00:00       2126            91   \n",
       "497240    manugavassi 2021-09-28 22:29:07+00:00       4395           306   \n",
       "497285    manugavassi 2021-08-13 23:31:10+00:00       2937           142   \n",
       "497395    manugavassi 2020-09-30 04:33:31+00:00      13170          1225   \n",
       "\n",
       "          domain i|e n|s t|f j|p  \n",
       "0       diplomat   e   n   f   j  \n",
       "1       diplomat   e   n   f   j  \n",
       "3       diplomat   e   n   f   j  \n",
       "4       diplomat   e   n   f   j  \n",
       "5       diplomat   e   n   f   j  \n",
       "...          ...  ..  ..  ..  ..  \n",
       "496999  explorer   e   s   f   p  \n",
       "497228  diplomat   e   n   f   p  \n",
       "497240  diplomat   e   n   f   p  \n",
       "497285  diplomat   e   n   f   p  \n",
       "497395  diplomat   e   n   f   p  \n",
       "\n",
       "[182352 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pd.to_pickle(tryagain,'tryagain.pkl')\n",
    "display(tryagain)\n",
    "\n",
    "\n",
    "pd.read_pickle('tryagain.pkl')\n",
    "\n",
    "toplevelGrouplist=['type','domain','i|e','n|s','t|f','j|p']\n",
    "for topGru in toplevelGrouplist:    \n",
    "    types=tryagain.groupby([f'{topGru}','name'])    \n",
    "    typekeys=types.groups.keys()\n",
    "    for key in typekeys:\n",
    "        df=types.get_group(key)\n",
    "       \n",
    "        # namegroup=df.groupby('name')\n",
    "        # namekeys=namegroup.groups.keys()\n",
    "        # for nm in namekeys:\n",
    "        #     print(nm)\n",
    "        #     celeb=namegroup.get_group(nm)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_data(df):\n",
    "    # create train and test (80/20 split) from the orginal dataframe\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=123, stratify=df.type)\n",
    "    # create train and validate (75/25 split) from the train dataframe\n",
    "    train, val = train_test_split(train, test_size=.25, random_state=123, stratify=train.type)\n",
    "    \n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i|e\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n|s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t|f\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j|p\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i|e\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n|s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t|f\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j|p\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i|e\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n|s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t|f\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j|p\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:32: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  valcounts=pd.Series(fulllist).value_counts()\n",
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/3237891019.py:40: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def allthewaydownfunc(df):\n",
    "\n",
    "    groups=['type', 'domain', 'i|e', 'n|s', 't|f', 'j|p']\n",
    "\n",
    "\n",
    "    allthewaydict={'group':{},'subgroup':{},'name':{},'docs':{},'lemmatized docs':{},'tf-idf w.r.t name':{},'freq table w.r.t name':{},'handle':{},'date':{},'lang'\t:{},'likeCount':{},'retweetCount':{}}\n",
    "    # groups=(groups[2:4])\n",
    "    count=0\n",
    "    for g in groups:\n",
    "        print(f'{g}\\n')\n",
    "        group=tryagain.groupby(g)\n",
    "        keys=group.groups.keys()\n",
    "        for key in keys:\n",
    "            # print(key)\n",
    "            kthgroup=group.get_group(key)\n",
    "            namegru=kthgroup.groupby('name')\n",
    "            subkeys=namegru.groups.keys()\n",
    "\n",
    "\n",
    "            for sk in subkeys:\n",
    "                curname=namegru.get_group(sk)        \n",
    "                nm=curname.name.unique()[0]\n",
    "                docs=curname.content.values\n",
    "                # splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "                lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "                lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "                fulllist=\" \".join(lemma).split()\n",
    "\n",
    "\n",
    "                valcounts=pd.Series(fulllist).value_counts()\n",
    "                freq=valcounts.values\n",
    "\n",
    "\n",
    "                ##TF-IDF calc\n",
    "                x=np.log10((len(lemma)/freq))\n",
    "                x*=freq\n",
    "                x=np.ndarray.round(x,3)\n",
    "                tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n",
    "                freqtable=dict(valcounts)\n",
    "\n",
    "                # df=idfDf(lemma);display(df)\n",
    "                allthewaydict['group'].update({count:g})\n",
    "                allthewaydict['subgroup'].update({count:key})\n",
    "                allthewaydict['name'].update({count:nm})\n",
    "                allthewaydict['docs'].update({count:list(docs)})\n",
    "                allthewaydict['lemmatized docs'].update({count:list(lemma)})\n",
    "                allthewaydict['tf-idf w.r.t name'].update({count:tf_idf})\n",
    "                allthewaydict['freq table w.r.t name'].update({count:freqtable})\n",
    "                allthewaydict['handle'].update({count:curname.handle.values[0]})\n",
    "                allthewaydict['date'].update({count:curname.date.values})\n",
    "              \n",
    "                allthewaydict['likeCount'].update({count:curname.likeCount.values.sum()})\n",
    "                allthewaydict['retweetCount'].update({count:curname.retweetCount.values.sum()})\n",
    "\n",
    "\n",
    "                count+=1\n",
    "\n",
    "\n",
    "    allthewaydown=pd.DataFrame(allthewaydict)\n",
    "\n",
    "\n",
    "    return allthewaydown\n",
    "\n",
    "\n",
    "\n",
    "def splitPrep(tryagain):\n",
    "    train, val, test=split_data(tryagain)\n",
    "    train=allthewaydownfunc(train)\n",
    "    val=allthewaydownfunc(val)\n",
    "    test=allthewaydownfunc(test)\n",
    "    pd.to_pickle(train,'mvp_plus_1_train.pkl')\n",
    "    pd.to_pickle(val,'mvp_plus_1_val.pkl')\n",
    "    pd.to_pickle(test,'mvp_plus_1_test.pkl')\n",
    " \n",
    "\n",
    " \n",
    "splitPrep(tryagain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>domain</th>\n",
       "      <th>i|e</th>\n",
       "      <th>n|s</th>\n",
       "      <th>t|f</th>\n",
       "      <th>j|p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>esfp</td>\n",
       "      <td>juicy j</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>511</td>\n",
       "      <td>148</td>\n",
       "      <td>explorer</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>2126</td>\n",
       "      <td>91</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>4395</td>\n",
       "      <td>306</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>2937</td>\n",
       "      <td>142</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>13170</td>\n",
       "      <td>1225</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182352 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type          name                                            content  \\\n",
       "0       enfj    katy perry  if you wanna know why any human is they way th...   \n",
       "1       enfj    katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3       enfj    katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4       enfj    katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5       enfj    katy perry  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...      ...           ...                                                ...   \n",
       "496999  esfp       juicy j  Block that devil out your ear this morning  #s...   \n",
       "497228  enfp  manu gavassi                                            Spaces?   \n",
       "497240  enfp  manu gavassi  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285  enfp  manu gavassi                                           .ebaselE   \n",
       "497395  enfp  manu gavassi                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "               handle                      date  likeCount  retweetCount  \\\n",
       "0           katyperry 2022-10-30 22:15:09+00:00      29840          4575   \n",
       "1           katyperry 2022-10-29 07:01:46+00:00      10502           881   \n",
       "3           katyperry 2022-10-27 19:07:44+00:00       3042           297   \n",
       "4           katyperry 2022-10-27 19:07:21+00:00       3654           336   \n",
       "5           katyperry 2022-10-27 19:05:27+00:00      20819          2318   \n",
       "...               ...                       ...        ...           ...   \n",
       "496999  therealjuicyj 2022-03-17 15:04:10+00:00        511           148   \n",
       "497228    manugavassi 2021-10-30 02:13:17+00:00       2126            91   \n",
       "497240    manugavassi 2021-09-28 22:29:07+00:00       4395           306   \n",
       "497285    manugavassi 2021-08-13 23:31:10+00:00       2937           142   \n",
       "497395    manugavassi 2020-09-30 04:33:31+00:00      13170          1225   \n",
       "\n",
       "          domain i|e n|s t|f j|p  \n",
       "0       diplomat   e   n   f   j  \n",
       "1       diplomat   e   n   f   j  \n",
       "3       diplomat   e   n   f   j  \n",
       "4       diplomat   e   n   f   j  \n",
       "5       diplomat   e   n   f   j  \n",
       "...          ...  ..  ..  ..  ..  \n",
       "496999  explorer   e   s   f   p  \n",
       "497228  diplomat   e   n   f   p  \n",
       "497240  diplomat   e   n   f   p  \n",
       "497285  diplomat   e   n   f   p  \n",
       "497395  diplomat   e   n   f   p  \n",
       "\n",
       "[182352 rows x 12 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pandas.io.pickle.to_pickle(obj: 'Any', filepath_or_buffer: 'FilePath | WriteBuffer[bytes]', compression: 'CompressionOptions' = 'infer', protocol: 'int' = 5, storage_options: 'StorageOptions' = None) -> 'None'>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_idf(word,**kwargs):\n",
    "#         documents =kwargs['docs']\n",
    "#         n_occurences = sum([1 for doc in documents if word in doc])\n",
    "#         return len(documents) / n_occurences\n",
    "\t\t\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get a list of the unique words\n",
    "# def idfDf(documents):\n",
    "#     unique_words = pd.Series(','.join(documents).split()).unique()\n",
    "#     kwargs={'docs':documents}\n",
    "#     # put the unique words into a data frame\n",
    "#     df=(pd.DataFrame(dict(word=unique_words))\n",
    "#      # calculate the idf for each word\n",
    "#      .assign(idf=lambda df: df.word.apply(calc_idf))\n",
    "#      # sort the data for presentation purposes\n",
    "#      .set_index('word')\n",
    "#      .sort_values(by='idf', ascending=False))\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# groups=['type', 'domain', 'i|e', 'n|s', 't|f', 'j|p']\n",
    "# allthewaydict={'group':{},'subgroup':{},'name':{},'docs':{},'lemmatized docs':{},'tf-idf w.r.t name':{},'freq table w.r.t name':{},'handle':{},'date':{},'lang'\t:{},'likeCount':{},'retweetCount':{}}\n",
    "# # groups=(groups[2:4])\n",
    "# count=0\n",
    "# g=groups[0]\n",
    "# print(f'{g}\\n')\n",
    "# group=tryagain.groupby(g)\n",
    "# keys=group.groups.keys()\n",
    "# # for key in keys:\n",
    "# key=list(keys)[0]\n",
    "# # print(key)\n",
    "# kthgroup=group.get_group(key)\n",
    "# namegru=kthgroup.groupby('name')\n",
    "# subkeys=namegru.groups.keys()\n",
    "# # for sk in subkeys:\n",
    "# #     count+=\n",
    "# curname=namegru.get_group(list(subkeys)[4])\n",
    "# display(curname)\n",
    "# nm=curname.name.unique()[0]\n",
    "# docs=curname.content.values\n",
    "# # splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "# lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "# lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "# fulllist=\" \".join(lemma).split()\n",
    "\n",
    "\n",
    "# valcounts=pd.Series(fulllist).value_counts()\n",
    "# freq=valcounts.values\n",
    "\n",
    "\n",
    "# ##TF-IDF calc\n",
    "# x=np.log10((len(lemma)/freq))\n",
    "# x*=freq\n",
    "# x=np.ndarray.round(x,3)\n",
    "# tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n",
    "# freqtable=dict(valcounts)\n",
    "\n",
    "# # df=idfDf(lemma);display(df)\n",
    "# allthewaydict['group'].update({count:g})\n",
    "# allthewaydict['subgroup'].update({count:key})\n",
    "# allthewaydict['name'].update({count:nm})\n",
    "# allthewaydict['docs'].update({count:docs})\n",
    "# allthewaydict['lemmatized docs'].update({count:lemma})\n",
    "# allthewaydict['tf-idf w.r.t name'].update({count:tf_idf})\n",
    "# allthewaydict['freq table w.r.t name'].update({count:freqtable})\n",
    "\n",
    "# allthewaydict['handle'].update({count:curname.handle.values[0]})\n",
    "# allthewaydict['date'].update({count:curname.date.values})\n",
    "# allthewaydict['lang'].update({count:curname.lang.values[0]})\n",
    "# allthewaydict['likeCount'].update({count:curname.likeCount.values.sum()})\n",
    "# allthewaydict['retweetCount'].update({count:curname.retweetCount.values.sum()})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# count+=1\n",
    "\n",
    "# pd.DataFrame(allthewaydict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep is finished \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explore is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>content</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>domain</th>\n",
       "      <th>i|e</th>\n",
       "      <th>n|s</th>\n",
       "      <th>t|f</th>\n",
       "      <th>j|p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>if you wanna know why any human is they way th...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-30 22:15:09+00:00</td>\n",
       "      <td>29840</td>\n",
       "      <td>4575</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-29 07:01:46+00:00</td>\n",
       "      <td>10502</td>\n",
       "      <td>881</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>heck I pour beer out of my tits (that‚Äôs a part...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:44+00:00</td>\n",
       "      <td>3042</td>\n",
       "      <td>297</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>The show‚Äôs set list is a fun üé¢  through memory...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:07:21+00:00</td>\n",
       "      <td>3654</td>\n",
       "      <td>336</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>enfj</td>\n",
       "      <td>katy perry</td>\n",
       "      <td>Welcoming all my #flatearthers #spaceisfakers ...</td>\n",
       "      <td>katyperry</td>\n",
       "      <td>2022-10-27 19:05:27+00:00</td>\n",
       "      <td>20819</td>\n",
       "      <td>2318</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496999</th>\n",
       "      <td>esfp</td>\n",
       "      <td>juicy j</td>\n",
       "      <td>Block that devil out your ear this morning  #s...</td>\n",
       "      <td>therealjuicyj</td>\n",
       "      <td>2022-03-17 15:04:10+00:00</td>\n",
       "      <td>511</td>\n",
       "      <td>148</td>\n",
       "      <td>explorer</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497228</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Spaces?</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-10-30 02:13:17+00:00</td>\n",
       "      <td>2126</td>\n",
       "      <td>91</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497240</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-09-28 22:29:07+00:00</td>\n",
       "      <td>4395</td>\n",
       "      <td>306</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497285</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>.ebaselE</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2021-08-13 23:31:10+00:00</td>\n",
       "      <td>2937</td>\n",
       "      <td>142</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497395</th>\n",
       "      <td>enfp</td>\n",
       "      <td>manu gavassi</td>\n",
       "      <td>Slow down, you‚Äôre doing fine ‚ù§Ô∏è</td>\n",
       "      <td>manugavassi</td>\n",
       "      <td>2020-09-30 04:33:31+00:00</td>\n",
       "      <td>13170</td>\n",
       "      <td>1225</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182352 rows √ó 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        type          name                                            content  \\\n",
       "0       enfj    katy perry  if you wanna know why any human is they way th...   \n",
       "1       enfj    katy perry  wait, if it‚Äôs called a ‚Äúfeed‚Äù are we literally...   \n",
       "3       enfj    katy perry  heck I pour beer out of my tits (that‚Äôs a part...   \n",
       "4       enfj    katy perry  The show‚Äôs set list is a fun üé¢  through memory...   \n",
       "5       enfj    katy perry  Welcoming all my #flatearthers #spaceisfakers ...   \n",
       "...      ...           ...                                                ...   \n",
       "496999  esfp       juicy j  Block that devil out your ear this morning  #s...   \n",
       "497228  enfp  manu gavassi                                            Spaces?   \n",
       "497240  enfp  manu gavassi  QUERO MORAR NESSE LOOK HELP. üòç #MPN2021 https:...   \n",
       "497285  enfp  manu gavassi                                           .ebaselE   \n",
       "497395  enfp  manu gavassi                    Slow down, you‚Äôre doing fine ‚ù§Ô∏è   \n",
       "\n",
       "               handle                      date  likeCount  retweetCount  \\\n",
       "0           katyperry 2022-10-30 22:15:09+00:00      29840          4575   \n",
       "1           katyperry 2022-10-29 07:01:46+00:00      10502           881   \n",
       "3           katyperry 2022-10-27 19:07:44+00:00       3042           297   \n",
       "4           katyperry 2022-10-27 19:07:21+00:00       3654           336   \n",
       "5           katyperry 2022-10-27 19:05:27+00:00      20819          2318   \n",
       "...               ...                       ...        ...           ...   \n",
       "496999  therealjuicyj 2022-03-17 15:04:10+00:00        511           148   \n",
       "497228    manugavassi 2021-10-30 02:13:17+00:00       2126            91   \n",
       "497240    manugavassi 2021-09-28 22:29:07+00:00       4395           306   \n",
       "497285    manugavassi 2021-08-13 23:31:10+00:00       2937           142   \n",
       "497395    manugavassi 2020-09-30 04:33:31+00:00      13170          1225   \n",
       "\n",
       "          domain i|e n|s t|f j|p  \n",
       "0       diplomat   e   n   f   j  \n",
       "1       diplomat   e   n   f   j  \n",
       "3       diplomat   e   n   f   j  \n",
       "4       diplomat   e   n   f   j  \n",
       "5       diplomat   e   n   f   j  \n",
       "...          ...  ..  ..  ..  ..  \n",
       "496999  explorer   e   s   f   p  \n",
       "497228  diplomat   e   n   f   p  \n",
       "497240  diplomat   e   n   f   p  \n",
       "497285  diplomat   e   n   f   p  \n",
       "497395  diplomat   e   n   f   p  \n",
       "\n",
       "[182352 rows x 12 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['group', 'subgroup', 'name', 'docs', 'lemmatized docs',\n",
       "       'tf-idf w.r.t name', 'freq table w.r.t name', 'handle', 'date', 'lang',\n",
       "       'likeCount', 'retweetCount'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tryagain=pd.read_pickle('mvp_plus_1_train.pkl')\n",
    "\n",
    "tryagain.columns\n",
    "\n",
    "\n",
    "\n",
    "# s = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "# df['sentiment'] = df.lemmatized.apply(lambda doc: s.polarity_scores(doc)['compound'])\n",
    "# df['message_length'] = df['lemmatized'].str.len()\n",
    "# df['word_count'] = (df['lemmatized'].str.split(' ').apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "domain\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('domain', 'analyst')\n",
      "\n",
      "\n",
      "new         2083\n",
      "love        1559\n",
      "us          1305\n",
      "today       1162\n",
      "day         1157\n",
      "            ... \n",
      "nsx            1\n",
      "supercar       1\n",
      "realdeal       1\n",
      "nothings       1\n",
      "sorcerer       1\n",
      "Length: 23089, dtype: int64\n",
      "\n",
      "\n",
      "('domain', 'diplomat')\n",
      "\n",
      "\n",
      "love       3190\n",
      "thank      2803\n",
      "new        2416\n",
      "day        1903\n",
      "bit        1838\n",
      "           ... \n",
      "brum          1\n",
      "eventim       1\n",
      "roid          1\n",
      "hangers       1\n",
      "schmidt       1\n",
      "Length: 24698, dtype: int64\n",
      "\n",
      "\n",
      "('domain', 'explorer')\n",
      "\n",
      "\n",
      "love       4797\n",
      "new        4675\n",
      "thank      4094\n",
      "happy      3121\n",
      "day        2740\n",
      "           ... \n",
      "maggies       1\n",
      "thong         1\n",
      "byeeeee       1\n",
      "papiace       1\n",
      "liwvid        1\n",
      "Length: 33204, dtype: int64\n",
      "\n",
      "\n",
      "('domain', 'sentinel')\n",
      "\n",
      "\n",
      "love        3095\n",
      "thank       2709\n",
      "new         2678\n",
      "happy       2176\n",
      "today       1920\n",
      "            ... \n",
      "fabusix        1\n",
      "arathi         1\n",
      "reflec         1\n",
      "corrects       1\n",
      "lunesss        1\n",
      "Length: 23094, dtype: int64\n",
      "\n",
      "\n",
      "i|e\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('i|e', 'e')\n",
      "\n",
      "\n",
      "love       8532\n",
      "new        8054\n",
      "thank      6756\n",
      "happy      5325\n",
      "today      5185\n",
      "           ... \n",
      "blunty        1\n",
      "swamped       1\n",
      "vue           1\n",
      "schick        1\n",
      "bello         1\n",
      "Length: 48070, dtype: int64\n",
      "\n",
      "\n",
      "('i|e', 'i')\n",
      "\n",
      "\n",
      "love        4109\n",
      "thank       3962\n",
      "new         3798\n",
      "happy       2588\n",
      "bit         2481\n",
      "            ... \n",
      "pyine          1\n",
      "vadivelu       1\n",
      "theknot        1\n",
      "attire         1\n",
      "sorcerer       1\n",
      "Length: 29607, dtype: int64\n",
      "\n",
      "\n",
      "j|p\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('j|p', 'j')\n",
      "\n",
      "\n",
      "love        5581\n",
      "new         4832\n",
      "thank       4572\n",
      "happy       3469\n",
      "today       3357\n",
      "            ... \n",
      "pnd            1\n",
      "fbgovo         1\n",
      "sprucing       1\n",
      "sprite         1\n",
      "prfctbws       1\n",
      "Length: 32601, dtype: int64\n",
      "\n",
      "\n",
      "('j|p', 'p')\n",
      "\n",
      "\n",
      "love       7060\n",
      "new        7020\n",
      "thank      6146\n",
      "happy      4444\n",
      "day        4323\n",
      "           ... \n",
      "kinnear       1\n",
      "gogol         1\n",
      "kirill        1\n",
      "sylvest       1\n",
      "bello         1\n",
      "Length: 45929, dtype: int64\n",
      "\n",
      "\n",
      "n|s\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('n|s', 'n')\n",
      "\n",
      "\n",
      "love        4749\n",
      "new         4499\n",
      "thank       3915\n",
      "day         3060\n",
      "us          2919\n",
      "            ... \n",
      "ignition       1\n",
      "kirilmkd       1\n",
      "fatgazbo       1\n",
      "pavement       1\n",
      "schmidt        1\n",
      "Length: 35956, dtype: int64\n",
      "\n",
      "\n",
      "('n|s', 's')\n",
      "\n",
      "\n",
      "love        7892\n",
      "new         7353\n",
      "thank       6803\n",
      "happy       5297\n",
      "today       4589\n",
      "            ... \n",
      "sinu           1\n",
      "volcanic       1\n",
      "cites          1\n",
      "stitched       1\n",
      "liwvid         1\n",
      "Length: 42977, dtype: int64\n",
      "\n",
      "\n",
      "type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('type', 'enfj')\n",
      "\n",
      "\n",
      "love        974\n",
      "thank       861\n",
      "new         698\n",
      "god         578\n",
      "today       556\n",
      "           ... \n",
      "sup           1\n",
      "capacity      1\n",
      "brasil        1\n",
      "siberian      1\n",
      "prfctbws      1\n",
      "Length: 11136, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'enfp')\n",
      "\n",
      "\n",
      "love       1306\n",
      "thank      1128\n",
      "new        1080\n",
      "day         814\n",
      "live        793\n",
      "           ... \n",
      "twofer        1\n",
      "krakens       1\n",
      "bennett       1\n",
      "netto         1\n",
      "schmidt       1\n",
      "Length: 15456, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'entj')\n",
      "\n",
      "\n",
      "live        676\n",
      "new         564\n",
      "love        555\n",
      "vs          428\n",
      "tonight     411\n",
      "           ... \n",
      "cheapest      1\n",
      "ritz          1\n",
      "sits          1\n",
      "assad         1\n",
      "aranburu      1\n",
      "Length: 9151, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'entp')\n",
      "\n",
      "\n",
      "new         954\n",
      "love        629\n",
      "us          586\n",
      "thank       563\n",
      "thanks      553\n",
      "           ... \n",
      "sword         1\n",
      "thc           1\n",
      "starving      1\n",
      "silicone      1\n",
      "shanah        1\n",
      "Length: 16586, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'esfj')\n",
      "\n",
      "\n",
      "love        1524\n",
      "thank       1204\n",
      "happy       1175\n",
      "new         1171\n",
      "day          914\n",
      "            ... \n",
      "weeps          1\n",
      "diluting       1\n",
      "booking        1\n",
      "liter          1\n",
      "kishan         1\n",
      "Length: 14564, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'esfp')\n",
      "\n",
      "\n",
      "love        1984\n",
      "new         1923\n",
      "thank       1672\n",
      "happy       1227\n",
      "today       1157\n",
      "            ... \n",
      "moosteph       1\n",
      "finn           1\n",
      "cristal        1\n",
      "kofler         1\n",
      "fleek          1\n",
      "Length: 20367, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'estj')\n",
      "\n",
      "\n",
      "new        638\n",
      "today      579\n",
      "check      501\n",
      "great      491\n",
      "love       489\n",
      "          ... \n",
      "ydkpwq       1\n",
      "thxs         1\n",
      "itzhak       1\n",
      "okgrind      1\n",
      "lunesss      1\n",
      "Length: 8483, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'estp')\n",
      "\n",
      "\n",
      "love        1071\n",
      "new         1026\n",
      "happy        734\n",
      "day          641\n",
      "thank        600\n",
      "            ... \n",
      "clever         1\n",
      "realizes       1\n",
      "arbs           1\n",
      "srilanka       1\n",
      "waddle         1\n",
      "Length: 13649, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'infj')\n",
      "\n",
      "\n",
      "love        679\n",
      "bit         622\n",
      "ly          605\n",
      "thank       541\n",
      "day         507\n",
      "           ... \n",
      "arshdeep      1\n",
      "ucl           1\n",
      "savings       1\n",
      "bravest       1\n",
      "lacking       1\n",
      "Length: 8745, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'infp')\n",
      "\n",
      "\n",
      "ly          505\n",
      "bit         476\n",
      "friends     356\n",
      "dear        316\n",
      "thank       273\n",
      "           ... \n",
      "forrest       1\n",
      "gump          1\n",
      "grimmest      1\n",
      "emphasis      1\n",
      "fitnesiz      1\n",
      "Length: 6100, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'intj')\n",
      "\n",
      "\n",
      "new         408\n",
      "love        278\n",
      "bit         255\n",
      "ly          231\n",
      "us          226\n",
      "           ... \n",
      "valley        1\n",
      "gino          1\n",
      "bartali       1\n",
      "relating      1\n",
      "finger        1\n",
      "Length: 5969, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'intp')\n",
      "\n",
      "\n",
      "us          165\n",
      "new         157\n",
      "world       153\n",
      "gat         151\n",
      "gatesnot    133\n",
      "           ... \n",
      "dreamy        1\n",
      "wander        1\n",
      "oculus        1\n",
      "ragu          1\n",
      "garb          1\n",
      "Length: 6331, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'isfj')\n",
      "\n",
      "\n",
      "thank      941\n",
      "love       897\n",
      "new        710\n",
      "happy      594\n",
      "day        478\n",
      "          ... \n",
      "mash         1\n",
      "probum       1\n",
      "scho         1\n",
      "grindin      1\n",
      "ignite       1\n",
      "Length: 10497, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'isfp')\n",
      "\n",
      "\n",
      "thank       1305\n",
      "love        1271\n",
      "new         1142\n",
      "happy        698\n",
      "today        640\n",
      "            ... \n",
      "triumphs       1\n",
      "christ         1\n",
      "christys       1\n",
      "peso           1\n",
      "liwvid         1\n",
      "Length: 11913, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'istj')\n",
      "\n",
      "\n",
      "great      192\n",
      "love       185\n",
      "thank      183\n",
      "new        159\n",
      "us         136\n",
      "          ... \n",
      "patriot      1\n",
      "illness      1\n",
      "unlike       1\n",
      "lat          1\n",
      "boostbx      1\n",
      "Length: 4796, dtype: int64\n",
      "\n",
      "\n",
      "('type', 'istp')\n",
      "\n",
      "\n",
      "new         584\n",
      "thank       517\n",
      "love        471\n",
      "happy       462\n",
      "iiii        459\n",
      "           ... \n",
      "stresses      1\n",
      "trivial       1\n",
      "spirited      1\n",
      "agnostic      1\n",
      "yolanthe      1\n",
      "Length: 10629, dtype: int64\n",
      "\n",
      "\n",
      "t|f\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "('t|f', 'f')\n",
      "\n",
      "\n",
      "love        8866\n",
      "thank       7925\n",
      "new         7362\n",
      "happy       5401\n",
      "day         5026\n",
      "            ... \n",
      "scanning       1\n",
      "cbynovel       1\n",
      "patsugui       1\n",
      "lori           1\n",
      "bello          1\n",
      "Length: 44180, dtype: int64\n",
      "\n",
      "\n",
      "('t|f', 't')\n",
      "\n",
      "\n",
      "new         4490\n",
      "love        3775\n",
      "thank       2793\n",
      "today       2739\n",
      "happy       2512\n",
      "            ... \n",
      "caziew         1\n",
      "mcgee          1\n",
      "beeb           1\n",
      "morrisey       1\n",
      "sorcerer       1\n",
      "Length: 34451, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "subgroupdict={\n",
    "'subgroup':{}, \n",
    "'names':{}, \n",
    "'docs':{}, \n",
    "'lemmatized docs':{},\n",
    "'handle':{}, \n",
    "'date':{}, \n",
    "'lang':{},\n",
    "'likeCount':{},\n",
    "'retweetCount':{},\n",
    "'docs w.r.t subgroup':{}, \n",
    "'lemmatized docs w.r.t subgroup':{},\n",
    "'freq table w.r.t subgroup':{},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "groups=tryagain.groupby('group')\n",
    "keys=groups.groups.keys()\n",
    "for k in keys:\n",
    "    cur=groups.get_group(k)\n",
    "    curgroup=cur.groupby(['group','subgroup'])\n",
    "    curkeys=curgroup.groups.keys()\n",
    "    \n",
    "    print('\\n')\n",
    "    print(k)\n",
    "    print('\\n')\n",
    "    for c in curkeys:\n",
    "        print('\\n')\n",
    "        print(c)\n",
    "        print('\\n')\n",
    "        subcur=cur=curgroup.get_group(c)\n",
    "        # display(subcur)\n",
    "        subgroupdict['subgroup'].update({count:subcur['subgroup'].values[0]})\n",
    "        subgroupdict['names'].update({count:subcur['name'].values})\n",
    "        subgroupdict['docs'].update({count:subcur['docs'].values})\n",
    "        subgroupdict['lemmatized docs'].update({count:subcur['lemmatized docs'].values})\n",
    "        subgroupdict['handle'].update({count:subcur['handle'].values})\n",
    "        subgroupdict['date'].update({count:subcur['date'].values})\n",
    "        subgroupdict['lang'].update({count:subcur['lang'].unique()[0]})\n",
    "        subgroupdict['likeCount'].update({count:sum(subcur['likeCount'].values)})\n",
    "        subgroupdict['retweetCount'].update({count:sum(subcur['retweetCount'].values)})\n",
    "\n",
    "\n",
    "        \n",
    "        docslist= subcur['docs'].values\n",
    "        lemmatized=[]\n",
    "        [lemmatized.extend(i) for i in (subcur['lemmatized docs'].values)]\n",
    "\n",
    "        ## lemmatized is now in sentenaces\n",
    "        docs=[]\n",
    "        [docs.extend(i) for i in docslist]\n",
    "        # lemmatized=' '.join(lemmatized).split(',')\n",
    "        valcounts=pd.Series(lemmatized).value_counts()\n",
    "        \n",
    "        lemmatized=' '.join(lemmatized)\n",
    "        valcounts=pd.Series(lemmatized.split()).value_counts();print(valcounts)\n",
    "        freq=valcounts.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        subgroupdict['docs w.r.t subgroup'].update({count:docs})\n",
    "        subgroupdict['lemmatized docs w.r.t subgroup'].update({count:lemmatized})\n",
    "        subgroupdict['freq table w.r.t subgroup'].update({count:dict(valcounts)})\n",
    "\n",
    "        count+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subgroupDf=pd.DataFrame(subgroupdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "istp\n",
      "istj\n",
      "isfp\n",
      "isfj\n",
      "intp\n",
      "intj\n",
      "infp\n",
      "infj\n",
      "estp\n",
      "estj\n",
      "esfp\n",
      "esfj\n",
      "entp\n",
      "entj\n",
      "enfp\n",
      "enfj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subgroup</th>\n",
       "      <th>names</th>\n",
       "      <th>docs</th>\n",
       "      <th>lemmatized docs</th>\n",
       "      <th>handle</th>\n",
       "      <th>date</th>\n",
       "      <th>lang</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>docs w.r.t subgroup</th>\n",
       "      <th>lemmatized docs w.r.t subgroup</th>\n",
       "      <th>freq table w.r.t subgroup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>analyst</td>\n",
       "      <td>[a.r.rahman, ab de villiers, al yankovic, andr...</td>\n",
       "      <td>[[Glad to launch  #ThikattaThikattaKathalippom...</td>\n",
       "      <td>[[glad launch aneethi youtu jwalik film musica...</td>\n",
       "      <td>[arrahman, ABdeVilliers17, alyankovic, Buenafu...</td>\n",
       "      <td>[[2022-10-31T12:33:14.000000000, 2022-10-29T14...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>338501446</td>\n",
       "      <td>38455192</td>\n",
       "      <td>[Glad to launch  #ThikattaThikattaKathalippom ...</td>\n",
       "      <td>glad launch aneethi youtu jwalik film musical ...</td>\n",
       "      <td>{'new': 2083, 'love': 1559, 'us': 1305, 'today...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>diplomat</td>\n",
       "      <td>[agnez mo, ahmet kural, alicia keys, angelica ...</td>\n",
       "      <td>[[@netflix This Messiah series thoooo üî•üî•üî•üî• (ca...</td>\n",
       "      <td>[[netflix messiah series thoooo cant stop watc...</td>\n",
       "      <td>[agnezmo, AHMTKURAL, aliciakeys, angelicavale,...</td>\n",
       "      <td>[[2022-10-04T02:12:34.000000000, 2022-07-07T05...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>546218964</td>\n",
       "      <td>92457260</td>\n",
       "      <td>[@netflix This Messiah series thoooo üî•üî•üî•üî• (can...</td>\n",
       "      <td>netflix messiah series thoooo cant stop watchi...</td>\n",
       "      <td>{'love': 3190, 'thank': 2803, 'new': 2416, 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explorer</td>\n",
       "      <td>[50cent, adam lambert, adam levine, ajay devgn...</td>\n",
       "      <td>[[Athens Greece same stadium where they did th...</td>\n",
       "      <td>[[athens greece stadium olympics many lol gian...</td>\n",
       "      <td>[50cent, adamlambert, adamlevine, ajaydevgn, A...</td>\n",
       "      <td>[[2022-10-31T00:30:53.000000000, 2022-10-30T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>850072105</td>\n",
       "      <td>98842116</td>\n",
       "      <td>[Athens Greece same stadium where they did the...</td>\n",
       "      <td>athens greece stadium olympics many lol giants...</td>\n",
       "      <td>{'love': 4797, 'new': 4675, 'thank': 4094, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentinel</td>\n",
       "      <td>[adal ramones, adele, akshay kumar, alejandro ...</td>\n",
       "      <td>[[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...</td>\n",
       "      <td>[[drnetas sublime, drnetas sublime, sorry, qtf...</td>\n",
       "      <td>[AdalRamones, Adele, akshaykumar, alexoficial,...</td>\n",
       "      <td>[[2020-06-19T19:27:39.000000000, 2020-06-19T19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>305813278</td>\n",
       "      <td>35556521</td>\n",
       "      <td>[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...</td>\n",
       "      <td>drnetas sublime drnetas sublime sorry qtf recu...</td>\n",
       "      <td>{'love': 3095, 'thank': 2709, 'new': 2678, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>[50cent, ab de villiers, adal ramones, adam la...</td>\n",
       "      <td>[[Athens Greece same stadium where they did th...</td>\n",
       "      <td>[[athens greece stadium olympics many lol gian...</td>\n",
       "      <td>[50cent, ABdeVilliers17, AdalRamones, adamlamb...</td>\n",
       "      <td>[[2022-10-31T00:30:53.000000000, 2022-10-30T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1301054647</td>\n",
       "      <td>166768529</td>\n",
       "      <td>[Athens Greece same stadium where they did the...</td>\n",
       "      <td>athens greece stadium olympics many lol giants...</td>\n",
       "      <td>{'love': 8532, 'new': 8054, 'thank': 6756, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i</td>\n",
       "      <td>[a.r.rahman, ahmet kural, ajay devgn, akshay k...</td>\n",
       "      <td>[[Glad to launch  #ThikattaThikattaKathalippom...</td>\n",
       "      <td>[[glad launch aneethi youtu jwalik film musica...</td>\n",
       "      <td>[arrahman, AHMTKURAL, ajaydevgn, akshaykumar, ...</td>\n",
       "      <td>[[2022-10-31T12:33:14.000000000, 2022-10-29T14...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>739551146</td>\n",
       "      <td>98542560</td>\n",
       "      <td>[Glad to launch  #ThikattaThikattaKathalippom ...</td>\n",
       "      <td>glad launch aneethi youtu jwalik film musical ...</td>\n",
       "      <td>{'love': 4109, 'thank': 3962, 'new': 3798, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>j</td>\n",
       "      <td>[adal ramones, adele, akshay kumar, alejandro ...</td>\n",
       "      <td>[[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...</td>\n",
       "      <td>[[drnetas sublime, drnetas sublime, sorry, qtf...</td>\n",
       "      <td>[AdalRamones, Adele, akshaykumar, alexoficial,...</td>\n",
       "      <td>[[2020-06-19T19:27:39.000000000, 2020-06-19T19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>632887869</td>\n",
       "      <td>84979590</td>\n",
       "      <td>[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...</td>\n",
       "      <td>drnetas sublime drnetas sublime sorry qtf recu...</td>\n",
       "      <td>{'love': 5581, 'new': 4832, 'thank': 4572, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p</td>\n",
       "      <td>[50cent, a.r.rahman, ab de villiers, adam lamb...</td>\n",
       "      <td>[[Athens Greece same stadium where they did th...</td>\n",
       "      <td>[[athens greece stadium olympics many lol gian...</td>\n",
       "      <td>[50cent, arrahman, ABdeVilliers17, adamlambert...</td>\n",
       "      <td>[[2022-10-31T00:30:53.000000000, 2022-10-30T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1407717924</td>\n",
       "      <td>180331499</td>\n",
       "      <td>[Athens Greece same stadium where they did the...</td>\n",
       "      <td>athens greece stadium olympics many lol giants...</td>\n",
       "      <td>{'love': 7060, 'new': 7020, 'thank': 6146, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n</td>\n",
       "      <td>[a.r.rahman, ab de villiers, agnez mo, ahmet k...</td>\n",
       "      <td>[[Glad to launch  #ThikattaThikattaKathalippom...</td>\n",
       "      <td>[[glad launch aneethi youtu jwalik film musica...</td>\n",
       "      <td>[arrahman, ABdeVilliers17, agnezmo, AHMTKURAL,...</td>\n",
       "      <td>[[2022-10-31T12:33:14.000000000, 2022-10-29T14...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>884720410</td>\n",
       "      <td>130912452</td>\n",
       "      <td>[Glad to launch  #ThikattaThikattaKathalippom ...</td>\n",
       "      <td>glad launch aneethi youtu jwalik film musical ...</td>\n",
       "      <td>{'love': 4749, 'new': 4499, 'thank': 3915, 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>s</td>\n",
       "      <td>[50cent, adal ramones, adam lambert, adam levi...</td>\n",
       "      <td>[[Athens Greece same stadium where they did th...</td>\n",
       "      <td>[[athens greece stadium olympics many lol gian...</td>\n",
       "      <td>[50cent, AdalRamones, adamlambert, adamlevine,...</td>\n",
       "      <td>[[2022-10-31T00:30:53.000000000, 2022-10-30T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1155885383</td>\n",
       "      <td>134398637</td>\n",
       "      <td>[Athens Greece same stadium where they did the...</td>\n",
       "      <td>athens greece stadium olympics many lol giants...</td>\n",
       "      <td>{'love': 7892, 'new': 7353, 'thank': 6803, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>enfj</td>\n",
       "      <td>[anggun official, ansel elgort, ashton irwin, ...</td>\n",
       "      <td>[[#EliHallo - Behind the scene photos of the m...</td>\n",
       "      <td>[[elihallo behind scene photos music video eli...</td>\n",
       "      <td>[Anggun_Cipta, AnselElgort, Ashton5SOS, BSchwe...</td>\n",
       "      <td>[[2022-09-27T19:40:04.000000000, 2022-09-23T15...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99837793</td>\n",
       "      <td>17779984</td>\n",
       "      <td>[#EliHallo - Behind the scene photos of the mu...</td>\n",
       "      <td>elihallo behind scene photos music video eliha...</td>\n",
       "      <td>{'love': 974, 'thank': 861, 'new': 698, 'god':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>enfp</td>\n",
       "      <td>[agnez mo, ayushmann khurrana, aziz ansari, br...</td>\n",
       "      <td>[[@netflix This Messiah series thoooo üî•üî•üî•üî• (ca...</td>\n",
       "      <td>[[netflix messiah series thoooo cant stop watc...</td>\n",
       "      <td>[agnezmo, ayushmannk, azizansari, brookeburke,...</td>\n",
       "      <td>[[2022-10-04T02:12:34.000000000, 2022-07-07T05...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>372188286</td>\n",
       "      <td>59229712</td>\n",
       "      <td>[@netflix This Messiah series thoooo üî•üî•üî•üî• (can...</td>\n",
       "      <td>netflix messiah series thoooo cant stop watchi...</td>\n",
       "      <td>{'love': 1306, 'thank': 1128, 'new': 1080, 'da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>entj</td>\n",
       "      <td>[busta rhymes, chris paul, dana white, dr. dre...</td>\n",
       "      <td>[[#SLAP BY THE üêâ ft. @WHOISCONWAY &amp;amp; @bigda...</td>\n",
       "      <td>[[slap ft listen gt linktr ee, slap taken stre...</td>\n",
       "      <td>[BustaRhymes, CP3, danawhite, drdre, LizGillie...</td>\n",
       "      <td>[[2022-10-31T14:56:45.000000000, 2022-10-28T17...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78219610</td>\n",
       "      <td>10582136</td>\n",
       "      <td>[#SLAP BY THE üêâ ft. @WHOISCONWAY &amp;amp; @bigdad...</td>\n",
       "      <td>slap ft listen gt linktr ee slap taken streets...</td>\n",
       "      <td>{'live': 676, 'new': 564, 'love': 555, 'vs': 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>entp</td>\n",
       "      <td>[ab de villiers, al yankovic, andreu buenafuen...</td>\n",
       "      <td>[[Dewald Brevis. No need to say more, Back on!...</td>\n",
       "      <td>[[dewald brevis need, back bicycle session don...</td>\n",
       "      <td>[ABdeVilliers17, alyankovic, Buenafuente, Anna...</td>\n",
       "      <td>[[2022-10-31T13:36:16.000000000, 2022-10-30T13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155514681</td>\n",
       "      <td>17789200</td>\n",
       "      <td>[Dewald Brevis. No need to say more, Back on! ...</td>\n",
       "      <td>dewald brevis need back bicycle session done j...</td>\n",
       "      <td>{'new': 954, 'love': 629, 'us': 586, 'thank': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>esfj</td>\n",
       "      <td>[adal ramones, alejandro fernandez, aline barr...</td>\n",
       "      <td>[[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...</td>\n",
       "      <td>[[drnetas sublime, drnetas sublime, sorry, qtf...</td>\n",
       "      <td>[AdalRamones, alexoficial, alinebarros, ahickm...</td>\n",
       "      <td>[[2020-06-19T19:27:39.000000000, 2020-06-19T19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159203758</td>\n",
       "      <td>17428252</td>\n",
       "      <td>[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...</td>\n",
       "      <td>drnetas sublime drnetas sublime sorry qtf recu...</td>\n",
       "      <td>{'love': 1524, 'thank': 1204, 'happy': 1175, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>esfp</td>\n",
       "      <td>[adam lambert, adam levine, akon, alyssa milan...</td>\n",
       "      <td>[[Starting in a few minutes!! @veeps\\n\\nGet yo...</td>\n",
       "      <td>[[starting minutes veeps tickets quick veeps, ...</td>\n",
       "      <td>[adamlambert, adamlevine, Akon, Alyssa_Milano,...</td>\n",
       "      <td>[[2022-10-31T02:49:14.000000000, 2022-10-30T19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>308686216</td>\n",
       "      <td>29566575</td>\n",
       "      <td>[Starting in a few minutes!! @veeps\\n\\nGet you...</td>\n",
       "      <td>starting minutes veeps tickets quick veeps ton...</td>\n",
       "      <td>{'love': 1984, 'new': 1923, 'thank': 1672, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>estj</td>\n",
       "      <td>[adele, cyprien, dr. mehmet oz, flo rida, giov...</td>\n",
       "      <td>[[Thank you Joe Talbot for creating a world fo...</td>\n",
       "      <td>[[thank joe talbot creating world putting bunc...</td>\n",
       "      <td>[Adele, MonsieurDream, DrOz, official_flo, gio...</td>\n",
       "      <td>[[2022-10-26T20:59:05.000000000, 2022-10-26T20...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19586610</td>\n",
       "      <td>2654764</td>\n",
       "      <td>[Thank you Joe Talbot for creating a world for...</td>\n",
       "      <td>thank joe talbot creating world putting bunch ...</td>\n",
       "      <td>{'new': 638, 'today': 579, 'check': 501, 'grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>estp</td>\n",
       "      <td>[50cent, ashley benson, bipasha basu, birdman,...</td>\n",
       "      <td>[[Athens Greece same stadium where they did th...</td>\n",
       "      <td>[[athens greece stadium olympics many lol gian...</td>\n",
       "      <td>[50cent, AshBenzo, bipsluvurself, BIRDMAN5STAR...</td>\n",
       "      <td>[[2022-10-31T00:30:53.000000000, 2022-10-30T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>107817693</td>\n",
       "      <td>11737906</td>\n",
       "      <td>[Athens Greece same stadium where they did the...</td>\n",
       "      <td>athens greece stadium olympics many lol giants...</td>\n",
       "      <td>{'love': 1071, 'new': 1026, 'happy': 734, 'day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>infj</td>\n",
       "      <td>[angelica vale, arjun rampal, bridgit mendler,...</td>\n",
       "      <td>[[¬°Feliz tarde! ¬øComo va su d√≠a? ¬°Espero espec...</td>\n",
       "      <td>[[feliz tarde va dia espero happy day going ho...</td>\n",
       "      <td>[angelicavale, rampalarjun, bridgitmendler, ch...</td>\n",
       "      <td>[[2022-10-27T23:35:42.000000000, 2022-10-16T11...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52800843</td>\n",
       "      <td>11759702</td>\n",
       "      <td>[¬°Feliz tarde! ¬øComo va su d√≠a? ¬°Espero espect...</td>\n",
       "      <td>feliz tarde va dia espero happy day going hope...</td>\n",
       "      <td>{'love': 679, 'bit': 622, 'ly': 605, 'thank': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>infp</td>\n",
       "      <td>[ahmet kural, alicia keys, calum hood, david l...</td>\n",
       "      <td>[[Fitnesiz, No fitne ü•ä @konuralpkocoglu #thesh...</td>\n",
       "      <td>[[fitnesiz fitne crossfit suadiye, posted phot...</td>\n",
       "      <td>[AHMTKURAL, aliciakeys, Calum5SOS, DAVID_LYNCH...</td>\n",
       "      <td>[[2018-08-01T11:29:09.000000000, 2017-01-01T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21392042</td>\n",
       "      <td>3687862</td>\n",
       "      <td>[Fitnesiz, No fitne ü•ä @konuralpkocoglu #thesho...</td>\n",
       "      <td>fitnesiz fitne crossfit suadiye posted photo b...</td>\n",
       "      <td>{'ly': 505, 'bit': 476, 'friends': 356, 'dear'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>intj</td>\n",
       "      <td>[elon musk, g-dragon, hardwell, jerry seinfeld...</td>\n",
       "      <td>[[@JennaEllisEsq @RealMarkFinchem Looking into...</td>\n",
       "      <td>[[looking, evafoxu, sep diageo news bjankow ki...</td>\n",
       "      <td>[elonmusk, IBGDRGN, HARDWELL, JerrySeinfeld, l...</td>\n",
       "      <td>[[2022-10-31T22:55:11.000000000, 2022-10-31T22...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96216345</td>\n",
       "      <td>9301247</td>\n",
       "      <td>[@JennaEllisEsq @RealMarkFinchem Looking into ...</td>\n",
       "      <td>looking evafoxu sep diageo news bjankow kieley...</td>\n",
       "      <td>{'new': 408, 'love': 278, 'bit': 255, 'ly': 23...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>intp</td>\n",
       "      <td>[a.r.rahman, bill gates, felipe andreoli, gary...</td>\n",
       "      <td>[[Glad to launch  #ThikattaThikattaKathalippom...</td>\n",
       "      <td>[[glad launch aneethi youtu jwalik film musica...</td>\n",
       "      <td>[arrahman, BillGates, andreolifelipe, GNev2, i...</td>\n",
       "      <td>[[2022-10-31T12:33:14.000000000, 2022-10-29T14...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8550810</td>\n",
       "      <td>782609</td>\n",
       "      <td>[Glad to launch  #ThikattaThikattaKathalippom ...</td>\n",
       "      <td>glad launch aneethi youtu jwalik film musical ...</td>\n",
       "      <td>{'us': 165, 'new': 157, 'world': 153, 'gat': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>isfj</td>\n",
       "      <td>[aluna sagita gutawa, anderson cooper, carrie ...</td>\n",
       "      <td>[[Now availble on my Youtube channel: Gita Gut...</td>\n",
       "      <td>[[availble youtube channel gita gutawa officia...</td>\n",
       "      <td>[gitagut, andersoncooper, carrieunderwood, dee...</td>\n",
       "      <td>[[2018-08-03T09:49:08.000000000, 2018-08-01T08...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106161637</td>\n",
       "      <td>13256213</td>\n",
       "      <td>[Now availble on my Youtube channel: Gita Guta...</td>\n",
       "      <td>availble youtube channel gita gutawa official ...</td>\n",
       "      <td>{'thank': 941, 'love': 897, 'new': 710, 'happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>isfp</td>\n",
       "      <td>[alejandro sanz, aleks syntek, ashlee simpson ...</td>\n",
       "      <td>[[Long live freedom üá∫üá∏ #Happy4thofJuly, I Told...</td>\n",
       "      <td>[[long live freedom, told, long live freedom, ...</td>\n",
       "      <td>[AlejandroSanz, syntekoficial, ashleesimpson, ...</td>\n",
       "      <td>[[2022-07-04T13:13:49.000000000, 2022-06-25T23...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>318333195</td>\n",
       "      <td>44173539</td>\n",
       "      <td>[Long live freedom üá∫üá∏ #Happy4thofJuly, I Told ...</td>\n",
       "      <td>long live freedom told long live freedom told ...</td>\n",
       "      <td>{'thank': 1305, 'love': 1271, 'new': 1142, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>istj</td>\n",
       "      <td>[akshay kumar, amitabh bachchan, boa, caitlyn ...</td>\n",
       "      <td>[[Thank you for celebrating your Diwali with u...</td>\n",
       "      <td>[[thank diwali us akshay kumar kudo coming hum...</td>\n",
       "      <td>[akshaykumar, SrBachchan, BoAkwon, Caitlyn_Jen...</td>\n",
       "      <td>[[2022-10-26T15:09:23.000000000, 2022-10-24T09...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20861273</td>\n",
       "      <td>2217292</td>\n",
       "      <td>[Thank you for celebrating your Diwali with us...</td>\n",
       "      <td>thank diwali us akshay kumar kudo coming humbl...</td>\n",
       "      <td>{'great': 192, 'love': 185, 'thank': 183, 'new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>istp</td>\n",
       "      <td>[ajay devgn, amr waked, andy murray, ashton ku...</td>\n",
       "      <td>[[Share the warmth of togetherness &amp;amp; famil...</td>\n",
       "      <td>[[share warmth family fun pan indian rockstar ...</td>\n",
       "      <td>[ajaydevgn, amrwaked, andy_murray, aplusk, Big...</td>\n",
       "      <td>[[2022-10-30T06:26:04.000000000, 2022-10-29T05...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115235001</td>\n",
       "      <td>13364096</td>\n",
       "      <td>[Share the warmth of togetherness &amp;amp; family...</td>\n",
       "      <td>share warmth family fun pan indian rockstar ds...</td>\n",
       "      <td>{'new': 584, 'thank': 517, 'love': 471, 'happy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>f</td>\n",
       "      <td>[adal ramones, adam lambert, adam levine, agne...</td>\n",
       "      <td>[[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...</td>\n",
       "      <td>[[drnetas sublime, drnetas sublime, sorry, qtf...</td>\n",
       "      <td>[AdalRamones, adamlambert, adamlevine, agnezmo...</td>\n",
       "      <td>[[2020-06-19T19:27:39.000000000, 2020-06-19T19...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1438603770</td>\n",
       "      <td>196881839</td>\n",
       "      <td>[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...</td>\n",
       "      <td>drnetas sublime drnetas sublime sorry qtf recu...</td>\n",
       "      <td>{'love': 8866, 'thank': 7925, 'new': 7362, 'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>t</td>\n",
       "      <td>[50cent, a.r.rahman, ab de villiers, adele, aj...</td>\n",
       "      <td>[[Athens Greece same stadium where they did th...</td>\n",
       "      <td>[[athens greece stadium olympics many lol gian...</td>\n",
       "      <td>[50cent, arrahman, ABdeVilliers17, Adele, ajay...</td>\n",
       "      <td>[[2022-10-31T00:30:53.000000000, 2022-10-30T10...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>602002023</td>\n",
       "      <td>68429250</td>\n",
       "      <td>[Athens Greece same stadium where they did the...</td>\n",
       "      <td>athens greece stadium olympics many lol giants...</td>\n",
       "      <td>{'new': 4490, 'love': 3775, 'thank': 2793, 'to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subgroup                                              names  \\\n",
       "0    analyst  [a.r.rahman, ab de villiers, al yankovic, andr...   \n",
       "1   diplomat  [agnez mo, ahmet kural, alicia keys, angelica ...   \n",
       "2   explorer  [50cent, adam lambert, adam levine, ajay devgn...   \n",
       "3   sentinel  [adal ramones, adele, akshay kumar, alejandro ...   \n",
       "4          e  [50cent, ab de villiers, adal ramones, adam la...   \n",
       "5          i  [a.r.rahman, ahmet kural, ajay devgn, akshay k...   \n",
       "6          j  [adal ramones, adele, akshay kumar, alejandro ...   \n",
       "7          p  [50cent, a.r.rahman, ab de villiers, adam lamb...   \n",
       "8          n  [a.r.rahman, ab de villiers, agnez mo, ahmet k...   \n",
       "9          s  [50cent, adal ramones, adam lambert, adam levi...   \n",
       "10      enfj  [anggun official, ansel elgort, ashton irwin, ...   \n",
       "11      enfp  [agnez mo, ayushmann khurrana, aziz ansari, br...   \n",
       "12      entj  [busta rhymes, chris paul, dana white, dr. dre...   \n",
       "13      entp  [ab de villiers, al yankovic, andreu buenafuen...   \n",
       "14      esfj  [adal ramones, alejandro fernandez, aline barr...   \n",
       "15      esfp  [adam lambert, adam levine, akon, alyssa milan...   \n",
       "16      estj  [adele, cyprien, dr. mehmet oz, flo rida, giov...   \n",
       "17      estp  [50cent, ashley benson, bipasha basu, birdman,...   \n",
       "18      infj  [angelica vale, arjun rampal, bridgit mendler,...   \n",
       "19      infp  [ahmet kural, alicia keys, calum hood, david l...   \n",
       "20      intj  [elon musk, g-dragon, hardwell, jerry seinfeld...   \n",
       "21      intp  [a.r.rahman, bill gates, felipe andreoli, gary...   \n",
       "22      isfj  [aluna sagita gutawa, anderson cooper, carrie ...   \n",
       "23      isfp  [alejandro sanz, aleks syntek, ashlee simpson ...   \n",
       "24      istj  [akshay kumar, amitabh bachchan, boa, caitlyn ...   \n",
       "25      istp  [ajay devgn, amr waked, andy murray, ashton ku...   \n",
       "26         f  [adal ramones, adam lambert, adam levine, agne...   \n",
       "27         t  [50cent, a.r.rahman, ab de villiers, adele, aj...   \n",
       "\n",
       "                                                 docs  \\\n",
       "0   [[Glad to launch  #ThikattaThikattaKathalippom...   \n",
       "1   [[@netflix This Messiah series thoooo üî•üî•üî•üî• (ca...   \n",
       "2   [[Athens Greece same stadium where they did th...   \n",
       "3   [[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...   \n",
       "4   [[Athens Greece same stadium where they did th...   \n",
       "5   [[Glad to launch  #ThikattaThikattaKathalippom...   \n",
       "6   [[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...   \n",
       "7   [[Athens Greece same stadium where they did th...   \n",
       "8   [[Glad to launch  #ThikattaThikattaKathalippom...   \n",
       "9   [[Athens Greece same stadium where they did th...   \n",
       "10  [[#EliHallo - Behind the scene photos of the m...   \n",
       "11  [[@netflix This Messiah series thoooo üî•üî•üî•üî• (ca...   \n",
       "12  [[#SLAP BY THE üêâ ft. @WHOISCONWAY &amp; @bigda...   \n",
       "13  [[Dewald Brevis. No need to say more, Back on!...   \n",
       "14  [[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...   \n",
       "15  [[Starting in a few minutes!! @veeps\\n\\nGet yo...   \n",
       "16  [[Thank you Joe Talbot for creating a world fo...   \n",
       "17  [[Athens Greece same stadium where they did th...   \n",
       "18  [[¬°Feliz tarde! ¬øComo va su d√≠a? ¬°Espero espec...   \n",
       "19  [[Fitnesiz, No fitne ü•ä @konuralpkocoglu #thesh...   \n",
       "20  [[@JennaEllisEsq @RealMarkFinchem Looking into...   \n",
       "21  [[Glad to launch  #ThikattaThikattaKathalippom...   \n",
       "22  [[Now availble on my Youtube channel: Gita Gut...   \n",
       "23  [[Long live freedom üá∫üá∏ #Happy4thofJuly, I Told...   \n",
       "24  [[Thank you for celebrating your Diwali with u...   \n",
       "25  [[Share the warmth of togetherness &amp; famil...   \n",
       "26  [[@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª,...   \n",
       "27  [[Athens Greece same stadium where they did th...   \n",
       "\n",
       "                                      lemmatized docs  \\\n",
       "0   [[glad launch aneethi youtu jwalik film musica...   \n",
       "1   [[netflix messiah series thoooo cant stop watc...   \n",
       "2   [[athens greece stadium olympics many lol gian...   \n",
       "3   [[drnetas sublime, drnetas sublime, sorry, qtf...   \n",
       "4   [[athens greece stadium olympics many lol gian...   \n",
       "5   [[glad launch aneethi youtu jwalik film musica...   \n",
       "6   [[drnetas sublime, drnetas sublime, sorry, qtf...   \n",
       "7   [[athens greece stadium olympics many lol gian...   \n",
       "8   [[glad launch aneethi youtu jwalik film musica...   \n",
       "9   [[athens greece stadium olympics many lol gian...   \n",
       "10  [[elihallo behind scene photos music video eli...   \n",
       "11  [[netflix messiah series thoooo cant stop watc...   \n",
       "12  [[slap ft listen gt linktr ee, slap taken stre...   \n",
       "13  [[dewald brevis need, back bicycle session don...   \n",
       "14  [[drnetas sublime, drnetas sublime, sorry, qtf...   \n",
       "15  [[starting minutes veeps tickets quick veeps, ...   \n",
       "16  [[thank joe talbot creating world putting bunc...   \n",
       "17  [[athens greece stadium olympics many lol gian...   \n",
       "18  [[feliz tarde va dia espero happy day going ho...   \n",
       "19  [[fitnesiz fitne crossfit suadiye, posted phot...   \n",
       "20  [[looking, evafoxu, sep diageo news bjankow ki...   \n",
       "21  [[glad launch aneethi youtu jwalik film musica...   \n",
       "22  [[availble youtube channel gita gutawa officia...   \n",
       "23  [[long live freedom, told, long live freedom, ...   \n",
       "24  [[thank diwali us akshay kumar kudo coming hum...   \n",
       "25  [[share warmth family fun pan indian rockstar ...   \n",
       "26  [[drnetas sublime, drnetas sublime, sorry, qtf...   \n",
       "27  [[athens greece stadium olympics many lol gian...   \n",
       "\n",
       "                                               handle  \\\n",
       "0   [arrahman, ABdeVilliers17, alyankovic, Buenafu...   \n",
       "1   [agnezmo, AHMTKURAL, aliciakeys, angelicavale,...   \n",
       "2   [50cent, adamlambert, adamlevine, ajaydevgn, A...   \n",
       "3   [AdalRamones, Adele, akshaykumar, alexoficial,...   \n",
       "4   [50cent, ABdeVilliers17, AdalRamones, adamlamb...   \n",
       "5   [arrahman, AHMTKURAL, ajaydevgn, akshaykumar, ...   \n",
       "6   [AdalRamones, Adele, akshaykumar, alexoficial,...   \n",
       "7   [50cent, arrahman, ABdeVilliers17, adamlambert...   \n",
       "8   [arrahman, ABdeVilliers17, agnezmo, AHMTKURAL,...   \n",
       "9   [50cent, AdalRamones, adamlambert, adamlevine,...   \n",
       "10  [Anggun_Cipta, AnselElgort, Ashton5SOS, BSchwe...   \n",
       "11  [agnezmo, ayushmannk, azizansari, brookeburke,...   \n",
       "12  [BustaRhymes, CP3, danawhite, drdre, LizGillie...   \n",
       "13  [ABdeVilliers17, alyankovic, Buenafuente, Anna...   \n",
       "14  [AdalRamones, alexoficial, alinebarros, ahickm...   \n",
       "15  [adamlambert, adamlevine, Akon, Alyssa_Milano,...   \n",
       "16  [Adele, MonsieurDream, DrOz, official_flo, gio...   \n",
       "17  [50cent, AshBenzo, bipsluvurself, BIRDMAN5STAR...   \n",
       "18  [angelicavale, rampalarjun, bridgitmendler, ch...   \n",
       "19  [AHMTKURAL, aliciakeys, Calum5SOS, DAVID_LYNCH...   \n",
       "20  [elonmusk, IBGDRGN, HARDWELL, JerrySeinfeld, l...   \n",
       "21  [arrahman, BillGates, andreolifelipe, GNev2, i...   \n",
       "22  [gitagut, andersoncooper, carrieunderwood, dee...   \n",
       "23  [AlejandroSanz, syntekoficial, ashleesimpson, ...   \n",
       "24  [akshaykumar, SrBachchan, BoAkwon, Caitlyn_Jen...   \n",
       "25  [ajaydevgn, amrwaked, andy_murray, aplusk, Big...   \n",
       "26  [AdalRamones, adamlambert, adamlevine, agnezmo...   \n",
       "27  [50cent, arrahman, ABdeVilliers17, Adele, ajay...   \n",
       "\n",
       "                                                 date  lang   likeCount  \\\n",
       "0   [[2022-10-31T12:33:14.000000000, 2022-10-29T14...   NaN   338501446   \n",
       "1   [[2022-10-04T02:12:34.000000000, 2022-07-07T05...   NaN   546218964   \n",
       "2   [[2022-10-31T00:30:53.000000000, 2022-10-30T10...   NaN   850072105   \n",
       "3   [[2020-06-19T19:27:39.000000000, 2020-06-19T19...   NaN   305813278   \n",
       "4   [[2022-10-31T00:30:53.000000000, 2022-10-30T10...   NaN  1301054647   \n",
       "5   [[2022-10-31T12:33:14.000000000, 2022-10-29T14...   NaN   739551146   \n",
       "6   [[2020-06-19T19:27:39.000000000, 2020-06-19T19...   NaN   632887869   \n",
       "7   [[2022-10-31T00:30:53.000000000, 2022-10-30T10...   NaN  1407717924   \n",
       "8   [[2022-10-31T12:33:14.000000000, 2022-10-29T14...   NaN   884720410   \n",
       "9   [[2022-10-31T00:30:53.000000000, 2022-10-30T10...   NaN  1155885383   \n",
       "10  [[2022-09-27T19:40:04.000000000, 2022-09-23T15...   NaN    99837793   \n",
       "11  [[2022-10-04T02:12:34.000000000, 2022-07-07T05...   NaN   372188286   \n",
       "12  [[2022-10-31T14:56:45.000000000, 2022-10-28T17...   NaN    78219610   \n",
       "13  [[2022-10-31T13:36:16.000000000, 2022-10-30T13...   NaN   155514681   \n",
       "14  [[2020-06-19T19:27:39.000000000, 2020-06-19T19...   NaN   159203758   \n",
       "15  [[2022-10-31T02:49:14.000000000, 2022-10-30T19...   NaN   308686216   \n",
       "16  [[2022-10-26T20:59:05.000000000, 2022-10-26T20...   NaN    19586610   \n",
       "17  [[2022-10-31T00:30:53.000000000, 2022-10-30T10...   NaN   107817693   \n",
       "18  [[2022-10-27T23:35:42.000000000, 2022-10-16T11...   NaN    52800843   \n",
       "19  [[2018-08-01T11:29:09.000000000, 2017-01-01T10...   NaN    21392042   \n",
       "20  [[2022-10-31T22:55:11.000000000, 2022-10-31T22...   NaN    96216345   \n",
       "21  [[2022-10-31T12:33:14.000000000, 2022-10-29T14...   NaN     8550810   \n",
       "22  [[2018-08-03T09:49:08.000000000, 2018-08-01T08...   NaN   106161637   \n",
       "23  [[2022-07-04T13:13:49.000000000, 2022-06-25T23...   NaN   318333195   \n",
       "24  [[2022-10-26T15:09:23.000000000, 2022-10-24T09...   NaN    20861273   \n",
       "25  [[2022-10-30T06:26:04.000000000, 2022-10-29T05...   NaN   115235001   \n",
       "26  [[2020-06-19T19:27:39.000000000, 2020-06-19T19...   NaN  1438603770   \n",
       "27  [[2022-10-31T00:30:53.000000000, 2022-10-30T10...   NaN   602002023   \n",
       "\n",
       "    retweetCount                                docs w.r.t subgroup  \\\n",
       "0       38455192  [Glad to launch  #ThikattaThikattaKathalippom ...   \n",
       "1       92457260  [@netflix This Messiah series thoooo üî•üî•üî•üî• (can...   \n",
       "2       98842116  [Athens Greece same stadium where they did the...   \n",
       "3       35556521  [@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...   \n",
       "4      166768529  [Athens Greece same stadium where they did the...   \n",
       "5       98542560  [Glad to launch  #ThikattaThikattaKathalippom ...   \n",
       "6       84979590  [@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...   \n",
       "7      180331499  [Athens Greece same stadium where they did the...   \n",
       "8      130912452  [Glad to launch  #ThikattaThikattaKathalippom ...   \n",
       "9      134398637  [Athens Greece same stadium where they did the...   \n",
       "10      17779984  [#EliHallo - Behind the scene photos of the mu...   \n",
       "11      59229712  [@netflix This Messiah series thoooo üî•üî•üî•üî• (can...   \n",
       "12      10582136  [#SLAP BY THE üêâ ft. @WHOISCONWAY &amp; @bigdad...   \n",
       "13      17789200  [Dewald Brevis. No need to say more, Back on! ...   \n",
       "14      17428252  [@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...   \n",
       "15      29566575  [Starting in a few minutes!! @veeps\\n\\nGet you...   \n",
       "16       2654764  [Thank you Joe Talbot for creating a world for...   \n",
       "17      11737906  [Athens Greece same stadium where they did the...   \n",
       "18      11759702  [¬°Feliz tarde! ¬øComo va su d√≠a? ¬°Espero espect...   \n",
       "19       3687862  [Fitnesiz, No fitne ü•ä @konuralpkocoglu #thesho...   \n",
       "20       9301247  [@JennaEllisEsq @RealMarkFinchem Looking into ...   \n",
       "21        782609  [Glad to launch  #ThikattaThikattaKathalippom ...   \n",
       "22      13256213  [Now availble on my Youtube channel: Gita Guta...   \n",
       "23      44173539  [Long live freedom üá∫üá∏ #Happy4thofJuly, I Told ...   \n",
       "24       2217292  [Thank you for celebrating your Diwali with us...   \n",
       "25      13364096  [Share the warmth of togetherness &amp; family...   \n",
       "26     196881839  [@reformaopinion @DrNetas SUBLIME üëèüèªüëèüèªüëèüèªüëèüèªüëèüèª, ...   \n",
       "27      68429250  [Athens Greece same stadium where they did the...   \n",
       "\n",
       "                       lemmatized docs w.r.t subgroup  \\\n",
       "0   glad launch aneethi youtu jwalik film musical ...   \n",
       "1   netflix messiah series thoooo cant stop watchi...   \n",
       "2   athens greece stadium olympics many lol giants...   \n",
       "3   drnetas sublime drnetas sublime sorry qtf recu...   \n",
       "4   athens greece stadium olympics many lol giants...   \n",
       "5   glad launch aneethi youtu jwalik film musical ...   \n",
       "6   drnetas sublime drnetas sublime sorry qtf recu...   \n",
       "7   athens greece stadium olympics many lol giants...   \n",
       "8   glad launch aneethi youtu jwalik film musical ...   \n",
       "9   athens greece stadium olympics many lol giants...   \n",
       "10  elihallo behind scene photos music video eliha...   \n",
       "11  netflix messiah series thoooo cant stop watchi...   \n",
       "12  slap ft listen gt linktr ee slap taken streets...   \n",
       "13  dewald brevis need back bicycle session done j...   \n",
       "14  drnetas sublime drnetas sublime sorry qtf recu...   \n",
       "15  starting minutes veeps tickets quick veeps ton...   \n",
       "16  thank joe talbot creating world putting bunch ...   \n",
       "17  athens greece stadium olympics many lol giants...   \n",
       "18  feliz tarde va dia espero happy day going hope...   \n",
       "19  fitnesiz fitne crossfit suadiye posted photo b...   \n",
       "20  looking evafoxu sep diageo news bjankow kieley...   \n",
       "21  glad launch aneethi youtu jwalik film musical ...   \n",
       "22  availble youtube channel gita gutawa official ...   \n",
       "23  long live freedom told long live freedom told ...   \n",
       "24  thank diwali us akshay kumar kudo coming humbl...   \n",
       "25  share warmth family fun pan indian rockstar ds...   \n",
       "26  drnetas sublime drnetas sublime sorry qtf recu...   \n",
       "27  athens greece stadium olympics many lol giants...   \n",
       "\n",
       "                            freq table w.r.t subgroup  \n",
       "0   {'new': 2083, 'love': 1559, 'us': 1305, 'today...  \n",
       "1   {'love': 3190, 'thank': 2803, 'new': 2416, 'da...  \n",
       "2   {'love': 4797, 'new': 4675, 'thank': 4094, 'ha...  \n",
       "3   {'love': 3095, 'thank': 2709, 'new': 2678, 'ha...  \n",
       "4   {'love': 8532, 'new': 8054, 'thank': 6756, 'ha...  \n",
       "5   {'love': 4109, 'thank': 3962, 'new': 3798, 'ha...  \n",
       "6   {'love': 5581, 'new': 4832, 'thank': 4572, 'ha...  \n",
       "7   {'love': 7060, 'new': 7020, 'thank': 6146, 'ha...  \n",
       "8   {'love': 4749, 'new': 4499, 'thank': 3915, 'da...  \n",
       "9   {'love': 7892, 'new': 7353, 'thank': 6803, 'ha...  \n",
       "10  {'love': 974, 'thank': 861, 'new': 698, 'god':...  \n",
       "11  {'love': 1306, 'thank': 1128, 'new': 1080, 'da...  \n",
       "12  {'live': 676, 'new': 564, 'love': 555, 'vs': 4...  \n",
       "13  {'new': 954, 'love': 629, 'us': 586, 'thank': ...  \n",
       "14  {'love': 1524, 'thank': 1204, 'happy': 1175, '...  \n",
       "15  {'love': 1984, 'new': 1923, 'thank': 1672, 'ha...  \n",
       "16  {'new': 638, 'today': 579, 'check': 501, 'grea...  \n",
       "17  {'love': 1071, 'new': 1026, 'happy': 734, 'day...  \n",
       "18  {'love': 679, 'bit': 622, 'ly': 605, 'thank': ...  \n",
       "19  {'ly': 505, 'bit': 476, 'friends': 356, 'dear'...  \n",
       "20  {'new': 408, 'love': 278, 'bit': 255, 'ly': 23...  \n",
       "21  {'us': 165, 'new': 157, 'world': 153, 'gat': 1...  \n",
       "22  {'thank': 941, 'love': 897, 'new': 710, 'happy...  \n",
       "23  {'thank': 1305, 'love': 1271, 'new': 1142, 'ha...  \n",
       "24  {'great': 192, 'love': 185, 'thank': 183, 'new...  \n",
       "25  {'new': 584, 'thank': 517, 'love': 471, 'happy...  \n",
       "26  {'love': 8866, 'thank': 7925, 'new': 7362, 'ha...  \n",
       "27  {'new': 4490, 'love': 3775, 'thank': 2793, 'to...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subgrouplist=subgroupDf.subgroup.value_counts().index.tolist()\n",
    "[print(i) for i in subgrouplist if len(i)==4]\n",
    "subgroupDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for the big prep loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/rqcbqynd6g96v17vbqppznm40000gn/T/ipykernel_36585/4115732160.py:16: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  cool=dict(pd.Series(stoped.split()).value_counts())\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "bigdict={'type':{},'name':{},'stoped_lemma':{},'freq':{}}\n",
    "for i in list(indexbyperson.keys()):\n",
    "    a=indexbyperson.get(i)\n",
    "    a=a['name']\n",
    "    for i1 in list(a.keys()):\n",
    "        listtonormaliz=str(a[i1]['content'])\n",
    "        newtext=lemmatizor(listtonormaliz,regexfilter=r'[^a-z0-9\\'\\s]')\n",
    "        lemma=newtext\n",
    "       \n",
    "        stoped=stopfilter(lemma)\n",
    "        stoped=stoped.replace('https','').replace('com','').replace('co','').replace(',','').strip()\n",
    "       \n",
    "        a[i1].update({'stopped_lemma':stoped})         \n",
    "     \n",
    "        cool=dict(pd.Series(stoped.split()).value_counts())\n",
    "        a[i1].update({'word freq':cool})\n",
    "        bigdict['type'].update({num:i})\n",
    "        bigdict['stoped_lemma'].update({num:stoped})\n",
    "        bigdict['freq'].update({num:cool})\n",
    "        bigdict['name'].update({num:i1})\n",
    "        num+=1\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>amitabh bachchan</td>\n",
       "      <td>unique film dostojee young bengali director pr...</td>\n",
       "      <td>{'film': 24, 'n': 24, 'love': 23, 'legend': 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>thank diwali us akshay kumar kudo ing humbling...</td>\n",
       "      <td>{'n': 84, 'ramsetu': 55, 'thank': 42, 'love': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>elissa</td>\n",
       "      <td>riyadh never ceize amaze energy love ing back ...</td>\n",
       "      <td>{'love': 53, 'thank': 21, 'morning': 20, 'nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>lord sugar</td>\n",
       "      <td>needs firing let sugar nator point wave firing...</td>\n",
       "      <td>{'firing': 46, 'hospital': 46, 'ormond': 45, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>gary barlow</td>\n",
       "      <td>happy folks thanks gang ready todays audience ...</td>\n",
       "      <td>{'show': 56, 'thank': 44, 'week': 42, 'night':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>INTP</td>\n",
       "      <td>felipe andreoli</td>\n",
       "      <td>arnold ndouble craques arnold ndouble craques ...</td>\n",
       "      <td>{'arnold': 2, 'great': 2, 'ndouble': 2, 'usope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>INTP</td>\n",
       "      <td>gary neville</td>\n",
       "      <td>minister sky news talking breaking model theyv...</td>\n",
       "      <td>{'us': 58, 'watch': 44, 'essexpr': 40, 'overla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>INTP</td>\n",
       "      <td>markus persson</td>\n",
       "      <td>entropy enough state cnot eigenbom irc happen ...</td>\n",
       "      <td>{'bit': 46, 'got': 38, 'game': 37, 'sure': 35,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>INTP</td>\n",
       "      <td>james may</td>\n",
       "      <td>reverse plumbing use bag able suck debris add ...</td>\n",
       "      <td>{'yes': 27, 'book': 19, 'new': 13, 'thanks': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>INTP</td>\n",
       "      <td>troian</td>\n",
       "      <td>posted photo angeles posted photo us sure sena...</td>\n",
       "      <td>{'photo': 63, 'posted': 57, 'thank': 38, 'supp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type              name  \\\n",
       "0    ISTJ  amitabh bachchan   \n",
       "1    ISTJ      akshay kumar   \n",
       "2    ISTJ            elissa   \n",
       "3    ISTJ        lord sugar   \n",
       "4    ISTJ       gary barlow   \n",
       "..    ...               ...   \n",
       "585  INTP   felipe andreoli   \n",
       "586  INTP      gary neville   \n",
       "587  INTP    markus persson   \n",
       "588  INTP         james may   \n",
       "589  INTP            troian   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "0    unique film dostojee young bengali director pr...   \n",
       "1    thank diwali us akshay kumar kudo ing humbling...   \n",
       "2    riyadh never ceize amaze energy love ing back ...   \n",
       "3    needs firing let sugar nator point wave firing...   \n",
       "4    happy folks thanks gang ready todays audience ...   \n",
       "..                                                 ...   \n",
       "585  arnold ndouble craques arnold ndouble craques ...   \n",
       "586  minister sky news talking breaking model theyv...   \n",
       "587  entropy enough state cnot eigenbom irc happen ...   \n",
       "588  reverse plumbing use bag able suck debris add ...   \n",
       "589  posted photo angeles posted photo us sure sena...   \n",
       "\n",
       "                                                  freq  \n",
       "0    {'film': 24, 'n': 24, 'love': 23, 'legend': 17...  \n",
       "1    {'n': 84, 'ramsetu': 55, 'thank': 42, 'love': ...  \n",
       "2    {'love': 53, 'thank': 21, 'morning': 20, 'nigh...  \n",
       "3    {'firing': 46, 'hospital': 46, 'ormond': 45, '...  \n",
       "4    {'show': 56, 'thank': 44, 'week': 42, 'night':...  \n",
       "..                                                 ...  \n",
       "585  {'arnold': 2, 'great': 2, 'ndouble': 2, 'usope...  \n",
       "586  {'us': 58, 'watch': 44, 'essexpr': 40, 'overla...  \n",
       "587  {'bit': 46, 'got': 38, 'game': 37, 'sure': 35,...  \n",
       "588  {'yes': 27, 'book': 19, 'new': 13, 'thanks': 1...  \n",
       "589  {'photo': 63, 'posted': 57, 'thank': 38, 'supp...  \n",
       "\n",
       "[590 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterwordslemma=pd.DataFrame(bigdict)\n",
    "twitterwordslemma.columns=['type','name','lemmatized','freq']\n",
    "twitterwordslemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>istj</td>\n",
       "      <td>amitabh bachchan</td>\n",
       "      <td>unique film dostojee young bengali director pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>istj</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>thank diwali us akshay kumar kudo ing humbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>istj</td>\n",
       "      <td>elissa</td>\n",
       "      <td>riyadh never ceize amaze energy love ing back ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>istj</td>\n",
       "      <td>lord sugar</td>\n",
       "      <td>needs firing let sugar nator point wave firing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>istj</td>\n",
       "      <td>gary barlow</td>\n",
       "      <td>happy folks thanks gang ready todays audience ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>intp</td>\n",
       "      <td>felipe andreoli</td>\n",
       "      <td>arnold ndouble craques arnold ndouble craques ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>intp</td>\n",
       "      <td>gary neville</td>\n",
       "      <td>minister sky news talking breaking model theyv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>intp</td>\n",
       "      <td>markus persson</td>\n",
       "      <td>entropy enough state cnot eigenbom irc happen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>intp</td>\n",
       "      <td>james may</td>\n",
       "      <td>reverse plumbing use bag able suck debris add ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>intp</td>\n",
       "      <td>troian</td>\n",
       "      <td>posted photo angeles posted photo us sure sena...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type              name                                         lemmatized\n",
       "0    istj  amitabh bachchan  unique film dostojee young bengali director pr...\n",
       "1    istj      akshay kumar  thank diwali us akshay kumar kudo ing humbling...\n",
       "2    istj            elissa  riyadh never ceize amaze energy love ing back ...\n",
       "3    istj        lord sugar  needs firing let sugar nator point wave firing...\n",
       "4    istj       gary barlow  happy folks thanks gang ready todays audience ...\n",
       "..    ...               ...                                                ...\n",
       "585  intp   felipe andreoli  arnold ndouble craques arnold ndouble craques ...\n",
       "586  intp      gary neville  minister sky news talking breaking model theyv...\n",
       "587  intp    markus persson  entropy enough state cnot eigenbom irc happen ...\n",
       "588  intp         james may  reverse plumbing use bag able suck debris add ...\n",
       "589  intp            troian  posted photo angeles posted photo us sure sena...\n",
       "\n",
       "[590 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterwordslemma['type']=twitterwordslemma.type.str.lower()\n",
    "\n",
    "pd.to_pickle(twitterwordslemma,'maindalemma.pkl')\n",
    "\n",
    "df=pd.read_pickle('maindalemma.pkl')\n",
    "df=df[[\t'type',\t'name',\t'lemmatized'\t]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaing is explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>istj</td>\n",
       "      <td>amitabh bachchan</td>\n",
       "      <td>unique film dostojee young bengali director pr...</td>\n",
       "      <td>{'film': 24, 'n': 24, 'love': 23, 'legend': 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>istj</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>thank diwali us akshay kumar kudo ing humbling...</td>\n",
       "      <td>{'n': 84, 'ramsetu': 55, 'thank': 42, 'love': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>istj</td>\n",
       "      <td>elissa</td>\n",
       "      <td>riyadh never ceize amaze energy love ing back ...</td>\n",
       "      <td>{'love': 53, 'thank': 21, 'morning': 20, 'nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>istj</td>\n",
       "      <td>lord sugar</td>\n",
       "      <td>needs firing let sugar nator point wave firing...</td>\n",
       "      <td>{'firing': 46, 'hospital': 46, 'ormond': 45, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>istj</td>\n",
       "      <td>gary barlow</td>\n",
       "      <td>happy folks thanks gang ready todays audience ...</td>\n",
       "      <td>{'show': 56, 'thank': 44, 'week': 42, 'night':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>intp</td>\n",
       "      <td>felipe andreoli</td>\n",
       "      <td>arnold ndouble craques arnold ndouble craques ...</td>\n",
       "      <td>{'arnold': 2, 'great': 2, 'ndouble': 2, 'usope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>intp</td>\n",
       "      <td>gary neville</td>\n",
       "      <td>minister sky news talking breaking model theyv...</td>\n",
       "      <td>{'us': 58, 'watch': 44, 'essexpr': 40, 'overla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>intp</td>\n",
       "      <td>markus persson</td>\n",
       "      <td>entropy enough state cnot eigenbom irc happen ...</td>\n",
       "      <td>{'bit': 46, 'got': 38, 'game': 37, 'sure': 35,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>intp</td>\n",
       "      <td>james may</td>\n",
       "      <td>reverse plumbing use bag able suck debris add ...</td>\n",
       "      <td>{'yes': 27, 'book': 19, 'new': 13, 'thanks': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>intp</td>\n",
       "      <td>troian</td>\n",
       "      <td>posted photo angeles posted photo us sure sena...</td>\n",
       "      <td>{'photo': 63, 'posted': 57, 'thank': 38, 'supp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type              name  \\\n",
       "0    istj  amitabh bachchan   \n",
       "1    istj      akshay kumar   \n",
       "2    istj            elissa   \n",
       "3    istj        lord sugar   \n",
       "4    istj       gary barlow   \n",
       "..    ...               ...   \n",
       "585  intp   felipe andreoli   \n",
       "586  intp      gary neville   \n",
       "587  intp    markus persson   \n",
       "588  intp         james may   \n",
       "589  intp            troian   \n",
       "\n",
       "                                            lemmatized  \\\n",
       "0    unique film dostojee young bengali director pr...   \n",
       "1    thank diwali us akshay kumar kudo ing humbling...   \n",
       "2    riyadh never ceize amaze energy love ing back ...   \n",
       "3    needs firing let sugar nator point wave firing...   \n",
       "4    happy folks thanks gang ready todays audience ...   \n",
       "..                                                 ...   \n",
       "585  arnold ndouble craques arnold ndouble craques ...   \n",
       "586  minister sky news talking breaking model theyv...   \n",
       "587  entropy enough state cnot eigenbom irc happen ...   \n",
       "588  reverse plumbing use bag able suck debris add ...   \n",
       "589  posted photo angeles posted photo us sure sena...   \n",
       "\n",
       "                                                  freq  \n",
       "0    {'film': 24, 'n': 24, 'love': 23, 'legend': 17...  \n",
       "1    {'n': 84, 'ramsetu': 55, 'thank': 42, 'love': ...  \n",
       "2    {'love': 53, 'thank': 21, 'morning': 20, 'nigh...  \n",
       "3    {'firing': 46, 'hospital': 46, 'ormond': 45, '...  \n",
       "4    {'show': 56, 'thank': 44, 'week': 42, 'night':...  \n",
       "..                                                 ...  \n",
       "585  {'arnold': 2, 'great': 2, 'ndouble': 2, 'usope...  \n",
       "586  {'us': 58, 'watch': 44, 'essexpr': 40, 'overla...  \n",
       "587  {'bit': 46, 'got': 38, 'game': 37, 'sure': 35,...  \n",
       "588  {'yes': 27, 'book': 19, 'new': 13, 'thanks': 1...  \n",
       "589  {'photo': 63, 'posted': 57, 'thank': 38, 'supp...  \n",
       "\n",
       "[590 rows x 4 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitterwordslemma=pd.read_pickle('maindalemma.pkl')\n",
    "twitterwordslemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['stoped_lemma'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb Cell 53\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#Y103sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#Y103sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bigdict_type\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m:{},\u001b[39m'\u001b[39m\u001b[39mstoped_lemma\u001b[39m\u001b[39m'\u001b[39m:{},\u001b[39m'\u001b[39m\u001b[39mfreq\u001b[39m\u001b[39m'\u001b[39m:{}}\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#Y103sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m group1\u001b[39m=\u001b[39mtwitterwordslemma[[\u001b[39m'\u001b[39;49m\u001b[39mstoped_lemma\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m]]\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#Y103sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m group1\u001b[39m.\u001b[39mgroups\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/wrangle_twitter.ipynb#Y103sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m group1\u001b[39m.\u001b[39mgroups\u001b[39m.\u001b[39mkeys():\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3509\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3510\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3511\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3513\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3514\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['stoped_lemma'] not in index\""
     ]
    }
   ],
   "source": [
    "num=0\n",
    "bigdict_type={'type':{},'stoped_lemma':{},'freq':{}}\n",
    "\n",
    "group1=twitterwordslemma[['stoped_lemma','type']].groupby('type')\n",
    "group1.groups.keys()\n",
    "for i in group1.groups.keys():\n",
    "    \n",
    "    x=(','.join(list(group1.get_group(i).stoped_lemma.values)).strip())\n",
    "  \n",
    "    x=stopfilter(x)\n",
    "    \n",
    "   \n",
    "    y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "    cool=dict(y)\n",
    "    bigdict_type['type'].update({num:i})\n",
    "    bigdict_type['stoped_lemma'].update({num:x})\n",
    "    bigdict_type['freq'].update({num:cool})\n",
    "       \n",
    "\n",
    "    num+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.DataFrame(bigdict_type)\n",
    "x=str(typeslemma.stoped_lemma.values).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'').replace(',',' ').split()\n",
    "y=dict(pd.Series(x).value_counts())\n",
    "aggregatewordfrreq=y\n",
    "# pd.to_pickle(aggregatewordfrreq,'agglemma.pkl')\n",
    "num=len(typeslemma)\n",
    "\n",
    "\n",
    "z=[i.replace(',',' ') for i in typeslemma.stoped_lemma.values]\n",
    "typeslemma=pd.concat([typeslemma,pd.DataFrame({'type':{num:'COMBINED'},'stoped_lemma':{num:str(z).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'')},'freq':{num:aggregatewordfrreq}})])\n",
    "pd.to_pickle(typeslemma,'typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.read_pickle('typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extroverteddf=typeslemma[['type','stoped_lemma']].iloc[0:7]\n",
    "extroverteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "introverteddf=typeslemma[['type','stoped_lemma']].iloc[8:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdict=dict(typeslemma.freq.values[-1])\n",
    "#set stuff\n",
    "setlist=[]\n",
    "for i in typeslemma.freq.values:\n",
    "    setlist.append(set(i.keys()))\n",
    "\n",
    "typelist=[]\n",
    "for i in typeslemma.type.values:\n",
    "    typelist.append(i)\n",
    "\n",
    "typessetdict=dict(zip(typelist,setlist))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys=list(typessetdict.keys())\n",
    "combined=keys.pop(-1);combined\n",
    "intersectiondict={}\n",
    "c=set(typessetdict.get(combined))\n",
    "keys.reverse()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['red','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigint=deepcopy(c)\n",
    "[bigint.intersection_update(typessetdict.get(key))for key in keys]\n",
    "print(f'We have a total of {len(bigint)} in the aggregate intersection\\nWe remove that intersection to compare')\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    kthset=kthset-bigint\n",
    "    int=(c&kthset)-bigint\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    unionwithoutk=unionwithoutk-bigint\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "    unique=(kthset-unionwithoutk)-bigint\n",
    "\n",
    "    print(f'number unique to {k}\\n{len(unique)}\\n',f'percent unique of aggregate union combined:\\n{(len(unique)/len(c))*100:.2f}%')\n",
    "    restint=(kthset&unionwithoutk)-bigint\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','.35','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from itertools import product \n",
    "  \n",
    "# # Get all permutations of length 2 \n",
    "# # and length 2 \n",
    "# x=[\"\".join(seq) for seq in product(\"01\", repeat=4)]\n",
    "# for i in x:\n",
    "#     print(i[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumforavg=dataframe[['type','name']].groupby(['type']).nunique().sum()\n",
    "tochart=(dataframe[['type','name']].groupby(['type']).nunique()/sumforavg)*100\n",
    "\n",
    "tochart=tochart.reset_index()\n",
    "tochart['percent']=tochart['name']\n",
    "\n",
    "tochart.drop(columns='name',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tochart.index=tochart.type\n",
    "tochart.drop(columns='type',inplace=True)\n",
    "tochart=tochart.sort_values(by='percent',ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "genpoppercent=['13.8% 12.3% 11.6% 8.8% 8.7% 8.5% 8.1% 5.4% 4.4% 4.3% 3.3% 3.2% 2.5% 2.1% 1.8% 1.5%']\n",
    "genpoppercent=str(genpoppercent).replace('%','').split()\n",
    "genpoppercent=[float(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in genpoppercent]\n",
    "\n",
    "\n",
    "types=['ISFJ ESFI ISTJ ISFP ESTI ESFP ENFP ISTP INFP ESTP INTP ENTP ENFJ INTJ ENTI INFT']\n",
    "types=str(types).split()\n",
    "\n",
    "\n",
    "types=[(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in types]\n",
    "\n",
    "pop=pd.DataFrame(index=types,data={'pop percentage':genpoppercent})\n",
    "\n",
    "tochart=pd.concat([tochart,pop],axis=1,join='inner')\n",
    "tochart.rename(columns={'percent':'found percent'},inplace=True)\n",
    "cols=['pop percentage','found percent']\n",
    "tochart=tochart[cols]\n",
    "tochart.sort_values(by='pop percentage',ascending=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "styleddf=tochart.T.style.background_gradient(cmap='Blues',axis=1).format(lambda x : f'{x:.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad1=[0,0,0,0]\n",
    "quad2=[0,0,0,0]\n",
    "quad3=[0,.25,.35,.25]\n",
    "quad4=[.35,.25,.35,.25]\n",
    "\n",
    "explode = []\n",
    "explode.extend(quad1)\n",
    "explode.extend(quad2)\n",
    "explode.extend(quad3)\n",
    "explode.extend(quad4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def format_axes(fig):\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n",
    "        ax.tick_params(labelbottom=False, labelleft=False)\n",
    "m=1.23\n",
    "fig = plt.figure(constrained_layout=False,figsize=(m*20,m*12.361))\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('MBTI: General Population Vs Twitter',fontsize=16,weight='demibold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "kwargs1={'title':'General Population (Pie)   ','ax':ax1,'legend':False,'ylabel':'',   'cmap':'Blues'}\n",
    "\n",
    "tochart.plot.pie(y='found percent',**kwargs1)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "kwargs2={'title':'  Twitter (Pie)   ','ax':ax2,'legend':False,'ylabel':'',   'cmap':'viridis'}\n",
    "tochart.plot.pie(y='pop percentage',**kwargs2)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,1])\n",
    "kwargs3={'ax':ax3,'legend':False,'title':'Twitter (Bar)',   'cmap':'viridis'}\n",
    "\n",
    "tochart.plot.barh(y='found percent',**kwargs3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "kwargs4={'ax':ax4,'legend':False,'title':'General Population (Bar)',   'cmap':'Blues'}\n",
    "tochart.plot.barh(y='pop percentage',**kwargs4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"GridSpec\")\n",
    "format_axes(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display('Summary',styleddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Below was an attempt at Multithreading and Paralellism\n",
    "* ## This can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Below are two ways of scraping using CLI commands.\n",
    "# Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "# then output two different csv files.\n",
    "\n",
    "# Query by username\n",
    "# Setting variables to be used in format string command below\n",
    "# def parralledindexer():\n",
    "#  #tweet count number is the last n tweets\n",
    "#  # #read in top 1,000 celebs\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(list(celebs.get('twitter').keys()))\n",
    "#     numindex=range(0,tweet_count*celeblen)\n",
    "\n",
    "#     c_with_slice={}\n",
    "#     for c in range(0,celeblen,1):  \n",
    "#             count=count+tweet_count\n",
    "#             maxnum=count+tweet_count     \n",
    "#             cur=numindex[count:maxnum]\n",
    "#             c_with_slice.update({c:cur})\n",
    "           \n",
    "\n",
    "    # mod10={}\n",
    "    # mod9={}\n",
    "    # mod8={}\n",
    "    # mod7={}\n",
    "    # mod6={}\n",
    "    # mod5={}\n",
    "    # mod4={}\n",
    "    # mod3={}\n",
    "    # mod2={}\n",
    "    # keylist=list(c_with_slice.keys())\n",
    "    # for i in keylist:\n",
    "    #     if i%10==0:  \n",
    "    #         mod10.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%9==0:      \n",
    "    #         mod9.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%8==0:       \n",
    "    #         mod8.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%7==0:      \n",
    "    #         mod7.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%6==0:      \n",
    "    #         mod6.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%5==0:      \n",
    "    #         mod5.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%4==0:       \n",
    "    #         mod4.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%3==0:      \n",
    "    #         mod3.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%2==0:     \n",
    "    #         mod2.update({i:c_with_slice.get(i)})\n",
    "#     moddicts=c_with_slice\n",
    "        \n",
    "\n",
    "#     # moddicts={**mod10,**mod9,\n",
    "#     # **mod8,\n",
    "#     # **mod7,\n",
    "#     # **mod6,\n",
    "#     # **mod5,\n",
    "#     # **mod4,\n",
    "#     # **mod3,\n",
    "#     # **mod2}\n",
    "#     return moddicts,celebs\n",
    "\n",
    "# moddicts,celebs=parralledindexer()\n",
    "\n",
    "# ###Think of schem to split then pu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def actualparrelel():\n",
    "#     '''\n",
    "    \n",
    "#     slow as shit for input output this is for data processing on the comp\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "#         with Pool(10) as pool:\n",
    "#             arglist=[]   \n",
    "#             for m in moddicts:\n",
    "#                 arglist.append((m,celebs)) \n",
    "\n",
    "\n",
    "           \n",
    "#             results_async=pool.starmap_async(partitionableTwitterscraper,arglist)\n",
    "#             # get the return values\n",
    "#             try:\n",
    "#                 for value in results_async.get():\n",
    "#                     values.append(value)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f'Failed with: {e}')\n",
    "#     return values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def actualthreading():\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "  \n",
    "#             arglist=[]   \n",
    "#             for m,v in moddicts.items():\n",
    "#                 arglist.append({m:v})\n",
    "#             # print(arglist) #this is fine    \n",
    "\n",
    "#     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "\n",
    "#     threads = min(50, len(moddicts))   \n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         data=[]\n",
    "           \n",
    "\n",
    "#         for arg in arglist:\n",
    "#             for res in executor.submit(partitionableTwitterscraper(arg)):\n",
    "#                 executor.shutdown(wait=True)\n",
    "    \n",
    "#                 try:\n",
    "#                     data.append((res))\n",
    "#                     # print(res[0])\n",
    "#                     # print(res[1])\n",
    "\n",
    "\n",
    "\n",
    "#                 except Exception as exc:\n",
    "#                     pass\n",
    "#                 #  print('%r generated an exception: %s' % (result, exc))\n",
    "#         # else:\n",
    "#         #     # print('%r page is %d bytes' % (result, len(data)))\n",
    "    \n",
    "#     #sets the number of threads to the lesser of 30 or length of urls\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "  \n",
    "# # empty list with global scope\n",
    "# result = []\n",
    "# # empty list with global scope\n",
    "\n",
    "# def square_list(mylist):\n",
    "#     \"\"\"\n",
    "#     function to square a given list\n",
    "#     \"\"\"\n",
    "#     global result\n",
    "#     # append squares of mylist to global list result\n",
    "#     for num in mylist:\n",
    "#         result.append(num * num)\n",
    "\n",
    "\n",
    "# def partitionableTwitterscraper(c_with_slice):\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     c_with_slice=c_with_slice\n",
    "#     # print(c_with_slice)\n",
    "#     tweet_count =100 \n",
    "#     dictcount=0\n",
    "#     dfdict={}\n",
    "#     errornames=[]\n",
    "\n",
    "\n",
    "    \n",
    "#     print(c_with_slice.keys()) \n",
    "#     print('\\n') \n",
    "#     c=list(c_with_slice.keys())[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     twitter_handle=celebs.get('twitter')\n",
    "#     name=celebs.get('name')\n",
    "#     cur=c_with_slice.get(c)  \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "    \n",
    "\n",
    "    \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next artist i.e. failsafe \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "\n",
    "#         if dictcount<=1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                     tweets_df1.update({'name':nameseries})\n",
    "#                     tweets_df1.update({'handle':handleseries})\n",
    "#                     a=tweets_df1.get(key)\n",
    "#                     b=dfdict.get(key)\n",
    "#                     c={**a,**b}\n",
    "#                     dfdict=dfdict.update({key:c})\n",
    "#     except:\n",
    "#         # errornames.append(name)\n",
    "#         # print('errornames:\\n',len(errornames))\n",
    "#         pass \n",
    "\n",
    "\n",
    "#     display(dfdict)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "   \n",
    "\n",
    "#     return dfdict\n",
    "\n",
    "\n",
    "  \n",
    "# moddicts,celebs=parralledindexer()\n",
    "# values=[]\n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=[]   \n",
    "#     for m,v in moddicts.items():\n",
    "#         arglist.append({m:v})\n",
    "  \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=partitionableTwitterscraper, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "  \n",
    "#     # print global result list\n",
    "#     print(\"Result(in main program): {}\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maindict=list()\n",
    "\n",
    "# def argcreateator(n=3):\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "\n",
    "#     tweet_count =n #this number is the last n tweets\n",
    "#     dflist=[]\n",
    "#     dfdict={}\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(celebs.get('twitter').keys())\n",
    "#     numindex=list(range(0,tweet_count*celeblen))\n",
    "#     len(numindex)\n",
    "#     arglist=[]\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#     for c in range(0,celeblen):\n",
    "#         count=count+tweet_count\n",
    "#         maxnum=count+tweet_count\n",
    "\n",
    "#         twitter_handle=(celebs.get('twitter')[c])\n",
    "#         name=(celebs.get('name')[c])\n",
    "\n",
    "#         cur=pd.Index(numindex[count:maxnum])\n",
    "\n",
    "#         #create Series to append the current handle to the dataframe\n",
    "#         handleseries={i:twitter_handle for i in cur}       \n",
    "#         #create Series to append the current name to dataframe   \n",
    "#         nameseries={i:name for i in cur}\n",
    "#         arglist.append([cur,nameseries,handleseries])\n",
    "#     return arglist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur):\n",
    "#     global maindict\n",
    "#     os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "#      # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#      #if there is an error then it will just move to the next  \n",
    "#     tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=cur).to_dict()\n",
    "#     tweets_df1.update({'name':nameseries})\n",
    "#     tweets_df1.update({'handle':handleseries})\n",
    "#     maindict.append(pd.DataFrame(tweets_df1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def threader(arglist):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     dictcount=0\n",
    "#     errornames=[]\n",
    "\n",
    "   \n",
    "\n",
    "#     # for i in range(0,len(arglist)):\n",
    "#     for i in range(0,len(arglist)):\n",
    "#         tweet_count=len(arglist[i][0]);#display(tweet_count)\n",
    "#         nameseries=arglist[i][1];#display(nameseries)\n",
    "#         handleseries=arglist[i][2];#display(handleseries)\n",
    "#         twitter_handle=list(handleseries.values())[0];#display(twitter_handle)\n",
    "#         cur=arglist[i][0];#display(cur)\n",
    "#         try:\n",
    "#             tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur)\n",
    "            \n",
    "\n",
    "    \n",
    "#         except:\n",
    "#             errornames.append(twitter_handle)\n",
    "#             print(errornames)\n",
    "#             pass\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=argcreateator(n=3)   \n",
    "    \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=threader, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "#     print(maindict)\n",
    "  \n",
    "#     # print global result list\n",
    " \n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
