{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import random\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "# import multiprocessing\n",
    "# from multiprocessing import Pool\n",
    "from datetime import date\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "from  nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "import concurrent.futures\n",
    "\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from pandas.plotting import table \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib_venn_wordcloud import venn3_wordcloud,venn2_wordcloud\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from matplotlib_venn import venn3, venn3_circles,venn2_circles,venn2,venn2_unweighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#read in top 1,000 celebs\n",
    "url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "celebs=pd.read_csv(url)\n",
    "\n",
    "celebs=celebs.to_dict()\n",
    "# celebs['twitter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Below are two ways of scraping using CLI commands.\n",
    "## Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "## then output two different csv files.\n",
    "#\n",
    "## Query by username\n",
    "## Setting variables to be used in format string command below\n",
    "\n",
    "\n",
    "# tweet_count =500 #this number is the last n tweets\n",
    "# dflist=[]\n",
    "# dfdict={}\n",
    "# count=-1*tweet_count\n",
    "# celeblen=len(celebs.get('twitter').keys())\n",
    "# numindex=list(range(0,tweet_count*celeblen))\n",
    "# len(numindex)\n",
    "\n",
    "\n",
    "# dictcount=0\n",
    "# errornames=[]\n",
    "\n",
    "\n",
    "# for c in range(0,celeblen):\n",
    "#     sleep(random.randrange(0,7)/10)\n",
    "#     dictcount+=1\n",
    "#     count=count+tweet_count\n",
    "#     maxnum=count+tweet_count\n",
    "\n",
    "#     twitter_handle=str(celebs.get('twitter')[c])\n",
    "#     name=str(celebs.get('name')[c])\n",
    "    \n",
    "#     cur=numindex[count:maxnum]\n",
    "   \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "  \n",
    "\n",
    "  \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next  \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "    \n",
    "#         if dictcount==1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                 tweets_df1.update({'name':nameseries})\n",
    "#                 tweets_df1.update({'handle':handleseries})\n",
    "#                 a=tweets_df1.get(key)\n",
    "#                 b=dfdict.get(key)\n",
    "#                 c={**a,**b}\n",
    "#                 dfdict.update({key:c})\n",
    "#     except:\n",
    "#         errornames.append(name)\n",
    "#         print(errornames)\n",
    "#         pass\n",
    "            \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "# dataframe=pd.DataFrame(dfdict).sort_index()\n",
    "# cols=list(set(dataframe.columns)-{'name','handle'})\n",
    "# cols.insert(0,'name')\n",
    "# cols.insert(0,'handle')\n",
    "\n",
    "\n",
    "# dataframe=dataframe[dataframe.lang=='en']\n",
    "\n",
    "# dataframe\n",
    "\n",
    "\n",
    "# pd.to_pickle(dataframe,\"500perpull.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning you need to uncomment above to scrape and create the pickle. \n",
    "\n",
    "## Your speed my vary but it took 45 mins for the scrape\n",
    "\n",
    "## The code below directly below is to the big scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the pickle and then display the unique file\n",
    "\n",
    "dataframe_a=pd.read_pickle(\"./fivezerominpull.pkl\")\n",
    "dataframe_b=pd.read_pickle(\"500perpull.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50762"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "246412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "297174"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataframe_a=dataframe_a[dataframe_a.lang=='en']\n",
    "display(len(dataframe_a))\n",
    "display(len(dataframe_b))\n",
    "merged=pd.concat([dataframe_a,dataframe_b])\n",
    "display(len(merged))\n",
    "merged.name=merged.name.str.lower()\n",
    "dataframe=merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptypeurl='https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv'\n",
    "\n",
    "ptypes=pd.read_csv(ptypeurl);ptypes\n",
    "newcols=[]\n",
    "for x in ptypes.columns.to_list():\n",
    "    y=x.lower()\n",
    "    newcols.append(y)\n",
    "\n",
    "ptypes.columns=newcols\n",
    "ptypes['handle']=ptypes.twitter\n",
    "ptypes.drop(columns='twitter',inplace=True)\n",
    "ptypes.name=ptypes.name.str.lower();ptypes\n",
    "\n",
    "dataframe.name=dataframe.name.str.lower();dataframe.columns\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['handle', 'name', 'content', 'quotedTweet', 'outlinks', 'retweetCount',\n",
       "       'hashtags', 'sourceLabel', 'sourceUrl', 'tcooutlinks', 'inReplyToUser',\n",
       "       'retweetedTweet', '_type', 'media', 'lang', 'url', 'conversationId',\n",
       "       'source', 'date', 'place', 'mentionedUsers', 'quoteCount',\n",
       "       'coordinates', 'replyCount', 'id', 'cashtags', 'inReplyToTweetId',\n",
       "       'user', 'likeCount', 'renderedContent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a=ptypes.name.values.tolist()\n",
    "b=ptypes.type.values.tolist()\n",
    "ptypemap=dict(zip(a,b))\n",
    "# ptypemap\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                      None\n",
       "1                                                      None\n",
       "3                                                      None\n",
       "4                                                      None\n",
       "5                                                      None\n",
       "                                ...                        \n",
       "497395                                                 None\n",
       "497432    {'_type': 'snscrape.modules.twitter.Tweet', 'u...\n",
       "497481    {'_type': 'snscrape.modules.twitter.Tweet', 'u...\n",
       "497487    {'_type': 'snscrape.modules.twitter.Tweet', 'u...\n",
       "497493    {'_type': 'snscrape.modules.twitter.Tweet', 'u...\n",
       "Name: quotedTweet, Length: 297174, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# dataframe.dropna(axis=1,inplace=True)\n",
    "dataframe['type']=dataframe.name.map(ptypemap)\n",
    "\n",
    "\n",
    "\n",
    "cols=list(set(dataframe.columns)-{'name','handle','type'})\n",
    "cols.insert(0,'handle')\n",
    "cols.insert(0,'name')\n",
    "cols.insert(0,'type')\n",
    "\n",
    "dataframe=dataframe[cols]\n",
    "\n",
    "dataframe.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sum(dataframe['quotedTweet']==None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframe.dropna(axis=0,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "display(pd.DataFrame((dataframe['name'].unique()),index=range(0,len(dataframe['name'].unique()))).T)\n",
    "display(dataframe[['type','name']].groupby(['type']).nunique().T)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dataframe.columns.to_frame().T)\n",
    "cols=['type','name','renderedContent','content','handle','date','lang','likeCount','retweetCount','quotedTweet']\n",
    "keep=dataframe[cols];display(len(keep))\n",
    "keep=keep[keep['lang']=='en'];display(len(keep))\n",
    "keep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=keep.groupby('type')\n",
    "typekeys=types.groups.keys()\n",
    "for key in typekeys:\n",
    "    df=types.get_group(key)\n",
    "    namegroup=df.groupby('name')\n",
    "    namekeys=namegroup.groups.keys()\n",
    "    for nm in namekeys:\n",
    "        celeb=namegroup.get_group(nm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquire is done \n",
    "\n",
    "\n",
    "## Prep is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1=keep[['type','name','content']].groupby(by=['type','name'])\n",
    "lista=set(group1.groups.keys())\n",
    "group2=keep[['type','name']].groupby(by=['type'])\n",
    "listb=list(set(group2.groups.keys()))\n",
    "group3=keep[['name','content']].groupby(by=['name'])   \n",
    "indexbyperson={}\n",
    "for b in listb:\n",
    "    g=list(group2.get_group(b).index)\n",
    "    n=list(group2.get_group(b).name.unique())\n",
    "    \n",
    "    ndict={}\n",
    "    for i in n:\n",
    "        k=list(group3.get_group(i).index)\n",
    "        c=list(group3.get_group(i).content)\n",
    "        ndict.update({i:{'index':k,'content':c}})\n",
    "    indexbyperson.update({b:{'index':g,'name':ndict}})\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# namegroups=allthewaydown.groupby('name')\n",
    "# keys=namegroups.groups.keys()\n",
    "# for k in keys:\n",
    "#     print(len(namegroups.get_group(k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "#pdf plumber\n",
    "#csv.preview\n",
    "import pandas as pd\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_stopwords = ['like', 'im', 'think', 'dont', 'people', 'know', 'one', 'get', 'really','thing',\n",
    "                  'would', 'time', 'type', 'make', 'friend', 'ive', 'much','amp','twitter',\n",
    "                 'say', 'way', 'see', 'thing', 'want', 'thing', 'good', 'something', 'lot',\n",
    "                  'also', 'go', 'always', 'even', 'well', 'someone','https','http','com','co',',',\"'\"]\n",
    "\n",
    "\n",
    "\n",
    "stops=stopwords.words(['french','german','english','spanish','portuguese'])+ more_stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(stops,'stopwords.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "    stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "   \n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops and len(x)>=2), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    " \n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''   \n",
    "    Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    #make a copy and begin to transform it\n",
    "    newtext = text.lower()\n",
    "\n",
    "    #encode into ascii then decode\n",
    "    newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "\n",
    "    #use re.sub to remove special characters\n",
    "    newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return newtext\n",
    "\n",
    "    \n",
    "def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    \n",
    "      Takes text, tokenizes it, lemmatizes it\n",
    "      lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "      needs to be commented out after the first run (up to modeling)\n",
    "      # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "      needs to be un commented commented\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "\n",
    "    #make ready the lemmatizer object\n",
    "    newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "    lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return lemmafiltered\n",
    "    \n",
    "    \n",
    "def split_apply_join(funct,listobj):\n",
    "    'helperfuction letters'\n",
    "\n",
    "    mapped=map(funct, listobj)\n",
    "    mapped=list(mapped)\n",
    "    mapped=''.join(mapped)\n",
    "  \n",
    "    return mapped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    ''' \n",
    "    For a large file just save it locally\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "    #make ready tokenizer object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "    return newtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_list = []\n",
    "def domainmapper(spot):\n",
    "        if (spot == 'intj') | (spot == 'entj') | (spot == 'intp') | (spot == 'entp'):\n",
    "            return 'analyst'\n",
    "        elif (spot == 'infj') | (spot == 'enfj') | (spot == 'infp') | (spot == 'enfp'):\n",
    "            return 'diplomat'\n",
    "        elif (spot == 'istj') | (spot == 'estj') | (spot == 'isfj') | (spot == 'esfj'):\n",
    "            return 'sentinel'\n",
    "        elif (spot == 'istp') | (spot == 'estp') | (spot == 'isfp') | (spot == 'esfp'):\n",
    "            return 'explorer'\n",
    "        # else:\n",
    "        #     new_list.append('other')\n",
    "\n",
    "\n",
    "\n",
    "def pairwiseattributemapper(df):\n",
    "    x=df['type']\n",
    "    i_e={}\n",
    "\n",
    "    n_s={}\n",
    "\n",
    "    t_f={}\n",
    "\n",
    "    j_p={}\n",
    "\n",
    "\n",
    "    for spot in x:\n",
    "        if (spot[0]=='i')|(spot[0]=='e'):\n",
    "            if spot[0]=='i':\n",
    "                i_e.update({spot:'i'})\n",
    "            else:\n",
    "                i_e.update({spot:'e'})    \n",
    "        if (spot[1]=='n')|(spot[1]=='s'):\n",
    "            if spot[1]=='n':\n",
    "                n_s.update({spot:'n'})\n",
    "            else:\n",
    "                n_s.update({spot:'s'})    \n",
    "        if (spot[2]=='t')|(spot[2]=='f'):\n",
    "            if spot[2]=='t':\n",
    "                t_f.update({spot:'t'})\n",
    "            else:\n",
    "                t_f.update({spot:'f'})    \n",
    "        if (spot[3]=='j')|(spot[3]=='p'):\n",
    "            if spot[3]=='j':\n",
    "                j_p.update({spot:'j'})\n",
    "            else:\n",
    "                j_p.update({spot:'p'})    \n",
    "    df['i|e']=x.map(i_e)\n",
    "    df['n|s']=x.map(n_s)\n",
    "    df['t|f']=x.map(t_f)\n",
    "    df['j|p']=x.map(j_p)\n",
    "    return df\n",
    "    \n",
    "       \n",
    "        # else:\n",
    "        #     new_list.append('other')\n",
    "\n",
    "\n",
    "tryagain=keep[['type','name','renderedContent','handle','date','lang','likeCount','retweetCount']]\n",
    "tryagain.type=tryagain.type.str.lower()        \n",
    "\n",
    "tryagain['content']=tryagain['renderedContent']\n",
    "tryagain=tryagain[['type','name','content','handle','date','lang','likeCount','retweetCount']]\n",
    "tryagain['domain'] = tryagain['type'].apply(domainmapper)\n",
    "tryagain=pairwiseattributemapper(tryagain)\n",
    "tryagain=tryagain.drop_duplicates()\n",
    "\n",
    "\n",
    "tryagain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    # create train and test (80/20 split) from the orginal dataframe\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=123, stratify=df.type)\n",
    "    # create train and validate (75/25 split) from the train dataframe\n",
    "    train, val = train_test_split(train, test_size=.25, random_state=123, stratify=train.type)\n",
    "    \n",
    "    return train, val, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def allthewaydownfunc(df):\n",
    "\n",
    "    groups=['type', 'domain', 'i|e', 'n|s', 't|f', 'j|p']\n",
    "\n",
    "\n",
    "    allthewaydict={'group':{},'subgroup':{},'name':{},'docs':{},'lemmatized docs':{},'tf-idf w.r.t name':{},'freq table w.r.t name':{},'handle':{},'date':{},'lang'\t:{},'likeCount':{},'retweetCount':{}}\n",
    "    # groups=(groups[2:4])\n",
    "    count=0\n",
    "    for g in groups:\n",
    "        print(f'{g}\\n')\n",
    "        group=tryagain.groupby(g)\n",
    "        keys=group.groups.keys()\n",
    "        for key in keys:\n",
    "            # print(key)\n",
    "            kthgroup=group.get_group(key)\n",
    "            namegru=kthgroup.groupby('name')\n",
    "            subkeys=namegru.groups.keys()\n",
    "\n",
    "\n",
    "            for sk in subkeys:\n",
    "                curname=namegru.get_group(sk)        \n",
    "                nm=curname.name.unique()[0]\n",
    "                docs=curname.content.values\n",
    "                # splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "                lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "                lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "                fulllist=\" \".join(lemma).split()\n",
    "\n",
    "\n",
    "                valcounts=pd.Series(fulllist).value_counts()\n",
    "                freq=valcounts.values\n",
    "\n",
    "\n",
    "                ##TF-IDF calc\n",
    "                x=np.log10((len(lemma)/freq))\n",
    "                x*=freq\n",
    "                x=np.ndarray.round(x,3)\n",
    "                tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n",
    "                freqtable=dict(valcounts)\n",
    "\n",
    "                # df=idfDf(lemma);display(df)\n",
    "                allthewaydict['group'].update({count:g})\n",
    "                allthewaydict['subgroup'].update({count:key})\n",
    "                allthewaydict['name'].update({count:nm})\n",
    "                allthewaydict['docs'].update({count:list(docs)})\n",
    "                allthewaydict['lemmatized docs'].update({count:list(lemma)})\n",
    "                allthewaydict['tf-idf w.r.t name'].update({count:tf_idf})\n",
    "                allthewaydict['freq table w.r.t name'].update({count:freqtable})\n",
    "                allthewaydict['handle'].update({count:curname.handle.values[0]})\n",
    "                allthewaydict['date'].update({count:curname.date.values})\n",
    "                allthewaydict['lang'].update({count:curname.lang.values[0]})\n",
    "                allthewaydict['likeCount'].update({count:curname.likeCount.values.sum()})\n",
    "                allthewaydict['retweetCount'].update({count:curname.retweetCount.values.sum()})\n",
    "\n",
    "\n",
    "                count+=1\n",
    "\n",
    "\n",
    "    allthewaydown=pd.DataFrame(allthewaydict)\n",
    "\n",
    "\n",
    "    return allthewaydown\n",
    "\n",
    "\n",
    "\n",
    "def splitPrep(tryagain):\n",
    "    train, val, test=split_data(tryagain)\n",
    "    train=allthewaydownfunc(train)\n",
    "    val=allthewaydownfunc(val)\n",
    "    test=allthewaydownfunc(test)\n",
    "    pd.to_pickle(train,'mvp_plus_1_train.pkl')\n",
    "    pd.to_pickle(val,'mvp_plus_1_val.pkl')\n",
    "    pd.to_pickle(test,'mvp_plus_1_test.pkl')\n",
    " \n",
    "\n",
    " \n",
    "splitPrep(tryagain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calc_idf(word,**kwargs):\n",
    "#         documents =kwargs['docs']\n",
    "#         n_occurences = sum([1 for doc in documents if word in doc])\n",
    "#         return len(documents) / n_occurences\n",
    "\t\t\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Get a list of the unique words\n",
    "# def idfDf(documents):\n",
    "#     unique_words = pd.Series(','.join(documents).split()).unique()\n",
    "#     kwargs={'docs':documents}\n",
    "#     # put the unique words into a data frame\n",
    "#     df=(pd.DataFrame(dict(word=unique_words))\n",
    "#      # calculate the idf for each word\n",
    "#      .assign(idf=lambda df: df.word.apply(calc_idf))\n",
    "#      # sort the data for presentation purposes\n",
    "#      .set_index('word')\n",
    "#      .sort_values(by='idf', ascending=False))\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# groups=['type', 'domain', 'i|e', 'n|s', 't|f', 'j|p']\n",
    "# allthewaydict={'group':{},'subgroup':{},'name':{},'docs':{},'lemmatized docs':{},'tf-idf w.r.t name':{},'freq table w.r.t name':{},'handle':{},'date':{},'lang'\t:{},'likeCount':{},'retweetCount':{}}\n",
    "# # groups=(groups[2:4])\n",
    "# count=0\n",
    "# g=groups[0]\n",
    "# print(f'{g}\\n')\n",
    "# group=tryagain.groupby(g)\n",
    "# keys=group.groups.keys()\n",
    "# # for key in keys:\n",
    "# key=list(keys)[0]\n",
    "# # print(key)\n",
    "# kthgroup=group.get_group(key)\n",
    "# namegru=kthgroup.groupby('name')\n",
    "# subkeys=namegru.groups.keys()\n",
    "# # for sk in subkeys:\n",
    "# #     count+=\n",
    "# curname=namegru.get_group(list(subkeys)[4])\n",
    "# display(curname)\n",
    "# nm=curname.name.unique()[0]\n",
    "# docs=curname.content.values\n",
    "# # splitdocs=docs.split('')\n",
    "\n",
    "\n",
    "# lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "# lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "# fulllist=\" \".join(lemma).split()\n",
    "\n",
    "\n",
    "# valcounts=pd.Series(fulllist).value_counts()\n",
    "# freq=valcounts.values\n",
    "\n",
    "\n",
    "# ##TF-IDF calc\n",
    "# x=np.log10((len(lemma)/freq))\n",
    "# x*=freq\n",
    "# x=np.ndarray.round(x,3)\n",
    "# tf_idf=(dict(zip((pd.Series(fulllist).value_counts().index),x)))\n",
    "# freqtable=dict(valcounts)\n",
    "\n",
    "# # df=idfDf(lemma);display(df)\n",
    "# allthewaydict['group'].update({count:g})\n",
    "# allthewaydict['subgroup'].update({count:key})\n",
    "# allthewaydict['name'].update({count:nm})\n",
    "# allthewaydict['docs'].update({count:docs})\n",
    "# allthewaydict['lemmatized docs'].update({count:lemma})\n",
    "# allthewaydict['tf-idf w.r.t name'].update({count:tf_idf})\n",
    "# allthewaydict['freq table w.r.t name'].update({count:freqtable})\n",
    "\n",
    "# allthewaydict['handle'].update({count:curname.handle.values[0]})\n",
    "# allthewaydict['date'].update({count:curname.date.values})\n",
    "# allthewaydict['lang'].update({count:curname.lang.values[0]})\n",
    "# allthewaydict['likeCount'].update({count:curname.likeCount.values.sum()})\n",
    "# allthewaydict['retweetCount'].update({count:curname.retweetCount.values.sum()})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# count+=1\n",
    "\n",
    "# pd.DataFrame(allthewaydict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep is finished \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explore is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tryagain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tryagain=pd.read_pickle('mvp_plus_1_train.pkl')\n",
    "\n",
    "tryagain.columns\n",
    "\n",
    "\n",
    "\n",
    "# s = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "# df['sentiment'] = df.lemmatized.apply(lambda doc: s.polarity_scores(doc)['compound'])\n",
    "# df['message_length'] = df['lemmatized'].str.len()\n",
    "# df['word_count'] = (df['lemmatized'].str.split(' ').apply(len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroupdict={\n",
    "'subgroup':{}, \n",
    "'names':{}, \n",
    "'docs':{}, \n",
    "'lemmatized docs':{},\n",
    "'handle':{}, \n",
    "'date':{}, \n",
    "'lang':{},\n",
    "'likeCount':{},\n",
    "'retweetCount':{},\n",
    "'docs w.r.t subgroup':{}, \n",
    "'lemmatized docs w.r.t subgroup':{},\n",
    "'freq table w.r.t subgroup':{},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "groups=tryagain.groupby('group')\n",
    "keys=groups.groups.keys()\n",
    "for k in keys:\n",
    "    cur=groups.get_group(k)\n",
    "    curgroup=cur.groupby(['group','subgroup'])\n",
    "    curkeys=curgroup.groups.keys()\n",
    "    \n",
    "    print('\\n')\n",
    "    print(k)\n",
    "    print('\\n')\n",
    "    for c in curkeys:\n",
    "        print('\\n')\n",
    "        print(c)\n",
    "        print('\\n')\n",
    "        subcur=cur=curgroup.get_group(c)\n",
    "        # display(subcur)\n",
    "        subgroupdict['subgroup'].update({count:subcur['subgroup'].values[0]})\n",
    "        subgroupdict['names'].update({count:subcur['name'].values})\n",
    "        subgroupdict['docs'].update({count:subcur['docs'].values})\n",
    "        subgroupdict['lemmatized docs'].update({count:subcur['lemmatized docs'].values})\n",
    "        subgroupdict['handle'].update({count:subcur['handle'].values})\n",
    "        subgroupdict['date'].update({count:subcur['date'].values})\n",
    "        subgroupdict['lang'].update({count:subcur['lang'].unique()[0]})\n",
    "        subgroupdict['likeCount'].update({count:sum(subcur['likeCount'].values)})\n",
    "        subgroupdict['retweetCount'].update({count:sum(subcur['retweetCount'].values)})\n",
    "\n",
    "\n",
    "        \n",
    "        docslist= subcur['docs'].values\n",
    "        lemmatized=[]\n",
    "        [lemmatized.extend(i) for i in (subcur['lemmatized docs'].values)]\n",
    "\n",
    "        ## lemmatized is now in sentenaces\n",
    "        docs=[]\n",
    "        [docs.extend(i) for i in docslist]\n",
    "        # lemmatized=' '.join(lemmatized).split(',')\n",
    "        valcounts=pd.Series(lemmatized).value_counts()\n",
    "        \n",
    "        lemmatized=' '.join(lemmatized)\n",
    "        valcounts=pd.Series(lemmatized.split()).value_counts();print(valcounts)\n",
    "        freq=valcounts.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        subgroupdict['docs w.r.t subgroup'].update({count:docs})\n",
    "        subgroupdict['lemmatized docs w.r.t subgroup'].update({count:lemmatized})\n",
    "        subgroupdict['freq table w.r.t subgroup'].update({count:dict(valcounts)})\n",
    "\n",
    "        count+=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subgroupDf=pd.DataFrame(subgroupdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrouplist=subgroupDf.subgroup.value_counts().index.tolist()\n",
    "[print(i) for i in subgrouplist if len(i)==4]\n",
    "subgroupDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for the big prep loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "bigdict={'type':{},'name':{},'stoped_lemma':{},'freq':{}}\n",
    "for i in list(indexbyperson.keys()):\n",
    "    a=indexbyperson.get(i)\n",
    "    a=a['name']\n",
    "    for i1 in list(a.keys()):\n",
    "        listtonormaliz=str(a[i1]['content'])\n",
    "        newtext=lemmatizor(listtonormaliz,regexfilter=r'[^a-z0-9\\'\\s]')\n",
    "        lemma=newtext\n",
    "       \n",
    "        stoped=stopfilter(lemma)\n",
    "        stoped=stoped.replace('https','').replace('com','').replace('co','').replace(',','').strip()\n",
    "       \n",
    "        a[i1].update({'stopped_lemma':stoped})         \n",
    "     \n",
    "        cool=dict(pd.Series(stoped.split()).value_counts())\n",
    "        a[i1].update({'word freq':cool})\n",
    "        bigdict['type'].update({num:i})\n",
    "        bigdict['stoped_lemma'].update({num:stoped})\n",
    "        bigdict['freq'].update({num:cool})\n",
    "        bigdict['name'].update({num:i1})\n",
    "        num+=1\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma=pd.DataFrame(bigdict)\n",
    "twitterwordslemma.columns=['type','name','lemmatized','freq']\n",
    "twitterwordslemma\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma['type']=twitterwordslemma.type.str.lower()\n",
    "\n",
    "pd.to_pickle(twitterwordslemma,'maindalemma.pkl')\n",
    "\n",
    "df=pd.read_pickle('maindalemma.pkl')\n",
    "df=df[[\t'type',\t'name',\t'lemmatized'\t]]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaing is explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterwordslemma=pd.read_pickle('maindalemma.pkl')\n",
    "twitterwordslemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "bigdict_type={'type':{},'stoped_lemma':{},'freq':{}}\n",
    "\n",
    "group1=twitterwordslemma[['stoped_lemma','type']].groupby('type')\n",
    "group1.groups.keys()\n",
    "for i in group1.groups.keys():\n",
    "    \n",
    "    x=(','.join(list(group1.get_group(i).stoped_lemma.values)).strip())\n",
    "  \n",
    "    x=stopfilter(x)\n",
    "    \n",
    "   \n",
    "    y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "    cool=dict(y)\n",
    "    bigdict_type['type'].update({num:i})\n",
    "    bigdict_type['stoped_lemma'].update({num:x})\n",
    "    bigdict_type['freq'].update({num:cool})\n",
    "       \n",
    "\n",
    "    num+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.DataFrame(bigdict_type)\n",
    "x=str(typeslemma.stoped_lemma.values).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'').replace(',',' ').split()\n",
    "y=dict(pd.Series(x).value_counts())\n",
    "aggregatewordfrreq=y\n",
    "# pd.to_pickle(aggregatewordfrreq,'agglemma.pkl')\n",
    "num=len(typeslemma)\n",
    "\n",
    "\n",
    "z=[i.replace(',',' ') for i in typeslemma.stoped_lemma.values]\n",
    "typeslemma=pd.concat([typeslemma,pd.DataFrame({'type':{num:'COMBINED'},'stoped_lemma':{num:str(z).replace(',',' ').replace('[','').replace(']','').replace('\"','').replace(\"'\",'')},'freq':{num:aggregatewordfrreq}})])\n",
    "pd.to_pickle(typeslemma,'typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "typeslemma=pd.read_pickle('typeslemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extroverteddf=typeslemma[['type','stoped_lemma']].iloc[0:7]\n",
    "extroverteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "introverteddf=typeslemma[['type','stoped_lemma']].iloc[8:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqdict=dict(typeslemma.freq.values[-1])\n",
    "#set stuff\n",
    "setlist=[]\n",
    "for i in typeslemma.freq.values:\n",
    "    setlist.append(set(i.keys()))\n",
    "\n",
    "typelist=[]\n",
    "for i in typeslemma.type.values:\n",
    "    typelist.append(i)\n",
    "\n",
    "typessetdict=dict(zip(typelist,setlist))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys=list(typessetdict.keys())\n",
    "combined=keys.pop(-1);combined\n",
    "intersectiondict={}\n",
    "c=set(typessetdict.get(combined))\n",
    "keys.reverse()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['red','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigint=deepcopy(c)\n",
    "[bigint.intersection_update(typessetdict.get(key))for key in keys]\n",
    "print(f'We have a total of {len(bigint)} in the aggregate intersection\\nWe remove that intersection to compare')\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    kthset=kthset-bigint\n",
    "    int=(c&kthset)-bigint\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=(25,25))\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    unionwithoutk=unionwithoutk-bigint\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "    unique=(kthset-unionwithoutk)-bigint\n",
    "\n",
    "    print(f'number unique to {k}\\n{len(unique)}\\n',f'percent unique of aggregate union combined:\\n{(len(unique)/len(c))*100:.2f}%')\n",
    "    restint=(kthset&unionwithoutk)-bigint\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','.35','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from itertools import product \n",
    "  \n",
    "# # Get all permutations of length 2 \n",
    "# # and length 2 \n",
    "# x=[\"\".join(seq) for seq in product(\"01\", repeat=4)]\n",
    "# for i in x:\n",
    "#     print(i[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumforavg=dataframe[['type','name']].groupby(['type']).nunique().sum()\n",
    "tochart=(dataframe[['type','name']].groupby(['type']).nunique()/sumforavg)*100\n",
    "\n",
    "tochart=tochart.reset_index()\n",
    "tochart['percent']=tochart['name']\n",
    "\n",
    "tochart.drop(columns='name',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tochart.index=tochart.type\n",
    "tochart.drop(columns='type',inplace=True)\n",
    "tochart=tochart.sort_values(by='percent',ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "genpoppercent=['13.8% 12.3% 11.6% 8.8% 8.7% 8.5% 8.1% 5.4% 4.4% 4.3% 3.3% 3.2% 2.5% 2.1% 1.8% 1.5%']\n",
    "genpoppercent=str(genpoppercent).replace('%','').split()\n",
    "genpoppercent=[float(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in genpoppercent]\n",
    "\n",
    "\n",
    "types=['ISFJ ESFI ISTJ ISFP ESTI ESFP ENFP ISTP INFP ESTP INTP ENTP ENFJ INTJ ENTI INFT']\n",
    "types=str(types).split()\n",
    "\n",
    "\n",
    "types=[(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in types]\n",
    "\n",
    "pop=pd.DataFrame(index=types,data={'pop percentage':genpoppercent})\n",
    "\n",
    "tochart=pd.concat([tochart,pop],axis=1,join='inner')\n",
    "tochart.rename(columns={'percent':'found percent'},inplace=True)\n",
    "cols=['pop percentage','found percent']\n",
    "tochart=tochart[cols]\n",
    "tochart.sort_values(by='pop percentage',ascending=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "styleddf=tochart.T.style.background_gradient(cmap='Blues',axis=1).format(lambda x : f'{x:.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad1=[0,0,0,0]\n",
    "quad2=[0,0,0,0]\n",
    "quad3=[0,.25,.35,.25]\n",
    "quad4=[.35,.25,.35,.25]\n",
    "\n",
    "explode = []\n",
    "explode.extend(quad1)\n",
    "explode.extend(quad2)\n",
    "explode.extend(quad3)\n",
    "explode.extend(quad4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def format_axes(fig):\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n",
    "        ax.tick_params(labelbottom=False, labelleft=False)\n",
    "m=1.23\n",
    "fig = plt.figure(constrained_layout=False,figsize=(m*20,m*12.361))\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('MBTI: General Population Vs Twitter',fontsize=16,weight='demibold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "kwargs1={'title':'General Population (Pie)   ','ax':ax1,'legend':False,'ylabel':'',   'cmap':'Blues'}\n",
    "\n",
    "tochart.plot.pie(y='found percent',**kwargs1)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "kwargs2={'title':'  Twitter (Pie)   ','ax':ax2,'legend':False,'ylabel':'',   'cmap':'viridis'}\n",
    "tochart.plot.pie(y='pop percentage',**kwargs2)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,1])\n",
    "kwargs3={'ax':ax3,'legend':False,'title':'Twitter (Bar)',   'cmap':'viridis'}\n",
    "\n",
    "tochart.plot.barh(y='found percent',**kwargs3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "kwargs4={'ax':ax4,'legend':False,'title':'General Population (Bar)',   'cmap':'Blues'}\n",
    "tochart.plot.barh(y='pop percentage',**kwargs4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"GridSpec\")\n",
    "format_axes(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display('Summary',styleddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything Below was an attempt at Multithreading and Paralellism\n",
    "* ## This can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Below are two ways of scraping using CLI commands.\n",
    "# Comment or uncomment as you need. If you currently run the script as is it will scrape both queries\n",
    "# then output two different csv files.\n",
    "\n",
    "# Query by username\n",
    "# Setting variables to be used in format string command below\n",
    "# def parralledindexer():\n",
    "#  #tweet count number is the last n tweets\n",
    "#  # #read in top 1,000 celebs\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(list(celebs.get('twitter').keys()))\n",
    "#     numindex=range(0,tweet_count*celeblen)\n",
    "\n",
    "#     c_with_slice={}\n",
    "#     for c in range(0,celeblen,1):  \n",
    "#             count=count+tweet_count\n",
    "#             maxnum=count+tweet_count     \n",
    "#             cur=numindex[count:maxnum]\n",
    "#             c_with_slice.update({c:cur})\n",
    "           \n",
    "\n",
    "    # mod10={}\n",
    "    # mod9={}\n",
    "    # mod8={}\n",
    "    # mod7={}\n",
    "    # mod6={}\n",
    "    # mod5={}\n",
    "    # mod4={}\n",
    "    # mod3={}\n",
    "    # mod2={}\n",
    "    # keylist=list(c_with_slice.keys())\n",
    "    # for i in keylist:\n",
    "    #     if i%10==0:  \n",
    "    #         mod10.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%9==0:      \n",
    "    #         mod9.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%8==0:       \n",
    "    #         mod8.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%7==0:      \n",
    "    #         mod7.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%6==0:      \n",
    "    #         mod6.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%5==0:      \n",
    "    #         mod5.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%4==0:       \n",
    "    #         mod4.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%3==0:      \n",
    "    #         mod3.update({i:c_with_slice.get(i)})\n",
    "    #     elif i%2==0:     \n",
    "    #         mod2.update({i:c_with_slice.get(i)})\n",
    "#     moddicts=c_with_slice\n",
    "        \n",
    "\n",
    "#     # moddicts={**mod10,**mod9,\n",
    "#     # **mod8,\n",
    "#     # **mod7,\n",
    "#     # **mod6,\n",
    "#     # **mod5,\n",
    "#     # **mod4,\n",
    "#     # **mod3,\n",
    "#     # **mod2}\n",
    "#     return moddicts,celebs\n",
    "\n",
    "# moddicts,celebs=parralledindexer()\n",
    "\n",
    "# ###Think of schem to split then pu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def actualparrelel():\n",
    "#     '''\n",
    "    \n",
    "#     slow as shit for input output this is for data processing on the comp\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "#         with Pool(10) as pool:\n",
    "#             arglist=[]   \n",
    "#             for m in moddicts:\n",
    "#                 arglist.append((m,celebs)) \n",
    "\n",
    "\n",
    "           \n",
    "#             results_async=pool.starmap_async(partitionableTwitterscraper,arglist)\n",
    "#             # get the return values\n",
    "#             try:\n",
    "#                 for value in results_async.get():\n",
    "#                     values.append(value)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f'Failed with: {e}')\n",
    "#     return values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def actualthreading():\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     values=[]\n",
    "    \n",
    "    \n",
    "#     # protect the entry point\n",
    "#     if __name__ == '__main__':\n",
    "#         # create and configure the process pool\n",
    "  \n",
    "#             arglist=[]   \n",
    "#             for m,v in moddicts.items():\n",
    "#                 arglist.append({m:v})\n",
    "#             # print(arglist) #this is fine    \n",
    "\n",
    "#     # We can use a with statement to ensure threads are cleaned up promptly\n",
    "\n",
    "#     threads = min(50, len(moddicts))   \n",
    "#     with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "#         # Start the load operations and mark each future with its URL\n",
    "#         data=[]\n",
    "           \n",
    "\n",
    "#         for arg in arglist:\n",
    "#             for res in executor.submit(partitionableTwitterscraper(arg)):\n",
    "#                 executor.shutdown(wait=True)\n",
    "    \n",
    "#                 try:\n",
    "#                     data.append((res))\n",
    "#                     # print(res[0])\n",
    "#                     # print(res[1])\n",
    "\n",
    "\n",
    "\n",
    "#                 except Exception as exc:\n",
    "#                     pass\n",
    "#                 #  print('%r generated an exception: %s' % (result, exc))\n",
    "#         # else:\n",
    "#         #     # print('%r page is %d bytes' % (result, len(data)))\n",
    "    \n",
    "#     #sets the number of threads to the lesser of 30 or length of urls\n",
    "#     return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "  \n",
    "# # empty list with global scope\n",
    "# result = []\n",
    "# # empty list with global scope\n",
    "\n",
    "# def square_list(mylist):\n",
    "#     \"\"\"\n",
    "#     function to square a given list\n",
    "#     \"\"\"\n",
    "#     global result\n",
    "#     # append squares of mylist to global list result\n",
    "#     for num in mylist:\n",
    "#         result.append(num * num)\n",
    "\n",
    "\n",
    "# def partitionableTwitterscraper(c_with_slice):\n",
    "#     moddicts,celebs=parralledindexer()\n",
    "#     c_with_slice=c_with_slice\n",
    "#     # print(c_with_slice)\n",
    "#     tweet_count =100 \n",
    "#     dictcount=0\n",
    "#     dfdict={}\n",
    "#     errornames=[]\n",
    "\n",
    "\n",
    "    \n",
    "#     print(c_with_slice.keys()) \n",
    "#     print('\\n') \n",
    "#     c=list(c_with_slice.keys())[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     twitter_handle=celebs.get('twitter')\n",
    "#     name=celebs.get('name')\n",
    "#     cur=c_with_slice.get(c)  \n",
    "#     #create Series to append the current handle to the dataframe\n",
    "#     handleseries={i:twitter_handle for i in cur}       \n",
    "#     #create Series to append the current name to dataframe   \n",
    "#     nameseries={i:name for i in cur}\n",
    "    \n",
    "\n",
    "    \n",
    "#     try:\n",
    "#         # Using OS library to call CLI commands in Python\n",
    "#         os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "\n",
    "#          # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#          #if there is an error then it will just move to the next artist i.e. failsafe \n",
    "#         tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=pd.Index(cur)).to_dict()\n",
    "\n",
    "#         if dictcount<=1:\n",
    "#             tweets_df1.update({'name':nameseries})\n",
    "#             tweets_df1.update({'handle':handleseries})\n",
    "#             dfdict={**dfdict,**tweets_df1}\n",
    "#         else:\n",
    "#             for key in dfdict.keys():\n",
    "#                     tweets_df1.update({'name':nameseries})\n",
    "#                     tweets_df1.update({'handle':handleseries})\n",
    "#                     a=tweets_df1.get(key)\n",
    "#                     b=dfdict.get(key)\n",
    "#                     c={**a,**b}\n",
    "#                     dfdict=dfdict.update({key:c})\n",
    "#     except:\n",
    "#         # errornames.append(name)\n",
    "#         # print('errornames:\\n',len(errornames))\n",
    "#         pass \n",
    "\n",
    "\n",
    "#     display(dfdict)\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "  \n",
    "   \n",
    "\n",
    "#     return dfdict\n",
    "\n",
    "\n",
    "  \n",
    "# moddicts,celebs=parralledindexer()\n",
    "# values=[]\n",
    "    \n",
    "    \n",
    "\n",
    "            \n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=[]   \n",
    "#     for m,v in moddicts.items():\n",
    "#         arglist.append({m:v})\n",
    "  \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=partitionableTwitterscraper, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "  \n",
    "#     # print global result list\n",
    "#     print(\"Result(in main program): {}\".format(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maindict=list()\n",
    "\n",
    "# def argcreateator(n=3):\n",
    "#     url='https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv'\n",
    "#     tweet_count=100\n",
    "\n",
    "#     celebs=pd.read_csv(url).to_dict()\n",
    "\n",
    "#     tweet_count =n #this number is the last n tweets\n",
    "#     dflist=[]\n",
    "#     dfdict={}\n",
    "#     count=-1*tweet_count\n",
    "#     celeblen=len(celebs.get('twitter').keys())\n",
    "#     numindex=list(range(0,tweet_count*celeblen))\n",
    "#     len(numindex)\n",
    "#     arglist=[]\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "#     for c in range(0,celeblen):\n",
    "#         count=count+tweet_count\n",
    "#         maxnum=count+tweet_count\n",
    "\n",
    "#         twitter_handle=(celebs.get('twitter')[c])\n",
    "#         name=(celebs.get('name')[c])\n",
    "\n",
    "#         cur=pd.Index(numindex[count:maxnum])\n",
    "\n",
    "#         #create Series to append the current handle to the dataframe\n",
    "#         handleseries={i:twitter_handle for i in cur}       \n",
    "#         #create Series to append the current name to dataframe   \n",
    "#         nameseries={i:name for i in cur}\n",
    "#         arglist.append([cur,nameseries,handleseries])\n",
    "#     return arglist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur):\n",
    "#     global maindict\n",
    "#     os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, twitter_handle))\n",
    "#      # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "#      #if there is an error then it will just move to the next  \n",
    "#     tweets_df1 = pd.read_json('user-tweets.json', lines=True).set_index(keys=cur).to_dict()\n",
    "#     tweets_df1.update({'name':nameseries})\n",
    "#     tweets_df1.update({'handle':handleseries})\n",
    "#     maindict.append(pd.DataFrame(tweets_df1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def threader(arglist):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     dictcount=0\n",
    "#     errornames=[]\n",
    "\n",
    "   \n",
    "\n",
    "#     # for i in range(0,len(arglist)):\n",
    "#     for i in range(0,len(arglist)):\n",
    "#         tweet_count=len(arglist[i][0]);#display(tweet_count)\n",
    "#         nameseries=arglist[i][1];#display(nameseries)\n",
    "#         handleseries=arglist[i][2];#display(handleseries)\n",
    "#         twitter_handle=list(handleseries.values())[0];#display(twitter_handle)\n",
    "#         cur=arglist[i][0];#display(cur)\n",
    "#         try:\n",
    "#             tweetrip(tweet_count,twitter_handle,nameseries,handleseries,cur)\n",
    "            \n",
    "\n",
    "    \n",
    "#         except:\n",
    "#             errornames.append(twitter_handle)\n",
    "#             print(errornames)\n",
    "#             pass\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # input list\n",
    "#     arglist=argcreateator(n=3)   \n",
    "    \n",
    "    \n",
    "  \n",
    "#     # creating new process\n",
    "#     p1 = multiprocessing.Process(target=threader, args=(arglist,))\n",
    "#     # starting process\n",
    "#     p1.start()\n",
    "#     # wait until process is finished\n",
    "#     p1.join()\n",
    "#     print(maindict)\n",
    "  \n",
    "#     # print global result list\n",
    " \n",
    "\n",
    "  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
