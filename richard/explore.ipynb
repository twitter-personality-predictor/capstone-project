{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import random\n",
    "\n",
    "from datetime import date\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plt.style.use('fivethirtyeight')\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "from nltk.sentiment import  SentimentIntensityAnalyzer\n",
    "\n",
    "import os\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ptwitterwordslemma plumber\n",
    "#csv.preview\n",
    "import pandas as pd\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib_venn_wordcloud import venn3_wordcloud,venn2_wordcloud\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from matplotlib_venn import venn3, venn3_circles,venn2_circles,venn2,venn2_unweighted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from the pickle and then display the unique file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The remaining is explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# twitterwordslemma['word_count'] = twitterwordslemma['lemmatized'].str.split(' ').apply(len)\n",
    "\n",
    "# #Message_length feature:\n",
    "# twitterwordslemma['message_length'] = twitterwordslemma['lemmatized'].str.len()\n",
    "\n",
    "# #sentiment feature:\n",
    "# s = SentimentIntensityAnalyzer()\n",
    "# twitterwordslemma['sentiment'] = twitterwordslemma.lemmatized.apply(lambda doc: s.polarity_scores(doc)['compound'])\n",
    "\n",
    "# new_list = []\n",
    "# for spot in twitterwordslemma['type']:\n",
    "#         if (spot == 'intj') | (spot == 'entj') | (spot == 'intp') | (spot == 'entp'):\n",
    "#             new_list.append('analyst')\n",
    "#         if (spot == 'infj') | (spot == 'enfj') | (spot == 'infp') | (spot == 'enfp'):\n",
    "#             new_list.append('diplomat')\n",
    "#         if (spot == 'istj') | (spot == 'estj') | (spot == 'isfj') | (spot == 'esfj'):\n",
    "#             new_list.append('sentinel')\n",
    "#         if (spot == 'istp') | (spot == 'estp') | (spot == 'isfp') | (spot == 'esfp'):\n",
    "#             new_list.append('explorer')\n",
    "#         # else:\n",
    "#         #     new_list.append('other')\n",
    "# twitterwordslemma['personality_domain'] = new_list\n",
    "\n",
    "\n",
    "\n",
    "# twitterwordslemma['i_e'] = np.where(twitterwordslemma['type'].str[0] == 'i', 0, 1)\n",
    "# twitterwordslemma['s_n'] = np.where(twitterwordslemma['type'].str[1] == 's', 0, 1)\n",
    "# twitterwordslemma['f_t'] = np.where(twitterwordslemma['type'].str[2] == 'f', 0, 1)\n",
    "# twitterwordslemma['p_j'] = np.where(twitterwordslemma['type'].str[3] == 'p', 0, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def wrangle():\n",
    "    \"\"\"This function is all encompassing to acquire and clean/prepare the data. there are 5 functions that are embedded inside this function that are used to return the \n",
    "    personality information in a DataFrame\"\"\"\n",
    "    dataframe=pd.read_pickle(\"./fivezerominpull.pkl\")\n",
    "    dataframe.name.nunique()\n",
    "\n",
    "    ptypeurl='https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv'\n",
    "    ptypes=pd.read_csv(ptypeurl);ptypes\n",
    "    newcols=[]\n",
    "    for x in ptypes.columns.to_list():\n",
    "        y=x.lower()\n",
    "        newcols.append(y)\n",
    "\n",
    "    ptypes.columns=newcols\n",
    "    ptypes['handle']=ptypes.twitter\n",
    "    ptypes.drop(columns='twitter',inplace=True)\n",
    "    ptypes.name=ptypes.name.str.lower();ptypes\n",
    "\n",
    "    dataframe.name=dataframe.name.str.lower();dataframe\n",
    "\n",
    "    a=ptypes.name.values.tolist()\n",
    "    b=ptypes.type.values.tolist()\n",
    "    ptypemap=dict(zip(a,b))\n",
    "    ptypemap\n",
    "    dataframe.dropna(axis=1,inplace=True)\n",
    "    dataframe['type']=dataframe.name.map(ptypemap)\n",
    "\n",
    "    cols=list(set(dataframe.columns)-{'name','handle','type'})\n",
    "    cols.insert(0,'handle')\n",
    "    cols.insert(0,'name')\n",
    "    cols.insert(0,'type')\n",
    "\n",
    "    dataframe=dataframe[cols]\n",
    "    dataframe.dropna(axis=0,inplace=True)\n",
    "\n",
    "    dataframe.columns.to_frame().T\n",
    "    cols=['type','name','renderedContent','content']\n",
    "    keep=dataframe[cols];keep\n",
    "\n",
    "    group1=keep[['type','name','content']].groupby(by=['type','name'])\n",
    "    lista=set(group1.groups.keys())\n",
    "    group2=keep[['type','name']].groupby(by=['type'])\n",
    "    listb=list(set(group2.groups.keys()))\n",
    "    group3=keep[['name','content']].groupby(by=['name'])   \n",
    "    indexbyperson={}\n",
    "    for b in listb:\n",
    "        g=list(group2.get_group(b).index)\n",
    "        n=list(group2.get_group(b).name.unique())\n",
    "        \n",
    "        ndict={}\n",
    "        for i in n:\n",
    "            k=list(group3.get_group(i).index)\n",
    "            c=list(group3.get_group(i).content)\n",
    "            ndict.update({i:{'index':k,'content':c}})\n",
    "        indexbyperson.update({b:{'index':g,'name':ndict}})\n",
    "    \n",
    "    more_stopwords = ['like', 'im', 'think', 'dont', 'people', 'know', 'one', 'get', 'really','thing',\n",
    "                  'would', 'time', 'type', 'make', 'friend', 'ive', 'feel', 'much', 'love',\n",
    "                 'say', 'way', 'see', 'thing', 'want', 'thing', 'good', 'something', 'lot',\n",
    "                  'also', 'go', 'always', 'even', 'well', 'someone','https','com','co',',',\"'\"]\n",
    "    stops=stopwords.words(['french','german','english','spanish','portuguese'])+ more_stopwords\n",
    "\n",
    "    pd.to_pickle(stops,'stopwords.pkl')\n",
    "\n",
    "    \n",
    "    \n",
    "    def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "        'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "        #create oujr english stopwords list\n",
    "        stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "    \n",
    "        stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "        stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "        # stops=(stops|stop_words_extend)-exclude_words\n",
    "        #another way\n",
    "        \n",
    "        filtered=list(filter((lambda x: x not in stops), text.split()))\n",
    "        filtered=' '.join(filtered)\n",
    "\n",
    "        return filtered\n",
    "\n",
    "    def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "        '''   \n",
    "        Filters out all special characters if you need to edit then supply a new regex filter \n",
    "        '''\n",
    "        #make a copy and begin to transform it\n",
    "        newtext = text.lower()\n",
    "        #encode into ascii then decode\n",
    "        newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8')\n",
    "        #use re.sub to remove special characters\n",
    "        newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "        return newtext\n",
    "       \n",
    "    def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "        '''     \n",
    "        Takes text, tokenizes it, lemmatizes it\n",
    "        lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "        needs to be commented out after the first run (up to modeling)\n",
    "        # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "        needs to be un commented commented      \n",
    "        '''\n",
    "        total=list(pd.read_pickle('words.pkl'))\n",
    "        #make ready the lemmatizer object\n",
    "        newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "        wnl = nltk.stem.WordNetLemmatizer()\n",
    "        lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "        # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "        # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "         # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "        lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "        lemmafiltered=' '.join(lemmafiltered)\n",
    "        lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "        return lemmafiltered \n",
    "        \n",
    "    def split_apply_join(funct,listobj):\n",
    "        'helperfuction letters'\n",
    "        mapped=map(funct, listobj)\n",
    "        mapped=list(mapped)\n",
    "        mapped=''.join(mapped)\n",
    "        return mapped\n",
    "\n",
    "    def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "        ''' \n",
    "        For a large file just save it locally\n",
    "        '''\n",
    "        newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "        #make ready tokenizer object\n",
    "        tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "        #use the tokenizer\n",
    "        newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "        return newtext\n",
    "\n",
    "    num=0\n",
    "    bigdict={'type':{},'name':{},'stoped_lemma':{},'freq':{}}\n",
    "    for i in list(indexbyperson.keys()):\n",
    "        a=indexbyperson.get(i)\n",
    "        a=a['name']\n",
    "        for i1 in list(a.keys()):\n",
    "            listtonormaliz=str(a[i1]['content'])\n",
    "            newtext=lemmatizor(listtonormaliz,regexfilter=r'[^a-z0-9\\'\\s]')\n",
    "            lemma=newtext\n",
    "        \n",
    "            stoped=stopfilter(lemma)\n",
    "            stoped=stoped.replace('https','').replace('com','').replace('co','').replace(',','').strip()\n",
    "        \n",
    "            a[i1].update({'stopped_lemma':stoped})         \n",
    "        \n",
    "            cool=dict(pd.Series(stoped.split()).value_counts())\n",
    "            a[i1].update({'word freq':cool})\n",
    "            bigdict['type'].update({num:i})\n",
    "            bigdict['stoped_lemma'].update({num:stoped})\n",
    "            bigdict['freq'].update({num:cool})\n",
    "            bigdict['name'].update({num:i1})\n",
    "            num+=1\n",
    "\n",
    "    twitterwordslemma=pd.DataFrame(bigdict)\n",
    "    twitterwordslemma.columns=['type','name','lemmatized','freq']\n",
    "    twitterwordslemma['type']=twitterwordslemma.type.str.lower()\n",
    "    pd.to_pickle(twitterwordslemma,'maindalemma.pkl')\n",
    "    df=pd.read_pickle('maindalemma.pkl')\n",
    "    df=df[[\t'type',\t'name',\t'lemmatized'\t]]\n",
    "    \n",
    "    new_list = []\n",
    "    for spot in df['type']:\n",
    "            if (spot == 'intj') | (spot == 'entj') | (spot == 'intp') | (spot == 'entp'):\n",
    "                new_list.append('analyst')\n",
    "            if (spot == 'infj') | (spot == 'enfj') | (spot == 'infp') | (spot == 'enfp'):\n",
    "                new_list.append('diplomat')\n",
    "            if (spot == 'istj') | (spot == 'estj') | (spot == 'isfj') | (spot == 'esfj'):\n",
    "                new_list.append('sentinel')\n",
    "            if (spot == 'istp') | (spot == 'estp') | (spot == 'isfp') | (spot == 'esfp'):\n",
    "                new_list.append('explorer')\n",
    "            # else:\n",
    "            #     new_list.append('other')\n",
    "    df['personality_domain'] = new_list\n",
    "    \n",
    "    s = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "    df['sentiment'] = df.lemmatized.apply(lambda doc: s.polarity_scores(doc)['compound'])\n",
    "    df['message_length'] = df['lemmatized'].str.len()\n",
    "    df['word_count'] = df['lemmatized'].str.split(' ').apply(len)\n",
    "    \n",
    "    df['i_e'] = np.where(df['type'].str[0] == 'i', 'i', 'e')\n",
    "    df['s_n'] = np.where(df['type'].str[1] == 's', 's', 'n')\n",
    "    df['f_t'] = np.where(df['type'].str[2] == 'f', 'f', 't')\n",
    "    df['p_j'] = np.where(df['type'].str[3] == 'p', 'p', 'j')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_data(df):\n",
    "    # create train and test (80/20 split) from the orginal dataframe\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=123, stratify=df.type)\n",
    "    # create train and validate (75/25 split) from the train dataframe\n",
    "    train, val = train_test_split(train, test_size=.25, random_state=123, stratify=train.type)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "    stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "   \n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train, val, test=split_data(df=wrangle())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "typelist=['intj',\n",
    " 'esfp',\n",
    " 'estp',\n",
    " 'isfj',\n",
    " 'istp',\n",
    " 'entp',\n",
    " 'istj',\n",
    " 'isfp',\n",
    " 'estj',\n",
    " 'entj',\n",
    " 'infp',\n",
    " 'infj',\n",
    " 'enfp',\n",
    " 'esfj',\n",
    " 'enfj',\n",
    " 'intp']\n",
    "\n",
    "bound=int((len(typelist))/2)\n",
    "typelist.sort()\n",
    "exts=typelist[0:bound]\n",
    "ints=typelist[bound:-1]\n",
    "\n",
    "\n",
    "bound=int((len(exts))/2)\n",
    "lowext=exts[0:bound]\n",
    "highext=exts[bound:-1]\n",
    "\n",
    "\n",
    "bound=int((len(ints))/2)\n",
    "lowints=ints[0:bound]\n",
    "highints=ints[bound:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enfj', 'enfp', 'entj', 'entp']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['esfj', 'esfp', 'estj']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['infj', 'infp', 'intj']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['enfj',\n",
       " 'enfp',\n",
       " 'entj',\n",
       " 'entp',\n",
       " 'esfj',\n",
       " 'esfp',\n",
       " 'estj',\n",
       " 'estp',\n",
       " 'infj',\n",
       " 'infp',\n",
       " 'intj',\n",
       " 'intp',\n",
       " 'isfj',\n",
       " 'isfp',\n",
       " 'istj',\n",
       " 'istp']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "display(lowext)\n",
    "display(highext)\n",
    "display(lowints)\n",
    "display(typelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>personality_domain</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>message_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>i_e</th>\n",
       "      <th>s_n</th>\n",
       "      <th>f_t</th>\n",
       "      <th>p_j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>intj</td>\n",
       "      <td>zedd</td>\n",
       "      <td>valorant gale adelade e valorant boys fuslie b...</td>\n",
       "      <td>analyst</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>2406</td>\n",
       "      <td>391</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>esfj</td>\n",
       "      <td>anushka sharma</td>\n",
       "      <td>handbags nvibrant hues every mood nshop amazin...</td>\n",
       "      <td>sentinel</td>\n",
       "      <td>0.9998</td>\n",
       "      <td>5005</td>\n",
       "      <td>785</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>estp</td>\n",
       "      <td>daniel alves</td>\n",
       "      <td>join members phygital art club mint medusa nft...</td>\n",
       "      <td>explorer</td>\n",
       "      <td>0.9959</td>\n",
       "      <td>3969</td>\n",
       "      <td>638</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>enfj</td>\n",
       "      <td>basti schweinsteiger</td>\n",
       "      <td>sunday walk proud nyou still vote another golf...</td>\n",
       "      <td>diplomat</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>4231</td>\n",
       "      <td>659</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>esfp</td>\n",
       "      <td>nicole polizzi</td>\n",
       "      <td>enjoy mawma job proud dwts made day thanks cra...</td>\n",
       "      <td>explorer</td>\n",
       "      <td>0.9811</td>\n",
       "      <td>3031</td>\n",
       "      <td>515</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>esfj</td>\n",
       "      <td>angel locsin</td>\n",
       "      <td>new vlog drop today nmore spain trip videos in...</td>\n",
       "      <td>sentinel</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>4037</td>\n",
       "      <td>685</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>esfp</td>\n",
       "      <td>jeremy piven</td>\n",
       "      <td>show called knew day e great hang hes legend m...</td>\n",
       "      <td>explorer</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>3422</td>\n",
       "      <td>572</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>entp</td>\n",
       "      <td>david spade</td>\n",
       "      <td>exactly glad going use luck let anyone sees le...</td>\n",
       "      <td>analyst</td>\n",
       "      <td>0.9976</td>\n",
       "      <td>2583</td>\n",
       "      <td>433</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>entp</td>\n",
       "      <td>craig ferguson</td>\n",
       "      <td>happy nmade big tam crusty old b sos horse lor...</td>\n",
       "      <td>analyst</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>4063</td>\n",
       "      <td>663</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>esfj</td>\n",
       "      <td>chris evans</td>\n",
       "      <td>loved pillars creation new photo makes happy n...</td>\n",
       "      <td>sentinel</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>3425</td>\n",
       "      <td>535</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     type                  name  \\\n",
       "216  intj                  zedd   \n",
       "378  esfj        anushka sharma   \n",
       "535  estp          daniel alves   \n",
       "517  enfj  basti schweinsteiger   \n",
       "75   esfp        nicole polizzi   \n",
       "..    ...                   ...   \n",
       "372  esfj          angel locsin   \n",
       "135  esfp          jeremy piven   \n",
       "356  entp           david spade   \n",
       "357  entp        craig ferguson   \n",
       "413  esfj           chris evans   \n",
       "\n",
       "                                            lemmatized personality_domain  \\\n",
       "216  valorant gale adelade e valorant boys fuslie b...            analyst   \n",
       "378  handbags nvibrant hues every mood nshop amazin...           sentinel   \n",
       "535  join members phygital art club mint medusa nft...           explorer   \n",
       "517  sunday walk proud nyou still vote another golf...           diplomat   \n",
       "75   enjoy mawma job proud dwts made day thanks cra...           explorer   \n",
       "..                                                 ...                ...   \n",
       "372  new vlog drop today nmore spain trip videos in...           sentinel   \n",
       "135  show called knew day e great hang hes legend m...           explorer   \n",
       "356  exactly glad going use luck let anyone sees le...            analyst   \n",
       "357  happy nmade big tam crusty old b sos horse lor...            analyst   \n",
       "413  loved pillars creation new photo makes happy n...           sentinel   \n",
       "\n",
       "     sentiment  message_length  word_count i_e s_n f_t p_j  \n",
       "216     0.9983            2406         391   i   n   t   j  \n",
       "378     0.9998            5005         785   e   s   f   j  \n",
       "535     0.9959            3969         638   e   s   t   p  \n",
       "517     0.9995            4231         659   e   n   f   j  \n",
       "75      0.9811            3031         515   e   s   f   p  \n",
       "..         ...             ...         ...  ..  ..  ..  ..  \n",
       "372     0.9136            4037         685   e   s   f   j  \n",
       "135     0.9994            3422         572   e   s   f   p  \n",
       "356     0.9976            2583         433   e   n   t   p  \n",
       "357     0.9984            4063         663   e   n   t   p  \n",
       "413     0.9992            3425         535   e   s   f   j  \n",
       "\n",
       "[359 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "twitterwordslemma=train\n",
    "twitterwordslemma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>freq</th>\n",
       "      <th>i_e</th>\n",
       "      <th>s_n</th>\n",
       "      <th>f_t</th>\n",
       "      <th>p_j</th>\n",
       "      <th>personality_domain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>sunday walk proud nyou still vote another golf...</td>\n",
       "      <td>{'amp': 137, 'thank': 109, 'new': 77, 'ting': ...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "      <td>diplomat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfp</td>\n",
       "      <td>please help dear year told days ago sober happ...</td>\n",
       "      <td>{'amp': 216, 'thank': 160, 'new': 130, 'today'...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>diplomat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entj</td>\n",
       "      <td>tricks treats join us sweet treats street burg...</td>\n",
       "      <td>{'live': 115, 'agt': 82, 'espnplus': 75, 'vs':...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entp</td>\n",
       "      <td>florida pg pg proud champ follow boy nuevo pas...</td>\n",
       "      <td>{'new': 142, 'amp': 108, 'hanx': 83, 'hulu': 8...</td>\n",
       "      <td>e</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esfj</td>\n",
       "      <td>handbags nvibrant hues every mood nshop amazin...</td>\n",
       "      <td>{'thank': 169, 'happy': 157, 'amp': 140, 'new'...</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "      <td>sentinel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>esfp</td>\n",
       "      <td>enjoy mawma job proud dwts made day thanks cra...</td>\n",
       "      <td>{'amp': 321, 'thank': 257, 'new': 219, 'day': ...</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>estj</td>\n",
       "      <td>take happy try watch twice watched least times...</td>\n",
       "      <td>{'check': 95, 'new': 87, 'agt': 86, 'god': 81,...</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "      <td>sentinel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>estp</td>\n",
       "      <td>join members phygital art club mint medusa nft...</td>\n",
       "      <td>{'amp': 116, 'gracias': 115, 'thank': 91, 'new...</td>\n",
       "      <td>e</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>infj</td>\n",
       "      <td>powerful plants keep yard pest free knew took ...</td>\n",
       "      <td>{'publicar': 92, 'happy': 89, 'day': 75, 'morn...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "      <td>diplomat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>infp</td>\n",
       "      <td>song video journey proud guys finally hope yal...</td>\n",
       "      <td>{'twitter': 99, 'friends': 98, 'dear': 97, 'am...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>diplomat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>intj</td>\n",
       "      <td>valorant gale adelade valorant boys fuslie bna...</td>\n",
       "      <td>{'new': 47, 'gt': 23, 'amp': 21, 'world': 20, ...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>intp</td>\n",
       "      <td>halo semuanya akun twitter gue cuman satu yang...</td>\n",
       "      <td>{'world': 28, 'amp': 24, 'new': 20, 'gue': 19,...</td>\n",
       "      <td>i</td>\n",
       "      <td>n</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>isfj</td>\n",
       "      <td>gin juju mc door opening starts tomorrow teddy...</td>\n",
       "      <td>{'thank': 120, 'amp': 99, 'new': 86, 'di': 64,...</td>\n",
       "      <td>i</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>j</td>\n",
       "      <td>sentinel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>isfp</td>\n",
       "      <td>benzema ad thats saturday sorted nwho wants pl...</td>\n",
       "      <td>{'thank': 228, 'amp': 147, 'new': 138, 'teampw...</td>\n",
       "      <td>i</td>\n",
       "      <td>s</td>\n",
       "      <td>f</td>\n",
       "      <td>p</td>\n",
       "      <td>explorer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>istj</td>\n",
       "      <td>elonmusk talktv great nvo talktv talktv thrill...</td>\n",
       "      <td>{'minoz': 85, 'leeminho': 75, 'great': 43, 'my...</td>\n",
       "      <td>i</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>j</td>\n",
       "      <td>sentinel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>istp</td>\n",
       "      <td>maureen please baby please theaters morning pi...</td>\n",
       "      <td>{'thank': 78, 'day': 72, 'happy': 71, 'reposte...</td>\n",
       "      <td>i</td>\n",
       "      <td>s</td>\n",
       "      <td>t</td>\n",
       "      <td>p</td>\n",
       "      <td>explorer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type                                         lemmatized  \\\n",
       "0   enfj  sunday walk proud nyou still vote another golf...   \n",
       "1   enfp  please help dear year told days ago sober happ...   \n",
       "2   entj  tricks treats join us sweet treats street burg...   \n",
       "3   entp  florida pg pg proud champ follow boy nuevo pas...   \n",
       "4   esfj  handbags nvibrant hues every mood nshop amazin...   \n",
       "5   esfp  enjoy mawma job proud dwts made day thanks cra...   \n",
       "6   estj  take happy try watch twice watched least times...   \n",
       "7   estp  join members phygital art club mint medusa nft...   \n",
       "8   infj  powerful plants keep yard pest free knew took ...   \n",
       "9   infp  song video journey proud guys finally hope yal...   \n",
       "10  intj  valorant gale adelade valorant boys fuslie bna...   \n",
       "11  intp  halo semuanya akun twitter gue cuman satu yang...   \n",
       "12  isfj  gin juju mc door opening starts tomorrow teddy...   \n",
       "13  isfp  benzema ad thats saturday sorted nwho wants pl...   \n",
       "14  istj  elonmusk talktv great nvo talktv talktv thrill...   \n",
       "15  istp  maureen please baby please theaters morning pi...   \n",
       "\n",
       "                                                 freq i_e s_n f_t p_j  \\\n",
       "0   {'amp': 137, 'thank': 109, 'new': 77, 'ting': ...   e   n   f   j   \n",
       "1   {'amp': 216, 'thank': 160, 'new': 130, 'today'...   e   n   f   p   \n",
       "2   {'live': 115, 'agt': 82, 'espnplus': 75, 'vs':...   e   n   t   j   \n",
       "3   {'new': 142, 'amp': 108, 'hanx': 83, 'hulu': 8...   e   n   t   p   \n",
       "4   {'thank': 169, 'happy': 157, 'amp': 140, 'new'...   e   s   f   j   \n",
       "5   {'amp': 321, 'thank': 257, 'new': 219, 'day': ...   e   s   f   p   \n",
       "6   {'check': 95, 'new': 87, 'agt': 86, 'god': 81,...   e   s   t   j   \n",
       "7   {'amp': 116, 'gracias': 115, 'thank': 91, 'new...   e   s   t   p   \n",
       "8   {'publicar': 92, 'happy': 89, 'day': 75, 'morn...   i   n   f   j   \n",
       "9   {'twitter': 99, 'friends': 98, 'dear': 97, 'am...   i   n   f   p   \n",
       "10  {'new': 47, 'gt': 23, 'amp': 21, 'world': 20, ...   i   n   t   j   \n",
       "11  {'world': 28, 'amp': 24, 'new': 20, 'gue': 19,...   i   n   t   p   \n",
       "12  {'thank': 120, 'amp': 99, 'new': 86, 'di': 64,...   i   s   f   j   \n",
       "13  {'thank': 228, 'amp': 147, 'new': 138, 'teampw...   i   s   f   p   \n",
       "14  {'minoz': 85, 'leeminho': 75, 'great': 43, 'my...   i   s   t   j   \n",
       "15  {'thank': 78, 'day': 72, 'happy': 71, 'reposte...   i   s   t   p   \n",
       "\n",
       "   personality_domain  \n",
       "0            diplomat  \n",
       "1            diplomat  \n",
       "2             analyst  \n",
       "3             analyst  \n",
       "4            sentinel  \n",
       "5            explorer  \n",
       "6            sentinel  \n",
       "7            explorer  \n",
       "8            diplomat  \n",
       "9            diplomat  \n",
       "10            analyst  \n",
       "11            analyst  \n",
       "12           sentinel  \n",
       "13           explorer  \n",
       "14           sentinel  \n",
       "15           explorer  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num=0\n",
    "bigdict_type={'type':{},'lemmatized':{},'freq':{},'i_e':{},'s_n':{},'f_t':{},'p_j':{},'personality_domain':{}}\n",
    "\n",
    "group1=twitterwordslemma[['lemmatized','type','i_e','s_n','f_t','p_j','personality_domain']].groupby('type')\n",
    "group1.groups.keys()\n",
    "for i in group1.groups.keys():\n",
    "    \n",
    "    x=(','.join(list(group1.get_group(i).lemmatized.values)).strip())\n",
    "  \n",
    "    x=stopfilter(x)\n",
    "    \n",
    "   \n",
    "    y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "    cool=dict(y)\n",
    "\n",
    "\n",
    "    # print(group1.get_group(i).i_e.values[0])\n",
    "    i_e=group1.get_group(i).i_e.values[0]\n",
    "    s_n=group1.get_group(i).s_n.values[0]\n",
    "    f_t=group1.get_group(i).f_t.values[0]\n",
    "    p_j=group1.get_group(i).p_j.values[0]\n",
    "    personality_domain=group1.get_group(i).personality_domain.values\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    bigdict_type['type'].update({num:i})\n",
    "    bigdict_type['lemmatized'].update({num:x})\n",
    "    bigdict_type['freq'].update({num:cool})\n",
    "    bigdict_type['personality_domain'].update({num:personality_domain[0]})\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    bigdict_type['i_e'].update({num:i_e})\n",
    "    bigdict_type['s_n'].update({num:s_n})\n",
    "    bigdict_type['f_t'].update({num:f_t})\n",
    "    bigdict_type['p_j'].update({num:p_j})\n",
    "\n",
    "       \n",
    "\n",
    "    num+=1\n",
    "\n",
    "types=pd.DataFrame(bigdict_type)\n",
    "\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num=0\n",
    "\n",
    "\n",
    "domaingroup_dict={'personality_domain':{},'lemmatized':{},'freq':{}}\n",
    "\n",
    "domaingroup=types.groupby('personality_domain')\n",
    "domainkeys=domaingroup.groups.keys()\n",
    "\n",
    "for i in domainkeys:\n",
    "\n",
    "  x=(','.join(list(domaingroup.get_group(i).lemmatized.values)).strip())\n",
    "  x=stopfilter(x)\n",
    "  y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "  cool=dict(y)\n",
    "\n",
    "  domaingroup_dict['personality_domain'].update({num:i})\n",
    "  domaingroup_dict['lemmatized'].update({num:x})\n",
    "  domaingroup_dict['freq'].update({num:cool})\n",
    "  \n",
    "  \n",
    "  num+=1\n",
    "\n",
    "\n",
    "domains=pd.DataFrame(domaingroup_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num=0\n",
    "pairwiselist=['i_e','s_n','f_t','p_j']\n",
    "pairwise={'attribute':{},'lemmatized':{},'freq':{}}\n",
    "\n",
    "for pair in pairwiselist:\n",
    "\n",
    "\n",
    "  pairwisegroup=types.groupby(pair)\n",
    "  pairkeys=pairwisegroup.groups.keys()\n",
    "\n",
    "  for i in pairkeys:\n",
    "\n",
    "    x=(','.join(list(pairwisegroup.get_group(i).lemmatized.values)).strip())\n",
    "    x=stopfilter(x)\n",
    "    y=(pd.Series(x.replace(',',' ').strip().split()).value_counts())\n",
    "    cool=dict(y)\n",
    "\n",
    "    pairwise['attribute'].update({num:i})\n",
    "    pairwise['lemmatized'].update({num:x})\n",
    "    pairwise['freq'].update({num:cool})\n",
    "\n",
    "\n",
    "    num+=1\n",
    "\n",
    "\n",
    "pairs=pd.DataFrame(pairwise)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combined_lemmatized=','.join(list(types.lemmatized))\n",
    "\n",
    "combined_lemmatized=combined_lemmatized.replace(',',' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enfj</td>\n",
       "      <td>sunday walk proud nyou still vote another golf...</td>\n",
       "      <td>{'amp': 137, 'thank': 109, 'new': 77, 'ting': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enfp</td>\n",
       "      <td>please help dear year told days ago sober happ...</td>\n",
       "      <td>{'amp': 216, 'thank': 160, 'new': 130, 'today'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entj</td>\n",
       "      <td>tricks treats join us sweet treats street burg...</td>\n",
       "      <td>{'live': 115, 'agt': 82, 'espnplus': 75, 'vs':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>entp</td>\n",
       "      <td>florida pg pg proud champ follow boy nuevo pas...</td>\n",
       "      <td>{'new': 142, 'amp': 108, 'hanx': 83, 'hulu': 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>esfj</td>\n",
       "      <td>handbags nvibrant hues every mood nshop amazin...</td>\n",
       "      <td>{'thank': 169, 'happy': 157, 'amp': 140, 'new'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>esfp</td>\n",
       "      <td>enjoy mawma job proud dwts made day thanks cra...</td>\n",
       "      <td>{'amp': 321, 'thank': 257, 'new': 219, 'day': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>estj</td>\n",
       "      <td>take happy try watch twice watched least times...</td>\n",
       "      <td>{'check': 95, 'new': 87, 'agt': 86, 'god': 81,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>estp</td>\n",
       "      <td>join members phygital art club mint medusa nft...</td>\n",
       "      <td>{'amp': 116, 'gracias': 115, 'thank': 91, 'new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>infj</td>\n",
       "      <td>powerful plants keep yard pest free knew took ...</td>\n",
       "      <td>{'publicar': 92, 'happy': 89, 'day': 75, 'morn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>infp</td>\n",
       "      <td>song video journey proud guys finally hope yal...</td>\n",
       "      <td>{'twitter': 99, 'friends': 98, 'dear': 97, 'am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>intj</td>\n",
       "      <td>valorant gale adelade valorant boys fuslie bna...</td>\n",
       "      <td>{'new': 47, 'gt': 23, 'amp': 21, 'world': 20, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>intp</td>\n",
       "      <td>halo semuanya akun twitter gue cuman satu yang...</td>\n",
       "      <td>{'world': 28, 'amp': 24, 'new': 20, 'gue': 19,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>isfj</td>\n",
       "      <td>gin juju mc door opening starts tomorrow teddy...</td>\n",
       "      <td>{'thank': 120, 'amp': 99, 'new': 86, 'di': 64,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>isfp</td>\n",
       "      <td>benzema ad thats saturday sorted nwho wants pl...</td>\n",
       "      <td>{'thank': 228, 'amp': 147, 'new': 138, 'teampw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>istj</td>\n",
       "      <td>elonmusk talktv great nvo talktv talktv thrill...</td>\n",
       "      <td>{'minoz': 85, 'leeminho': 75, 'great': 43, 'my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>istp</td>\n",
       "      <td>maureen please baby please theaters morning pi...</td>\n",
       "      <td>{'thank': 78, 'day': 72, 'happy': 71, 'reposte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>combined</td>\n",
       "      <td>sunday walk proud nyou still vote another golf...</td>\n",
       "      <td>{'amp': 1575, 'thank': 1563, 'new': 1399, 'hap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                                         lemmatized  \\\n",
       "0       enfj  sunday walk proud nyou still vote another golf...   \n",
       "1       enfp  please help dear year told days ago sober happ...   \n",
       "2       entj  tricks treats join us sweet treats street burg...   \n",
       "3       entp  florida pg pg proud champ follow boy nuevo pas...   \n",
       "4       esfj  handbags nvibrant hues every mood nshop amazin...   \n",
       "5       esfp  enjoy mawma job proud dwts made day thanks cra...   \n",
       "6       estj  take happy try watch twice watched least times...   \n",
       "7       estp  join members phygital art club mint medusa nft...   \n",
       "8       infj  powerful plants keep yard pest free knew took ...   \n",
       "9       infp  song video journey proud guys finally hope yal...   \n",
       "10      intj  valorant gale adelade valorant boys fuslie bna...   \n",
       "11      intp  halo semuanya akun twitter gue cuman satu yang...   \n",
       "12      isfj  gin juju mc door opening starts tomorrow teddy...   \n",
       "13      isfp  benzema ad thats saturday sorted nwho wants pl...   \n",
       "14      istj  elonmusk talktv great nvo talktv talktv thrill...   \n",
       "15      istp  maureen please baby please theaters morning pi...   \n",
       "16  combined  sunday walk proud nyou still vote another golf...   \n",
       "\n",
       "                                                 freq  \n",
       "0   {'amp': 137, 'thank': 109, 'new': 77, 'ting': ...  \n",
       "1   {'amp': 216, 'thank': 160, 'new': 130, 'today'...  \n",
       "2   {'live': 115, 'agt': 82, 'espnplus': 75, 'vs':...  \n",
       "3   {'new': 142, 'amp': 108, 'hanx': 83, 'hulu': 8...  \n",
       "4   {'thank': 169, 'happy': 157, 'amp': 140, 'new'...  \n",
       "5   {'amp': 321, 'thank': 257, 'new': 219, 'day': ...  \n",
       "6   {'check': 95, 'new': 87, 'agt': 86, 'god': 81,...  \n",
       "7   {'amp': 116, 'gracias': 115, 'thank': 91, 'new...  \n",
       "8   {'publicar': 92, 'happy': 89, 'day': 75, 'morn...  \n",
       "9   {'twitter': 99, 'friends': 98, 'dear': 97, 'am...  \n",
       "10  {'new': 47, 'gt': 23, 'amp': 21, 'world': 20, ...  \n",
       "11  {'world': 28, 'amp': 24, 'new': 20, 'gue': 19,...  \n",
       "12  {'thank': 120, 'amp': 99, 'new': 86, 'di': 64,...  \n",
       "13  {'thank': 228, 'amp': 147, 'new': 138, 'teampw...  \n",
       "14  {'minoz': 85, 'leeminho': 75, 'great': 43, 'my...  \n",
       "15  {'thank': 78, 'day': 72, 'happy': 71, 'reposte...  \n",
       "16  {'amp': 1575, 'thank': 1563, 'new': 1399, 'hap...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "combined_freq=dict(pd.Series([i.strip() for i in combined_lemmatized.split()]).value_counts())\n",
    "combined_freq\n",
    "num=len(types)\n",
    "combinedtype={'type':{num:'combined'},'lemmatized':{num:combined_lemmatized},'freq':{num:combined_freq}}\n",
    "combinedDF=pd.DataFrame(combinedtype)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "types=pd.concat([types,combinedDF])\n",
    "\n",
    "\n",
    "\n",
    "combinedtype={'personality_domain':{num:'combined'},'lemmatized':{num:combined_lemmatized},'freq':{num:combined_freq}}\n",
    "combinedDF=pd.DataFrame(combinedtype)\n",
    "\n",
    "\n",
    "domains=pd.concat([domains,combinedDF])\n",
    "\n",
    "combinedtype={'attribute':{num:'combined'},'lemmatized':{num:combined_lemmatized},'freq':{num:combined_freq}}\n",
    "combinedDF=pd.DataFrame(combinedtype)\n",
    "\n",
    "\n",
    "pairs=pd.concat([pairs,combinedDF])\n",
    "\n",
    "\n",
    "\n",
    "types=types[['type','lemmatized','freq']]\n",
    "\n",
    "\n",
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'combined'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(attributes)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     at_or\u001b[39m=\u001b[39mattributes[i:i\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     num\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     df\u001b[39m=\u001b[39m((pairs[(pairs\u001b[39m.\u001b[39mattribute\u001b[39m==\u001b[39mat_or[\u001b[39m0\u001b[39m])\u001b[39m|\u001b[39m(pairs\u001b[39m.\u001b[39mattribute\u001b[39m==\u001b[39mat_or[\u001b[39m1\u001b[39m])]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# index=list(range(num-3,num))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/explore.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# print(index)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "attributes=pairs.attribute.tolist()\n",
    "aggunion=attributes.pop(-1);display(aggunion)\n",
    "len(attributes)\n",
    "k=0\n",
    "attributesdflist=[]\n",
    "for i in range(0,len(attributes)-1,2):\n",
    "   \n",
    "    at_or=attributes[i:i+2]\n",
    "    num=len(df)\n",
    "   \n",
    "    df=((pairs[(pairs.attribute==at_or[0])|(pairs.attribute==at_or[1])]))\n",
    "    # index=list(range(num-3,num))\n",
    "    # print(index)\n",
    "    \n",
    "    combinedlemmas=(','.join(list(df.lemmatized.values)))\n",
    "    freq=dict(pd.Series(combinedlemmas.split()).value_counts())\n",
    "    dfdict={'attribute':{num:f'{at_or[0]}|{at_or[1]}'},'lemmatized':{num:combinedlemmas},'freq':{num:freq}}\n",
    "    df=(pd.concat([df,pd.DataFrame(dfdict)]))\n",
    "    df=df.reset_index(drop=True)\n",
    "    attributesdflist.append(df)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "attributesDF=pd.concat(attributesdflist,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "attributesDF['group']=attributesDF.index\n",
    "\n",
    "\n",
    "groupSeries=[]\n",
    "\n",
    "for i in range(0,len(attributesDF),3):\n",
    "    gru=(attributesDF.iloc[i+2].attribute)\n",
    "    groupSeries.extend(attributesDF.group.iloc[i:i+3].map(lambda x:gru))\n",
    "   \n",
    "\n",
    "\n",
    "attributesDF['group']=groupSeries\n",
    "\n",
    "combinedname=[f'({i})' for i in list(set(groupSeries))]\n",
    "combinedname='&'.join(combinedname)\n",
    "\n",
    "\n",
    "combodf=pairs[pairs.attribute==aggunion]\n",
    "combodf['group']=combinedname\n",
    "attributesDF=pd.concat([attributesDF,combodf],ignore_index=True)\n",
    "attributesDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# check out gx boost and multinomial niave base\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(types)\n",
    "display(domains)\n",
    "display(attributesDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Importing mask\n",
    "# from PIL import Image\n",
    "\n",
    "# twitter_mask = np.array(Image.open(\"./Twitter.png\"))\n",
    "# # Plot the wordcloud with the mask applied\n",
    "# wc = WordCloud(background_color='skyblue', mask= twitter_mask, colormap = 'Blues',\n",
    "#                contour_color='white', contour_width=1).generate(' '.join(all_words))\n",
    "# plt.figure(figsize=[10,10])\n",
    "# plt.tight_layout()\n",
    "# plt.imshow(wc, interpolation=\"bilinear\")\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfsetvennprepp(df):\n",
    "    keepkeys=list(df.columns)\n",
    "    keys=list(df[keepkeys[0]].values)\n",
    "    vals=list(df[keepkeys[1]].values)\n",
    "    feqvals=list(df[keepkeys[-1]].values)\n",
    "    tempdict=dict(zip(keys,vals))\n",
    "    freqdict=dict(zip(keys,feqvals))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    combined=keys.pop(-1);combined\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    c=set(freqdict.get(combined).keys())\n",
    "\n",
    "    tempdict\n",
    "    newdict={}\n",
    "\n",
    "    for key in keys:\n",
    "        a=tempdict.get(key)\n",
    "        a=a.replace(',',' ').split()\n",
    "        newdict.update({key:a})\n",
    "\n",
    "    combinedfreq=freqdict.get(combined)\n",
    "    return keys,c,freqdict,newdict,combinedfreq\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributesDF\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typesWordSetInspection():\n",
    "    keys,c,freqdict,newdict,combinedfreq=dfsetvennprepp(types)\n",
    "\n",
    "    vennfigsize=(10,10)\n",
    "\n",
    "    intersectiondict={}\n",
    "    from itertools import combinations\n",
    "    from functools import reduce\n",
    "    keys=list(combinations(keys, 2))\n",
    "\n",
    "\n",
    "    for i in keys:\n",
    "        k=i[0]\n",
    "        kplus1=i[1]\n",
    "    \n",
    "        keyscopy=deepcopy(keys)\n",
    "        kthset=set(newdict.get(k))\n",
    "        kthplus1set=set(newdict.get(kplus1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        inter=kthplus1set&kthset\n",
    "        union=kthplus1set|kthset\n",
    "        sym=kthplus1set.symmetric_difference(kthset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        kfreq=freqdict.get(k)\n",
    "        kplus1freq=freqdict.get(kplus1)\n",
    "        kthfreq=[kfreq,kplus1freq]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        combinedfreq=(reduce(lambda d1,d2: {k: d1.get(k,0)+d2.get(k,0)\n",
    "        for k in set(d1)|set(d2)}, kthfreq))\n",
    "    \n",
    "        totalwords=(sum(list(combinedfreq.values())))\n",
    "        interfreq=[]\n",
    "        for i in inter:\n",
    "            interfreq.append(combinedfreq.get(i))\n",
    "\n",
    "        interfreq=(sum(interfreq))\n",
    "\n",
    "        disimularfreq=[]\n",
    "        for i in sym:    \n",
    "            disimularfreq.append(combinedfreq.get(i))\n",
    "        disimularfreq=(sum(disimularfreq))\n",
    "\n",
    "\n",
    "        unique_to_k=kthset-kthplus1set\n",
    "        unique_k_freq=[]\n",
    "        for i in unique_to_k:    \n",
    "            unique_k_freq.append(combinedfreq.get(i))\n",
    "        unique_k_freq=sum(unique_k_freq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        unique_to_k_plus_1=kthplus1set-kthset\n",
    "        unique_to_k_plus_1_freq=[]\n",
    "        for i in unique_to_k_plus_1:    \n",
    "            unique_to_k_plus_1_freq.append(combinedfreq.get(i))\n",
    "        unique_to_k_plus_1_freq=sum(unique_to_k_plus_1_freq)\n",
    "\n",
    "\n",
    "        kthinterfreq=[]\n",
    "        for i in inter:\n",
    "            kthinterfreq.append(kfreq.get(i))\n",
    "        kthinterfreq=sum(kthinterfreq)\n",
    "\n",
    "        kplus1freqinterfreq=[]\n",
    "        for i in inter:\n",
    "            kplus1freqinterfreq.append(kplus1freq.get(i))\n",
    "        kplus1freqinterfreq=sum(kplus1freqinterfreq)\n",
    "\n",
    "        combined_intersection_freq=[]\n",
    "        for i in inter:\n",
    "            combined_intersection_freq.append(combinedfreq.get(i))\n",
    "        combined_intersection_freq=sum(combined_intersection_freq)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "        fig,ax=plt.subplots(figsize=vennfigsize)\n",
    "        title=f'{k.capitalize()} vs {kplus1.capitalize()}'\n",
    "\n",
    "        # [unionwithoutk.update(set(newdict.get(cop)))for cop in keyscopy]\n",
    "        print(f'{\"_\":>2}'*45,f'\\n\\n{title:>60}\\n\\n',f'{\"_\":>2}'*45)\n",
    "        print(f'\\n\\nintersection set length:\\n{len(inter)}')\n",
    "        print(f'intersection frequency set length:\\n{interfreq}')\n",
    "        print(f'union set length:\\n{totalwords}')\n",
    "        print(f'union frequency set length:\\n{totalwords}')\n",
    "\n",
    "\n",
    "        print(f'intersection freq divided by union freq percent:\\n{((interfreq)/(totalwords))*100:.2f}%')\n",
    "        print(f'symmetric diff freq divided by union freq percent (vals not intersection):\\n{((disimularfreq)/(totalwords))*100:.2f}%')\n",
    "        print(f'unique to {k} freq:\\n{((unique_k_freq)/(totalwords))*100:.2f}% ')\n",
    "        print(f'unique to {kplus1} freq:\\n{((unique_to_k_plus_1_freq)/(totalwords))*100:.2f}% ')\n",
    "\n",
    "        print(f'Abs diff unique ratio from {k} and {kplus1}:\\n{abs((unique_k_freq)/(totalwords)-(unique_to_k_plus_1_freq)/(totalwords))*100:.2f}% ')\n",
    "\n",
    "        print(f'{k} freq contribution to intersection:\\n{((kthinterfreq)/(combined_intersection_freq))*100:.2f}% ')\n",
    "        print(f'{kplus1} freq contribution to intersection:\\n{((kplus1freqinterfreq)/(combined_intersection_freq))*100:.2f}% ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        venn2_wordcloud([kthset,kthplus1set], set_colors=['lime','w'],set_edgecolors=['0', '0'],ax=ax,set_labels=[f'{k.capitalize()}',f'{kplus1.capitalize()}'],word_to_frequency=combinedfreq)#\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def domainsWordSetInspection():\n",
    "    keys,c,freqdict,newdict,combinedfreq=dfsetvennprepp(types)\n",
    "\n",
    "    vennfigsize=(10,10)\n",
    "\n",
    "    intersectiondict={}\n",
    "    from itertools import combinations\n",
    "    from functools import reduce\n",
    "    keys=list(combinations(keys, 2))\n",
    "\n",
    "\n",
    "    for i in keys:\n",
    "        k=i[0]\n",
    "        kplus1=i[1]\n",
    "    \n",
    "        keyscopy=deepcopy(keys)\n",
    "        kthset=set(newdict.get(k))\n",
    "        kthplus1set=set(newdict.get(kplus1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        inter=kthplus1set&kthset\n",
    "        union=kthplus1set|kthset\n",
    "        sym=kthplus1set.symmetric_difference(kthset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        kfreq=freqdict.get(k)\n",
    "        kplus1freq=freqdict.get(kplus1)\n",
    "        kthfreq=[kfreq,kplus1freq]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        combinedfreq=(reduce(lambda d1,d2: {k: d1.get(k,0)+d2.get(k,0)\n",
    "        for k in set(d1)|set(d2)}, kthfreq))\n",
    "    \n",
    "        totalwords=(sum(list(combinedfreq.values())))\n",
    "        interfreq=[]\n",
    "        for i in inter:\n",
    "            interfreq.append(combinedfreq.get(i))\n",
    "\n",
    "        interfreq=(sum(interfreq))\n",
    "\n",
    "        disimularfreq=[]\n",
    "        for i in sym:    \n",
    "            disimularfreq.append(combinedfreq.get(i))\n",
    "        disimularfreq=(sum(disimularfreq))\n",
    "\n",
    "\n",
    "        unique_to_k=kthset-kthplus1set\n",
    "        unique_k_freq=[]\n",
    "        for i in unique_to_k:    \n",
    "            unique_k_freq.append(combinedfreq.get(i))\n",
    "        unique_k_freq=sum(unique_k_freq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        unique_to_k_plus_1=kthplus1set-kthset\n",
    "        unique_to_k_plus_1_freq=[]\n",
    "        for i in unique_to_k_plus_1:    \n",
    "            unique_to_k_plus_1_freq.append(combinedfreq.get(i))\n",
    "        unique_to_k_plus_1_freq=sum(unique_to_k_plus_1_freq)\n",
    "\n",
    "\n",
    "        kthinterfreq=[]\n",
    "        for i in inter:\n",
    "            kthinterfreq.append(kfreq.get(i))\n",
    "        kthinterfreq=sum(kthinterfreq)\n",
    "\n",
    "        kplus1freqinterfreq=[]\n",
    "        for i in inter:\n",
    "            kplus1freqinterfreq.append(kplus1freq.get(i))\n",
    "        kplus1freqinterfreq=sum(kplus1freqinterfreq)\n",
    "\n",
    "        combined_intersection_freq=[]\n",
    "        for i in inter:\n",
    "            combined_intersection_freq.append(combinedfreq.get(i))\n",
    "        combined_intersection_freq=sum(combined_intersection_freq)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "        fig,ax=plt.subplots(figsize=vennfigsize)\n",
    "        title=f'{k.capitalize()} vs {kplus1.capitalize()}'\n",
    "\n",
    "        # [unionwithoutk.update(set(newdict.get(cop)))for cop in keyscopy]\n",
    "        print(f'{\"_\":>2}'*45,f'\\n\\n{title:>60}\\n\\n',f'{\"_\":>2}'*45)\n",
    "        print(f'\\n\\nintersection set length:\\n{len(inter)}')\n",
    "        print(f'intersection frequency set length:\\n{interfreq}')\n",
    "        print(f'union set length:\\n{totalwords}')\n",
    "        print(f'union frequency set length:\\n{totalwords}')\n",
    "\n",
    "\n",
    "        print(f'intersection freq divided by union freq percent:\\n{((interfreq)/(totalwords))*100:.2f}%')\n",
    "        print(f'symmetric diff freq divided by union freq percent (vals not intersection):\\n{((disimularfreq)/(totalwords))*100:.2f}%')\n",
    "        print(f'unique to {k} freq:\\n{((unique_k_freq)/(totalwords))*100:.2f}% ')\n",
    "        print(f'unique to {kplus1} freq:\\n{((unique_to_k_plus_1_freq)/(totalwords))*100:.2f}% ')\n",
    "\n",
    "        print(f'Abs diff unique ratio from {k} and {kplus1}:\\n{abs((unique_k_freq)/(totalwords)-(unique_to_k_plus_1_freq)/(totalwords))*100:.2f}% ')\n",
    "\n",
    "        print(f'{k} freq contribution to intersection:\\n{((kthinterfreq)/(combined_intersection_freq))*100:.2f}% ')\n",
    "        print(f'{kplus1} freq contribution to intersection:\\n{((kplus1freqinterfreq)/(combined_intersection_freq))*100:.2f}% ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        venn2_wordcloud([kthset,kthplus1set], set_colors=['lime','w'],set_edgecolors=['0', '0'],ax=ax,set_labels=[f'{k.capitalize()}',f'{kplus1.capitalize()}'],word_to_frequency=combinedfreq)#\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Importing mask\n",
    "from PIL import Image\n",
    "\n",
    "twitter_mask = np.array(Image.open(\"./Twitter.png\"))\n",
    "# Plot the wordcloud with the mask applied\n",
    "wc = WordCloud(background_color='skyblue', mask= twitter_mask, colormap = 'Blues',\n",
    "               contour_color='white', contour_width=1).generate(' '.join(all_words))\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.tight_layout()\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wordcloud_kwargs=dict(max_font_size=10000, min_font_size=.5)\n",
    "\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    int=c&kthset\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=vennfigsize)\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "   \n",
    "\n",
    "    print(f'number unique to \\n{len(kthset-unionwithoutk)}\\n',f'percent unique of aggregate union combined:\\n{((len(kthset-unionwithoutk))/len(c))*100:.2f}%')\n",
    "    restint=kthset&unionwithoutk\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['red','c','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigint=deepcopy(c)\n",
    "[bigint.intersection_update(typessetdict.get(key))for key in keys]\n",
    "print(f'We have a total of {len(bigint)} in the aggregate intersection\\nWe remove that intersection to compare')\n",
    "for i,k in enumerate(keys):\n",
    "    keyscopy=deepcopy(keys)\n",
    "    kthset=set(typessetdict.get(k))\n",
    "    kthset=kthset-bigint\n",
    "    int=(c&kthset)-bigint\n",
    "    intersectiondict.update({k:int,'intersection count':len(int)})\n",
    "    kfreq=freqdict.get(k)\n",
    "   \n",
    "    keyscopy.pop(i)\n",
    "    unionwithoutk=set()\n",
    "    fig,ax=plt.subplots(figsize=vennfigsize)\n",
    "    \n",
    "    [unionwithoutk.update(typessetdict.get(cop))for cop in keyscopy]\n",
    "    unionwithoutk=unionwithoutk-bigint\n",
    "    print(f'{\"_\":>2}'*45,f'\\n\\n{k:>60}\\n\\n',f'{\"_\":>2}'*45,f'\\n\\nintersection length:\\n{len(int)}',f'\\nintersection combined percent:\\n{(len(int)/len(c))*100:.2f}%')\n",
    "\n",
    "    unique=(kthset-unionwithoutk)-bigint\n",
    "\n",
    "    print(f'number unique to {k}\\n{len(unique)}\\n',f'percent unique of aggregate union combined:\\n{(len(unique)/len(c))*100:.2f}%')\n",
    "    restint=(kthset&unionwithoutk)-bigint\n",
    "    print(f'intersection with rest length:\\n{len(restint)}',f'\\nintersection with rest percent overlap with combined:\\n{(len(restint)/len(c))*100:.2f}%\\n\\n')\n",
    "    \n",
    "    venn3_wordcloud([kthset,unionwithoutk,kthset-unionwithoutk], set_colors=['lime','.35','w'],set_edgecolors=['0', '0','0'],ax=ax,set_labels=[f'{k}',f'Union w/o {k}',f'Unique {k}'],word_to_frequency=freqdict)#\n",
    "    plt.show()\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from itertools import product \n",
    "  \n",
    "# # Get all permutations of length 2 \n",
    "# # and length 2 \n",
    "# x=[\"\".join(seq) for seq in product(\"01\", repeat=4)]\n",
    "# for i in x:\n",
    "#     print(i[0])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumforavg=twitterwordslemma[['type','name']].groupby(['type']).nunique().sum()\n",
    "tochart=(twitterwordslemma[['type','name']].groupby(['type']).nunique()/sumforavg)*100\n",
    "\n",
    "tochart=tochart.reset_index()\n",
    "tochart['percent']=tochart['name']\n",
    "\n",
    "tochart.drop(columns='name',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tochart.index=tochart.type\n",
    "tochart.drop(columns='type',inplace=True)\n",
    "tochart=tochart.sort_values(by='percent',ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "genpoppercent=['13.8% 12.3% 11.6% 8.8% 8.7% 8.5% 8.1% 5.4% 4.4% 4.3% 3.3% 3.2% 2.5% 2.1% 1.8% 1.5%']\n",
    "genpoppercent=str(genpoppercent).replace('%','').split()\n",
    "genpoppercent=[float(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in genpoppercent]\n",
    "\n",
    "\n",
    "types=['ISFJ ESFI ISTJ ISFP ESTI ESFP ENFP ISTP INFP ESTP INTP ENTP ENFJ INTJ ENTI INFT']\n",
    "types=str(types).split()\n",
    "\n",
    "\n",
    "types=[(i.replace('[','').replace(']','').replace('\"','').strip(\"'\").lower()) for i in types]\n",
    "\n",
    "pop=pd.DataFrame(index=types,data={'pop percentage':genpoppercent})\n",
    "\n",
    "tochart=pd.concat([tochart,pop],axis=1,join='inner')\n",
    "tochart.rename(columns={'percent':'found percent'},inplace=True)\n",
    "cols=['pop percentage','found percent']\n",
    "tochart=tochart[cols]\n",
    "tochart.sort_values(by='pop percentage',ascending=False,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "styleddf=tochart.T.style.background_gradient(cmap='Blues',axis=1).format(lambda x : f'{x:.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quad1=[0,0,0,0]\n",
    "quad2=[0,0,0,0]\n",
    "quad3=[0,.25,.35,.25]\n",
    "quad4=[.35,.25,.35,.25]\n",
    "\n",
    "explode = []\n",
    "explode.extend(quad1)\n",
    "explode.extend(quad2)\n",
    "explode.extend(quad3)\n",
    "explode.extend(quad4)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def format_axes(fig):\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n",
    "        ax.tick_params(labelbottom=False, labelleft=False)\n",
    "m=1.23\n",
    "fig = plt.figure(constrained_layout=False,figsize=(m*20,m*12.361))\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('MBTI: General Population Vs Twitter',fontsize=16,weight='demibold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "kwargs1={'title':'General Population (Pie)   ','ax':ax1,'legend':False,'ylabel':'',   'cmap':'Blues'}\n",
    "\n",
    "tochart.plot.pie(y='found percent',**kwargs1)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "kwargs2={'title':'  Twitter (Pie)   ','ax':ax2,'legend':False,'ylabel':'',   'cmap':'viridis'}\n",
    "tochart.plot.pie(y='pop percentage',**kwargs2)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,1])\n",
    "kwargs3={'ax':ax3,'legend':False,'title':'Twitter (Bar)',   'cmap':'viridis'}\n",
    "\n",
    "tochart.plot.barh(y='found percent',**kwargs3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "kwargs4={'ax':ax4,'legend':False,'title':'General Population (Bar)',   'cmap':'Blues'}\n",
    "tochart.plot.barh(y='pop percentage',**kwargs4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"GridSpec\")\n",
    "format_axes(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display('Summary',styleddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'1': [1,2,3],'2': [1,2,3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 =  {'3': [1,2,3],'4': [1,2,3]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = {**d1,**d2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
