{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import unicode character database\n",
    "import unicodedata\n",
    "#import regular expression operations\n",
    "import re\n",
    "\n",
    "#import natural language toolkit\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "#import our aquire\n",
    "\n",
    "\n",
    "#import our stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from emot.emo_unicode import EMOJI_UNICODE, EMOTICONS_EMO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from datetime import date\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib_venn_wordcloud import venn3_wordcloud,venn2_wordcloud\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from matplotlib_venn import venn3, venn3_circles,venn2_circles,venn2,venn2_unweighted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "more_stopwords = ['like', 'im', 'think', 'dont', 'people', 'know', 'one', 'get', 'really','thing',\n",
    "                  'would', 'time', 'type', 'make', 'friend', 'ive', 'much','amp','twitter',\n",
    "                 'say', 'way', 'see', 'thing', 'want', 'thing', 'good', 'something', 'lot',\n",
    "                  'also', 'go', 'always', 'even', 'well', 'someone','https','http','com','co',',',\"'\"]\n",
    "\n",
    "\n",
    "\n",
    "stops=stopwords.words(['french','german','english','spanish','portuguese'])+ more_stopwords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(stops,'stopwords.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stopfilter(text,stop_words_extend_reduce=[\"'\"]):\n",
    "    'we use symmetric difference so if a is already in stop words then it will be added to our third set else our third set will be missing it'\n",
    "    #create oujr english stopwords list\n",
    "    stops = set(pd.read_pickle('stopwords.pkl'))\n",
    "\n",
    "   \n",
    "    stop_words_extend_reduce=set(stop_words_extend_reduce)\n",
    "    stops=stops.symmetric_difference(stop_words_extend_reduce)\n",
    "\n",
    "    # stops=(stops|stop_words_extend)-exclude_words\n",
    "    #another way\n",
    "    \n",
    "    filtered=list(filter((lambda x: x not in stops and len(x)>=2), text.split()))\n",
    "    filtered=' '.join(filtered)\n",
    " \n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "#     '''   \n",
    "#     Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     #make a copy and begin to transform it\n",
    "#     newtext = text.lower()\n",
    "\n",
    "#     #encode into ascii then decode\n",
    "#     newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "#     .encode('ascii', 'ignore')\\\n",
    "#     .decode('utf-8')\n",
    "\n",
    "#     #use re.sub to remove special characters\n",
    "#     newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     return newtext\n",
    "\n",
    "    \n",
    "# def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "#     '''    \n",
    "    \n",
    "#       Takes text, tokenizes it, lemmatizes it\n",
    "#       lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "#       needs to be commented out after the first run (up to modeling)\n",
    "#       # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "#       needs to be un commented commented\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "\n",
    "#     #make ready the lemmatizer object\n",
    "#     newtext=tokenizer(text,regexfilter=regexfilter)\n",
    "#     wnl = nltk.stem.WordNetLemmatizer()\n",
    "#     lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "#     # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "#     # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "#     # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "\n",
    "#     lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "\n",
    "#     lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "#     lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "#     return lemmafiltered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Converting emojis to words\n",
    "def convert_emojis(text):\n",
    "    for emot in EMOJI_UNICODE:\n",
    "        text = text.replace(emot, \"_\".join(EMOJI_UNICODE[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
    "        text = text.replace(':','').replace('_',' ')\n",
    "    return text\n",
    "# Converting emoticons to words    \n",
    "def convert_emoticons(text):\n",
    "    for emot in EMOTICONS_EMO:\n",
    "        text = re.sub(re.escape(emot),EMOTICONS_EMO[emot],text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def basic_clean(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''   \n",
    "    Filters out all special characters if you need to edit then supply a new regex filter \n",
    "    \n",
    "    '''\n",
    "    newtext = convert_emojis(convert_emoticons(text))\n",
    "    \n",
    "    #make a copy and begin to transform it\n",
    "    newtext = newtext.lower()\n",
    "\n",
    "    #encode into ascii then decode\n",
    "    newtext = unicodedata.normalize('NFKD', newtext)\\\n",
    "    .encode('ascii', 'ignore')\\\n",
    "    .decode('utf-8')\n",
    "\n",
    "    #use re.sub to remove special characters\n",
    "    newtext = re.sub(fr'{regexfilter}', ' ', newtext)\n",
    "\n",
    "    return newtext\n",
    "\n",
    "    \n",
    "def lemmatizor(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    '''    \n",
    "    \n",
    "      Takes text, tokenizes it, lemmatizes it\n",
    "      lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "      needs to be commented out after the first run (up to modeling)\n",
    "      # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "      needs to be un commented commented\n",
    "     \n",
    "    '''\n",
    "    total=list(pd.read_pickle('words.pkl'))\n",
    "    \n",
    "    # do basic clean on text and translate emojis/emoticons\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "\n",
    "    #make ready the lemmatizer object\n",
    "    newtext=tokenizer(newtext,regexfilter=regexfilter)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized=split_apply_join(wnl.lemmatize,newtext)\n",
    "\n",
    "    # since the average word lenght in English is 4.7 characters we will apply a conservative estimate and drop any word that is larger than 8 characters as it is likely not a word\n",
    "    # we also recursivley took the set of all words generated then compared that to nltk.corpus.words.words() and used that list as filter this is where total comes from\n",
    "\n",
    "    # lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True and (x in  total)), lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=list(filter(lambda x: (len(x)>1 and len(x)<9 and x.isalpha()==True),  lemmatized.split()))\n",
    "\n",
    "    lemmafiltered=' '.join(lemmafiltered)\n",
    "  \n",
    "    # lemmafiltered=basic_clean(lemmafiltered,regexfilter=regexfilter)\n",
    "\n",
    "    return lemmafiltered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def split_apply_join(funct,listobj):\n",
    "    'helperfuction letters'\n",
    "\n",
    "    mapped=map(funct, listobj)\n",
    "    mapped=list(mapped)\n",
    "    mapped=''.join(mapped)\n",
    "  \n",
    "    return mapped\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(text,regexfilter=r'[^a-z0-9\\'\\s]'):\n",
    "    ''' \n",
    "    For a large file just save it locally\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    newtext=basic_clean(text,regexfilter=regexfilter)\n",
    "    #make ready tokenizer object\n",
    "    tokenize = nltk.tokenize.ToktokTokenizer()\n",
    "    #use the tokenizer\n",
    "    newtext = tokenize.tokenize(newtext, return_str=True)\n",
    "    return newtext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taylor swift         871\n",
       "rihanna              784\n",
       "katy perry           733\n",
       "justin bieber        686\n",
       "the countess         678\n",
       "                    ... \n",
       "yordi rosado           1\n",
       "ivete sangalo          1\n",
       "nicolas vazquez        1\n",
       "tata werneck           1\n",
       "serginho groisman      1\n",
       "Name: name, Length: 590, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tryagain=pd.read_pickle('tryagain.pkl')\n",
    "tryagain.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m docs\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# splitdocs=docs.split('')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m lemma\u001b[39m=\u001b[39m[stopfilter(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [lemmatizor(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m lemma\u001b[39m=\u001b[39m[i\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m lemma \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(i)\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m full_list\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(lemma)\u001b[39m.\u001b[39msplit()\n",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb Cell 3\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m docs\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# splitdocs=docs.split('')\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m lemma\u001b[39m=\u001b[39m[stopfilter(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [lemmatizor(d) \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m lemma\u001b[39m=\u001b[39m[i\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m lemma \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(i)\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m full_list\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(lemma)\u001b[39m.\u001b[39msplit()\n",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb Cell 3\u001b[0m in \u001b[0;36mlemmatizor\u001b[0;34m(text, regexfilter)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m total\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(pd\u001b[39m.\u001b[39mread_pickle(\u001b[39m'\u001b[39m\u001b[39mwords.pkl\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m \u001b[39m# do basic clean on text and translate emojis/emoticons\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m newtext\u001b[39m=\u001b[39mbasic_clean(text,regexfilter\u001b[39m=\u001b[39;49mregexfilter)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m \u001b[39m#make ready the lemmatizer object\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m newtext\u001b[39m=\u001b[39mtokenizer(newtext,regexfilter\u001b[39m=\u001b[39mregexfilter)\n",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb Cell 3\u001b[0m in \u001b[0;36mbasic_clean\u001b[0;34m(text, regexfilter)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbasic_clean\u001b[39m(text,regexfilter\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^a-z0-9\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39ms]\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m     \u001b[39m'''   \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m \u001b[39m    Filters out all special characters if you need to edit then supply a new regex filter \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=178'>179</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=179'>180</a>\u001b[0m     newtext \u001b[39m=\u001b[39m convert_emojis(convert_emoticons(text))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=181'>182</a>\u001b[0m     \u001b[39m#make a copy and begin to transform it\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m     newtext \u001b[39m=\u001b[39m newtext\u001b[39m.\u001b[39mlower()\n",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb Cell 3\u001b[0m in \u001b[0;36mconvert_emojis\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=162'>163</a>\u001b[0m \u001b[39mfor\u001b[39;00m emot \u001b[39min\u001b[39;00m EMOJI_UNICODE:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(emot, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(EMOJI_UNICODE[emot]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39msplit()))\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m     text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/richard/tryagain_prep.ipynb#X10sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39mreturn\u001b[39;00m text\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "count=0\n",
    "dflist=[]\n",
    "\n",
    "aggName={\n",
    "'name':{},\n",
    "'type':{},\n",
    "'handle':{},\n",
    "'date':{},\n",
    "'likeCount':{},\n",
    "'retweetCount':{},\n",
    "'domain':{},\n",
    "'i_e':{},\n",
    "'n_s':{},\n",
    "'t_f':{},\n",
    "'j_p':{},\n",
    "'docs':{},\n",
    "'lemmatized':{},\n",
    "'freqTableCeleb':{},\n",
    "'noon_21':{},\n",
    "'fiveAm_noon':{},\n",
    "'nightowl':{}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "types=tryagain.groupby(['type','name'])    \n",
    "typekeys=list(types.groups.keys())\n",
    "\n",
    "for key in typekeys:\n",
    "   df=types.get_group(key).sort_values(by='date')\n",
    "   if len(df)<=49:##This is our filter for enough \n",
    "        continue\n",
    "   else:\n",
    "    name=list(df['name'].unique())[0]\n",
    "    mbtype=list(df['type'].unique())[0]\n",
    "    handle=list(df['handle'].unique())[0]\n",
    "    date=list(df['date'].unique());#display(date)\n",
    "    likeCount=(df['likeCount'].unique()).sum()\n",
    "    retweetCount=(df['retweetCount'].unique()).sum()\n",
    "    domain=list(df['domain'].unique())[0]\n",
    "    i_e=df['i|e'].unique().tolist()[0]\n",
    "    n_s=df['n|s'].unique().tolist()[0]\n",
    "    t_f=df['t|f'].unique().tolist()[0]\n",
    "    j_p=df['j|p'].unique().tolist()[0]\n",
    "    docs=df['content'].unique()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # splitdocs=docs.split('')\n",
    "    \n",
    "    \n",
    "    lemma=[stopfilter(i) for i in [lemmatizor(d) for d in docs]]\n",
    "    lemma=[i.strip() for i in lemma if len(i)>=2]\n",
    "    full_list=\" \".join(lemma).split()\n",
    "    full_list=[TextBlob(stopfilter(i)) for i in full_list]\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    valcounts=pd.Series(full_list).value_counts()\n",
    "    freqtable=valcounts\n",
    "    \n",
    "    \n",
    "    # valDf=pd.DataFrame(valcounts)\n",
    "    # #valDf=valDf[valDf[0]>2]# This is a filter for frequencies. I am turning it off\n",
    "    \n",
    "    # keep=list(valDf.index)\n",
    "    # full_list=[i for i in full_list if i in keep]\n",
    "    # val_keep=[int(str(i).replace('[','').replace(']','').strip()) for i in valDf.values]\n",
    "    # freqtable=dict(zip(keep,val_keep))\n",
    "\n",
    "\n",
    "    aggName['name'].update({count:name})\n",
    "    aggName['type'].update({count:mbtype})\n",
    "    aggName['handle'].update({count:handle})\n",
    "    aggName['date'].update({count:date})\n",
    "    nightowlsum=0\n",
    "    pmsum=0\n",
    "    amsum=0\n",
    "    tot=0\n",
    "    for d in date:\n",
    "        if d.hour>=21:\n",
    "            nightowlsum+=1\n",
    "        elif d.hour>=12:\n",
    "            pmsum+=1\n",
    "        elif d.hour >=5:\n",
    "            amsum+=1\n",
    "        elif d.hour >=0:\n",
    "            nightowlsum+=1\n",
    "    tot=nightowlsum+pmsum+amsum \n",
    "    aggName['noon_21'].update({count:float(f'{(pmsum/tot)*100:.1f}')})\n",
    "    aggName['fiveAm_noon'].update({count:float(f'{(amsum/tot)*100:.1f}')})\n",
    "    aggName['nightowl'].update({count:float(f'{(nightowlsum/tot)*100:.1f}')})\n",
    "    aggName['likeCount'].update({count:likeCount})\n",
    "    aggName['retweetCount'].update({count:retweetCount})\n",
    "    aggName['domain'].update({count:domain})\n",
    "    aggName['i_e'].update({count:i_e})\n",
    "    aggName['n_s'].update({count:n_s})\n",
    "    aggName['t_f'].update({count:t_f})\n",
    "    aggName['j_p'].update({count:j_p})\n",
    "    aggName['docs'].update({count:docs})\n",
    "    aggName['lemmatized'].update({count:full_list})\n",
    "    aggName['freqTableCeleb'].update({count:freqtable})\n",
    "   \n",
    "    count+=1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aggName_df=pd.DataFrame(aggName);display(aggName_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tochart=(aggName_df['type'].value_counts(normalize=True)*100).sort_index().apply(lambda x:float(f'{x:.2f}'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "genpoppercent=['13.8% 12.3% 11.6% 8.8% 8.7% 8.5% 8.1% 5.4% 4.4% 4.3% 3.3% 3.2% 2.5% 2.1% 1.8% 1.5%']\n",
    "genpoppercent=str(genpoppercent).replace('%','').split()\n",
    "genpoppercent=[float(i.replace('[','').replace(']','').replace('\"','').strip(\"'\")) for i in genpoppercent]\n",
    "\n",
    "\n",
    "types=['ISFJ ESFI ISTJ ISFP ESTI ESFP ENFP ISTP INFP ESTP INTP ENTP ENFJ INTJ ENTI INFT']\n",
    "types=str(types).split()\n",
    "\n",
    "\n",
    "types=[(i.replace('[','').replace(']','').replace('\"','').strip(\"'\").lower()) for i in types]\n",
    "\n",
    "pop=pd.DataFrame(index=types,data={'general population':genpoppercent})\n",
    "tochart=pd.DataFrame(tochart)\n",
    "tochart['twitter']=tochart['type']\n",
    "tochart=tochart['twitter']\n",
    "tochart=pd.DataFrame(tochart)\n",
    "tochart=pd.concat([tochart,pop],axis=1,join='inner')\n",
    "tochart['abs_diff']=abs(tochart.twitter-tochart['general population'])\n",
    "tochart.sort_values(by='abs_diff',ascending=False,inplace=True)\n",
    "styledf=tochart.T.style.background_gradient(cmap='Blues_r',axis=1).format(lambda x : f'{x:.1f}%')\n",
    "styledf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tochart.sort_values(by='general population',ascending=False,inplace=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "def format_axes(fig):\n",
    "    for i, ax in enumerate(fig.axes):\n",
    "        ax.text(0.5, 0.5, \"ax%d\" % (i+1), va=\"center\", ha=\"center\")\n",
    "        ax.tick_params(labelbottom=False, labelleft=False)\n",
    "m=.75\n",
    "fig = plt.figure(constrained_layout=False,figsize=(m*20,m*12.361))\n",
    "\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# identical to ax1 = plt.subplot(gs.new_subplotspec((0, 0), colspan=3))\n",
    "\n",
    "\n",
    "\n",
    "plt.suptitle('MBTI: General Population Vs Twitter',fontsize=16,weight='demibold')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "kwargs1={'title':'General Population (Pie)   ','ax':ax1,'legend':False,'ylabel':'',   'cmap':'Blues_r'}\n",
    "\n",
    "tochart.plot.pie(y='general population',**kwargs1)\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(gs[1, 1])\n",
    "\n",
    "kwargs2={'title':'  Twitter (Pie)   ','ax':ax2,'legend':False,'ylabel':'',   'cmap':'Blues'}\n",
    "tochart.plot.pie(y='twitter',**kwargs2)\n",
    "\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,1])\n",
    "kwargs3={'ax':ax3,'legend':False,'title':'Twitter (Bar)',   'cmap':'Blues'}\n",
    "\n",
    "tochart.sort_index().plot.barh(y='twitter',**kwargs3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "kwargs4={'ax':ax4,'legend':False,'title':'General Population (Bar)',   'cmap':'Blues_r'}\n",
    "tochart.sort_index().plot.barh(y='general population',**kwargs4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle(\"GridSpec\")\n",
    "format_axes(fig)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display('Summary',styledf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.to_pickle(aggName_df,'aggName.pkl')\n",
    "aggName_df=pd.read_pickle('aggName.pkl')\n",
    "   \n",
    "aggName_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topLevelGroups=['type',\n",
    "'domain',\n",
    "'i_e',\n",
    "'n_s',\n",
    "'t_f',\n",
    "'j_p']\n",
    "grudict={i:{} for i in topLevelGroups}\n",
    "\n",
    "\n",
    "topLevelGroupsDflist=[]\n",
    "\n",
    "for gru in topLevelGroups:\n",
    "  \n",
    "\n",
    "    typegroups=aggName_df.groupby(gru)\n",
    "\n",
    "    aggType ={\n",
    "    'names':{},\n",
    "    'type':{},\n",
    "    'handles':{},\n",
    "    'dates':{},\n",
    "    'likeCount':{},\n",
    "    'retweetCount':{},\n",
    "    'domain':{},\n",
    "    'i_e':{},\n",
    "    'n_s':{},\n",
    "    't_f':{},\n",
    "    'j_p':{},\n",
    "    'docs':{},\n",
    "    'lemmatized':{},\n",
    "    'freqTableGroup':{},\n",
    "    'noon_21':{},\n",
    "    'fiveAm_noon':{},\n",
    "    'nightowl':{}\n",
    "    }\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    typekeys=typegroups.groups.keys()\n",
    "    count=0\n",
    "    for key in typekeys:\n",
    "\n",
    "      \n",
    "\n",
    "        \n",
    "        df=typegroups.get_group(key)\n",
    "\n",
    "        noon_21=df.noon_21\n",
    "        fiveAm_noon=df.fiveAm_noon\n",
    "        nightowl=df.nightowl\n",
    "        noon_21=float(f'{(noon_21/len(noon_21)).sum():.1f}')\n",
    "        fiveAm_noon=float(f'{(fiveAm_noon/len(fiveAm_noon)).sum():.1f}')\n",
    "        nightowl=float(f'{(nightowl/len(nightowl)).sum():.1f}')\n",
    "    \n",
    "        aggType['names'].update({count:df.name.unique()})\n",
    "    \n",
    "        aggType['handles'].update({count:df.handle.unique()})\n",
    "        aggType['dates'].update({count:df.date.values})\n",
    "\n",
    "        aggType['noon_21'].update({count:noon_21})\n",
    "        aggType['fiveAm_noon'].update({count:fiveAm_noon})\n",
    "        aggType['nightowl'].update({count:nightowl})\n",
    "        aggType['likeCount'].update({count:likeCount.sum()})\n",
    "        aggType['retweetCount'].update({count:retweetCount.sum()})\n",
    "        aggType['domain'].update({count:set(df.domain.unique())})\n",
    "        aggType['type'].update({count:set(df.type.unique())})\n",
    "        aggType['i_e'].update({count:set(df.i_e.unique())})\n",
    "        aggType['n_s'].update({count:set(df.n_s.unique())})\n",
    "        aggType['t_f'].update({count:set(df.t_f.unique())})\n",
    "        aggType['j_p'].update({count:set(df.j_p.unique())})\n",
    "        aggType['docs'].update({count:df.docs.values})\n",
    "        lemmatized=df.lemmatized.to_list()\n",
    "        strings=str(lemmatized).replace('[','').replace(']','').replace(\"'\",'').replace('\"','').strip().split(',')\n",
    "        freqtable=dict(pd.Series(strings).value_counts())   \n",
    "        aggType['lemmatized'].update({count:lemmatized})\n",
    "        aggType['freqTableGroup'].update({count:freqtable})\n",
    "        \n",
    "        \n",
    "        grudict[f'{gru}'].update({key:pd.DataFrame(aggType)})\n",
    "        count+=1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grudict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typelist=[]\n",
    "for key in grudict['type'].keys():\n",
    "    print(key)\n",
    "    typelist.append(grudict['type'].get(key).freqTableGroup.values[0])\n",
    "\n",
    "keyset=set()\n",
    "\n",
    "[keyset.update(i.keys()) for i in typelist]\n",
    "\n",
    "\n",
    "bigdict={i:[] for i in keyset}\n",
    "for d in typelist:\n",
    "    for i,v in d.items():\n",
    "        bigdict[i].append(v)\n",
    "[bigdict.update({i:sum(bigdict[i])}) for i in (bigdict.keys())]\n",
    "words=pd.Series(bigdict)\n",
    "\n",
    "wordslist=[]\n",
    "collist=[]\n",
    "for key in grudict['type'].keys():\n",
    "\n",
    "    wordslist.append(pd.Series(grudict['type'].get(key).freqTableGroup.values[0]))\n",
    "    collist.append(key)\n",
    "wordslist.append(words)\n",
    "collist.append('combined')\n",
    "wordslist.reverse()\n",
    "collist.reverse()\n",
    "\n",
    "word_counts = (pd.concat(wordslist, axis=1, sort=True)\n",
    "                .set_axis(collist, axis=1, inplace=False)\n",
    "                .fillna(0)\n",
    "                .apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts.head()\n",
    "percentlist=[]\n",
    "for i in list(set(word_counts.columns)-{'combined'}):\n",
    "    word_counts[f'p_{i}']=word_counts[i]/word_counts.combined\n",
    "    percentlist.append(f'p_{i}')\n",
    "word_counts[percentlist].head(20).plot.barh(stacked=True)\n",
    "\n",
    "[print(i) for i in wordslist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "vennfigsize=(10,10)\n",
    " \n",
    "for key in grudict.keys():\n",
    "    subgrukey=grudict[key].keys()\n",
    "\n",
    "    combos=combinations(subgrukey,2)\n",
    "    for c in combos:\n",
    "        commondict={}\n",
    "        ckeys=set()\n",
    "        a=grudict[key].get(c[0]).freqTableGroup.values[0]\n",
    "        b=grudict[key].get(c[1]).freqTableGroup.values[1]\n",
    "        \n",
    "\n",
    "\n",
    "        akeys=set((a).keys())\n",
    "        bkeys=set((b).keys())     \n",
    "        ckeys.update(akeys)\n",
    "        ckeys.update(bkeys)\n",
    "        ckeys=list(ckeys)\n",
    "        commondict={i:[] for i in ckeys}\n",
    "        [commondict[i].append(a.get(i)) for i in akeys]\n",
    "        [commondict[i].append(b.get(i)) for i in bkeys]    \n",
    "        [commondict.update({i:sum(commondict.get(i))}) for i in ckeys]   \n",
    "\n",
    "    \n",
    "\n",
    "        k=c[0]\n",
    "        kplus1=c[1]\n",
    "        kfreq=a\n",
    "        kplus1freq=b\n",
    "        kthset=set(kfreq.keys())\n",
    "        kthplus1set=set(kplus1freq.keys())\n",
    "\n",
    "\n",
    "        inter=kthplus1set&kthset\n",
    "        union=kthplus1set|kthset\n",
    "        sym=kthplus1set.symmetric_difference(kthset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        combinedfreq=commondict\n",
    "    \n",
    "        totalwords=(sum(list(combinedfreq.values())))\n",
    "        interfreq=[]\n",
    "        for i in inter:\n",
    "            interfreq.append(combinedfreq.get(i))\n",
    "\n",
    "        interfreq=(sum(interfreq))\n",
    "\n",
    "        disimularfreq=[]\n",
    "        for i in sym:    \n",
    "            disimularfreq.append(combinedfreq.get(i))\n",
    "        disimularfreq=(sum(disimularfreq))\n",
    "\n",
    "\n",
    "        unique_to_k=kthset-kthplus1set\n",
    "        unique_k_freq=[]\n",
    "        for i in unique_to_k:    \n",
    "            unique_k_freq.append(combinedfreq.get(i))\n",
    "        unique_k_freq=sum(unique_k_freq)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        unique_to_k_plus_1=kthplus1set-kthset\n",
    "        unique_to_k_plus_1_freq=[]\n",
    "        for i in unique_to_k_plus_1:    \n",
    "            unique_to_k_plus_1_freq.append(combinedfreq.get(i))\n",
    "        unique_to_k_plus_1_freq=sum(unique_to_k_plus_1_freq)\n",
    "\n",
    "\n",
    "        kthinterfreq=[]\n",
    "        for i in inter:\n",
    "            kthinterfreq.append(kfreq.get(i))\n",
    "        kthinterfreq=sum(kthinterfreq)\n",
    "\n",
    "        kplus1freqinterfreq=[]\n",
    "        for i in inter:\n",
    "            kplus1freqinterfreq.append(kplus1freq.get(i))\n",
    "        kplus1freqinterfreq=sum(kplus1freqinterfreq)\n",
    "\n",
    "        combined_intersection_freq=[]\n",
    "        for i in inter:\n",
    "            combined_intersection_freq.append(combinedfreq.get(i))\n",
    "        combined_intersection_freq=sum(combined_intersection_freq)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "        fig,ax=plt.subplots(figsize=vennfigsize)\n",
    "        title=f'{k.capitalize()} vs {kplus1.capitalize()}'\n",
    "\n",
    "        # [unionwithoutk.update(set(newdict.get(cop)))for cop in keyscopy]\n",
    "        print(f'{\"_\":>2}'*45,f'\\n\\n{title:>60}\\n\\n',f'{\"_\":>2}'*45)\n",
    "        print(f'\\n\\nintersection set length:\\n{len(inter)}')\n",
    "        print(f'intersection frequency set length:\\n{interfreq}')\n",
    "        print(f'union set length:\\n{len(union)}')\n",
    "        print(f'union frequency set length:\\n{totalwords}')\n",
    "\n",
    "\n",
    "        print(f'intersection freq divided by union freq percent:\\n{((interfreq)/(totalwords))*100:.2f}%')\n",
    "        print(f'symmetric diff freq divided by union freq percent (words not in intersection):\\n{((disimularfreq)/(totalwords))*100:.2f}%')\n",
    "        print(f'unique to {k} freq:\\n{((unique_k_freq)/(totalwords))*100:.2f}% ')\n",
    "        print(f'unique to {kplus1} freq:\\n{((unique_to_k_plus_1_freq)/(totalwords))*100:.2f}% ')\n",
    "\n",
    "        print(f'Abs diff unique ratio from {k} and {kplus1}:\\n{abs((unique_k_freq)/(totalwords)-(unique_to_k_plus_1_freq)/(totalwords))*100:.2f}% ')\n",
    "\n",
    "        print(f'{k} freq contribution to intersection:\\n{((kthinterfreq)/(combined_intersection_freq))*100:.2f}% ')\n",
    "        print(f'{kplus1} freq contribution to intersection:\\n{((kplus1freqinterfreq)/(combined_intersection_freq))*100:.2f}% ')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        venn2_wordcloud([kthset,kthplus1set], set_colors=['c','w'],set_edgecolors=['0', '0'],ax=ax,set_labels=[f'{k.capitalize()}',f'{kplus1.capitalize()}'],word_to_frequency=combinedfreq)#\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
