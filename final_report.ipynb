{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffad363-09bf-409f-b76a-515ca0039f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ds imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for wrangling and exploration\n",
    "import wrangle\n",
    "import explore\n",
    "import model\n",
    "\n",
    "# for statistical testing\n",
    "import scipy.stats as stats\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=2)\n",
    "\n",
    "# filter out noise\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# align markdown tables to the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af158eaf-0aab-469e-bc3b-f1054b3c4906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a176c5-e00e-4b5b-a1d2-44c9c5c0d405",
   "metadata": {},
   "source": [
    "# MYERS BRIGGS TWITTER ASSESSMENT\n",
    "\n",
    "For company managers who are dissatisfied with currently available group assignment practices. Our service is a psychographic segmentation solution that enhances workplace efficiency and improves employee satisfaction. We harnessed the power of artificial intelligence to create a personality profile predictor that is based on assessment of orgainic text as opposed to intrusive, biased self-reporting. Management uses this more accurate classifiation to build structured, efficient, and satisfied teams. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a35a994-42e8-41b2-8d20-2c4fa81397a8",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Executive Summary:\n",
    "\n",
    "Goals:\n",
    "- Analyze relationships between tweets and the myers briggs personality types and domains\n",
    "- Build a classification model to predict a personality type based on a user's tweets\n",
    "\n",
    "Key Findings:\n",
    "- Exploring domains produces more distinct contrast between groups than looking at all 16 personability types individually\n",
    "- Explorers reached the largest audience despite having the lowest average compound sentiment\n",
    "\n",
    "Takeaways:\n",
    "- Our best model classifying into the domains, a XGradientBoosting model using TF-IDF, performed at 73.5% accuracy on the test dataset. This barely outperforms the baseline at 46.7%\n",
    "\n",
    "Recommendations:\n",
    "- This tool can be used to add an additional layer of diversity when building teams.\n",
    "- Looking at how types and domains interact can provide business insights for targeted marketing and training for known personality types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57917374-e0f6-4efc-9446-f3ac04bfd053",
   "metadata": {},
   "source": [
    "----\n",
    "## 1. Planning\n",
    " - Create deliverables:\n",
    "     - README\n",
    "     - final_report.ipynb\n",
    "     - working_report.ipynb\n",
    "     - canva presentation\n",
    " - Bring over functional wrangle.py, explore.py, and model.py files\n",
    " - Acquire the data from the Kaggle and Twitter via acquire.py's web scraping. Save the data locally.\n",
    " - Prepare and split the data via the prepare.py functions\n",
    " - Explore the data and define hypothesis. Run the appropriate statistical tests in order to accept or reject each null hypothesis. Document findings and takeaways.\n",
    " - Model a baseline in predicting personality type and document the accuracy.\n",
    " - Fit and train classification models to predict personality type or domain on the train dataset.\n",
    " - Evaluate the models by comparing the train and validation data.\n",
    " - Select the best model and evaluate it on the train data.\n",
    " - Develop and document all findings, takeaways, recommendations and next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a20600-05d2-4d7c-a399-fa0bd2c29b2c",
   "metadata": {},
   "source": [
    "----\n",
    "## 2. Data Wrangling\n",
    "This step calls the wrangle function from the wrangle.py. This function:\n",
    "Grabs the data from a github repository of twitter handles and celebrity names [here](https://gist.githubusercontent.com/mbejda/9c3353780270e7298763/raw/1bfc4810db4240d85947e6aef85fcae71f475493/Top-1000-Celebrity-Twitter-Accounts.csv) then we used the handles to create a list of the twitter handles. We use the SNSCRAPE twitter module and iterate through  the list gathering the last 500 tweets per celeb. From there the results are saved as a JSON, the JSON is read into a data frame which is updated for each celeb.\n",
    "After that we merge the data frame with the data from [here](https://raw.githubusercontent.com/twitter-personality-predictor/twitter-personality-predictor/main/twitter_handles.csv)\n",
    "\n",
    "- Feature engineers:\n",
    "    - personality_domain -> bins the 16 personalities to their domain\n",
    "    - sentiment -> compound score using the Senitment Intensity Analyzer\n",
    "    - message_length -> count of characters in each tweet after lemmitization\n",
    "    - word_count -> count of words in each tweet after lemmitization\n",
    "    - personality pairs -> creates boolean features for each pair\n",
    "- Splits the data into 60/20/20 for train, validate, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f126f1ba-9a84-48e8-b817-f3d4c99c187b",
   "metadata": {},
   "source": [
    "#### Data Dictionary\n",
    "\n",
    "| Target | Type | Description |\n",
    "| ---- | ---- | ---- |\n",
    "| type | str | the 16 myers briggs personality types |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59bdec-c101-4df7-9778-286e23285b8d",
   "metadata": {},
   "source": [
    "| Feature Name | Type | Description |\n",
    "| ---- | ---- | ---- |\n",
    "| name | str | the owner of the twitter account scraped |\n",
    "| lemmatized | str | the lemmatized version of past 100 tweets scraped by user |\n",
    "| personality_domain | str | which of the 4 domains the personality type is in |\n",
    "| sentiment | float | the compound sentiment score |\n",
    "| message_length | int | the count of characters in the combined 100 tweets after lemmatization |\n",
    "| word_count | int | the count of words in the combined 100 tweets after lemmatization |\n",
    "| i_e | int | i for introvert, e for extrovert |\n",
    "| s_n | int | s for sensing, n for intuitive |\n",
    "| f_t | int | f for feeling, t for thinking |\n",
    "| p_j | int | p for perceiving, j for judging |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db15f0b1-8100-4549-843a-d335160035b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tweet_data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/richardmacken/codeup-data-science/capstone-project/final_report.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/final_report.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# call the wrangle function from wrangle.py\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/richardmacken/codeup-data-science/capstone-project/final_report.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m wrangle\u001b[39m.\u001b[39;49mwrangle()\n",
      "File \u001b[0;32m~/codeup-data-science/capstone-project/wrangle.py:23\u001b[0m, in \u001b[0;36mwrangle\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrangle\u001b[39m():\n\u001b[1;32m     22\u001b[0m     \u001b[39m\"\"\"This function is all encompassing to acquire and clean/prepare the data. there are 5 functions that are embedded inside this function that are used to return the personality information in a DataFrame\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_pickle(\u001b[39m'\u001b[39;49m\u001b[39mtweet_data.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     24\u001b[0m     c\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     25\u001b[0m     words_per_tweet \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/pickle.py:187\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    186\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[0;32m--> 187\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    188\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    189\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    190\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    191\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    192\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    193\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    201\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:798\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    799\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    801\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tweet_data.pkl'"
     ]
    }
   ],
   "source": [
    "# call the wrangle function from wrangle.py\n",
    "df = wrangle.wrangle()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import httplib2 as h\n",
    "# url='https://drive.google.com/file/d/1YCU9t-rhhwRkvRikFP9TsRrrYNmnfnU_/view?usp=share_link'\n",
    "\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1YCU9t-rhhwRkvRikFP9TsRrrYNmnfnU_'\n",
    "url='https://drive.google.com/file/d/1DwQ79KSIqNZ9hStqHzxrsKIxVou6Jh9K/view?usp=share_link'\n",
    "\n",
    "\n",
    "\n",
    "# # url='https://drive.google.com/uc?id=1DwQ79KSIqNZ9hStqHzxrsKIxVou6Jh9K'\n",
    "# # pk = pickle.load(h.request(',\"GET\"))\n",
    "# urllib.request.urlretrieve(url,'newpickle.pkl')\n",
    "# pk=pd.read_pickle('newpickle.pkl')\n",
    "# pk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e232bbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f98365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b96ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bd6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d0855-dbb7-4352-ab7d-060fbca90ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split\n",
    "train, val, test = wrangle.split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c4d7b-d9ef-4253-afd5-4cd5a777a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the split\n",
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e8f2d5-b233-49cc-bf04-ceed39c1dd0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Wrangling Takeaways\n",
    "- The data acquired contains 599 celebrities most recent 100 tweets over 3 features\n",
    "- Feature engineered an additional 8 features to support further exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7ccfa-886f-43a4-bcc0-3ad626b617c2",
   "metadata": {},
   "source": [
    "----\n",
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c326ff-7ee9-4f2a-8b5e-8b6171fcda0d",
   "metadata": {},
   "source": [
    "### What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c0e75-09ad-48aa-bd94-2189b2146f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the overall visualization\n",
    "explore.overall_data_vis(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf9dfc-2922-4f12-9938-8603639fee42",
   "metadata": {},
   "source": [
    "### Question 1: What scale of groupings shows a significant relationship in sentiment? Pairs, types, or domains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a90c0-2f96-454a-965d-da46338d02fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the viz for q1\n",
    "explore.q1_vis(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4088d2-4929-4a86-8684-3f98e7cec7cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Hypothesis 1:\n",
    "\n",
    "    Ho -> The mean sentiment for explorers is less than or equal to the mean sentiment for analysts\n",
    "    Ha -> The mean sentiment for explorers is greater than the mean sentiment for analysts\n",
    "    T-Test, Two-Sample, One-Tailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193a608-aa8b-41c4-9db3-69386028036b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the stats results for q1\n",
    "explore.q1_stats(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a61e2a-20f2-4f01-a81f-b8a8f6d81849",
   "metadata": {},
   "source": [
    "### Question 1 Takeaways\n",
    "- Even though there is variance in the mean sentiment between domains, there is not a significant difference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07e5d9-acc7-4ade-a62e-090bac64b0bc",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 2a: What words are seen across all personality types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d10a35-69f9-4084-b1ce-0a422121a6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the viz for q2a\n",
    "explore.q2a_vis(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5428b9-f79a-4338-8c65-b3a7b8185d8b",
   "metadata": {},
   "source": [
    "### Question 2b: What words are unique by domain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de63d278-0f7e-49c1-b689-1bbf95336ea2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src='capstoneDomains.png' alt='domains' width='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521c4ccc-b842-43f5-8243-425c945fecf7",
   "metadata": {},
   "source": [
    "### Question 2 Takeaways\n",
    "- Despite the most common words being easy to capture, it is what makes the domains different that will be impactful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc6c8e-8e14-445d-94c3-ab1cd262b2dc",
   "metadata": {},
   "source": [
    "---\n",
    "### Question 3: Is there a relationship between word count and personality type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3082a-df55-4931-adee-ba655c0c8347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# show the viz for q3\n",
    "explore.q3_vis(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc645537-6b87-4ed2-89c1-8c4b28949107",
   "metadata": {},
   "source": [
    "### Question 3 Takeaways\n",
    " - Explorers reached the largest audience\n",
    " - It makes sense explorers also had the most likes after getting the most retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa2a01-fccb-4222-999d-e569ba029842",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Modeling\n",
    "The goal is to maximize accuracy.\n",
    "\n",
    "| Features Dropped | Features Kept |\n",
    "| ---- | ---- |\n",
    "| pairs | lemmatized |\n",
    "| domain | bow |\n",
    "| sentiment | TF-IDF |\n",
    "| message_length |  |\n",
    "| word_count |  |\n",
    "|  |  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286dc2da-bce2-43c1-9241-9e7c7f7e3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the baseline accuracy\n",
    "model.get_baseline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090adbc-ddb1-4b5f-bf1a-81d02a899262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330bbc4-efdf-4642-9ae4-d2de6edfeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model results on train and validate\n",
    "model.top_3_personality_domains(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db82074-7cf6-4385-8dbe-badf9bef867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the best model's results on test\n",
    "model.personality_domain_test(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f0ca97-f5a8-42e2-85e8-c297835b756d",
   "metadata": {},
   "source": [
    "### Modeling Takeaways:\n",
    "- The best model was the TF-IDF XGradientBoosting model at 73.5% accuracy on test.\n",
    "- The other models showed strong signs of overfitting the data on train, but were producing similar results on validate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186445ad-3288-4971-86fd-67721ddad6f8",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusion\n",
    "We can accurately use tweets to analyze behavior and classify authors of text into a Myers Briggs domain, which can provide guidance when managers are assigning teams and roles. Our best model performed at 73.5% accuracy vs baseline's 46.7%. \n",
    "\n",
    "### Recommendations\n",
    " - Exercise this model as a tool in a toolbox, not a guideline heuristic\n",
    " - Use this model to create another layer in building diverse teams\n",
    "\n",
    "### Next Steps:\n",
    "- Explore further associations in the words to inlcude emojis, swear words, ect. \n",
    "- Investigate other social networks and classification methods\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "38cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
